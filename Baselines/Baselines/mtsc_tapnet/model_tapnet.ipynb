{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import euclidean_dist, normalize, output_conv_size, dump_embedding\n",
    "import numpy as np\n",
    "\n",
    "class TapNet(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat, len_ts, n_train, nclass, dropout, filters, kernels, dilation, layers, use_rp, rp_params,\n",
    "                 use_att=True, use_ss=False, sup_ratio=1, use_metric=False, use_muse=False, use_lstm=False, use_cnn=True):\n",
    "        super(TapNet, self).__init__()\n",
    "        self.nclass = nclass\n",
    "        self.n_train = n_train\n",
    "        self.dropout = dropout\n",
    "        self.use_metric = use_metric\n",
    "        self.use_muse = use_muse\n",
    "        self.use_lstm = use_lstm\n",
    "        self.use_cnn = use_cnn\n",
    "\n",
    "        # parameters for random projection\n",
    "        self.use_rp = use_rp\n",
    "        self.rp_group, self.rp_dim = rp_params\n",
    "\n",
    "        if not self.use_muse:\n",
    "            # LSTM\n",
    "            self.channel = nfeat\n",
    "            self.ts_length = len_ts\n",
    "\n",
    "            self.lstm_dim = 128\n",
    "            self.lstm = nn.LSTM(self.ts_length, self.lstm_dim)\n",
    "            #self.dropout = nn.Dropout(0.8)\n",
    "\n",
    "            # convolutional layer\n",
    "            # features for each hidden layers\n",
    "            # out_channels = [256, 128, 256]\n",
    "            # filters = [256, 256, 128]\n",
    "            # poolings = [2, 2, 2]\n",
    "            paddings = [0, 0, 0]\n",
    "            print(\"dilation\", dilation)\n",
    "            if self.use_rp:\n",
    "                self.conv_1_models = nn.ModuleList()\n",
    "                self.idx = []\n",
    "                for i in range(self.rp_group):\n",
    "                    self.conv_1_models.append(nn.Conv1d(self.rp_dim, filters[0], kernel_size=kernels[0], dilation=dilation, stride=1, padding=paddings[0]))\n",
    "                    self.idx.append(np.random.permutation(nfeat)[0: self.rp_dim])\n",
    "            else:\n",
    "                self.conv_1 = nn.Conv1d(self.channel, filters[0], kernel_size=kernels[0], dilation=dilation, stride=1, padding=paddings[0])\n",
    "            # self.maxpool_1 = nn.MaxPool1d(poolings[0])\n",
    "            self.conv_bn_1 = nn.BatchNorm1d(filters[0])\n",
    "\n",
    "            self.conv_2 = nn.Conv1d(filters[0], filters[1], kernel_size=kernels[1], stride=1, padding=paddings[1])\n",
    "            # self.maxpool_2 = nn.MaxPool1d(poolings[1])\n",
    "            self.conv_bn_2 = nn.BatchNorm1d(filters[1])\n",
    "\n",
    "            self.conv_3 = nn.Conv1d(filters[1], filters[2], kernel_size=kernels[2], stride=1, padding=paddings[2])\n",
    "            # self.maxpool_3 = nn.MaxPool1d(poolings[2])\n",
    "            self.conv_bn_3 = nn.BatchNorm1d(filters[2])\n",
    "\n",
    "            # compute the size of input for fully connected layers\n",
    "            fc_input = 0\n",
    "            if self.use_cnn:\n",
    "                conv_size = len_ts\n",
    "                for i in range(len(filters)):\n",
    "                    conv_size = output_conv_size(conv_size, kernels[i], 1, paddings[i])\n",
    "                fc_input += conv_size \n",
    "                #* filters[-1]\n",
    "            if self.use_lstm:\n",
    "                fc_input += conv_size * self.lstm_dim\n",
    "            \n",
    "            if self.use_rp:\n",
    "                fc_input = self.rp_group * filters[2] + self.lstm_dim\n",
    "\n",
    "\n",
    "        # Representation mapping function\n",
    "        layers = [fc_input] + layers\n",
    "        print(\"Layers\", layers)\n",
    "        self.mapping = nn.Sequential()\n",
    "        for i in range(len(layers) - 2):\n",
    "            self.mapping.add_module(\"fc_\" + str(i), nn.Linear(layers[i], layers[i + 1]))\n",
    "            self.mapping.add_module(\"bn_\" + str(i), nn.BatchNorm1d(layers[i + 1]))\n",
    "            self.mapping.add_module(\"relu_\" + str(i), nn.LeakyReLU())\n",
    "\n",
    "        # add last layer\n",
    "        self.mapping.add_module(\"fc_\" + str(len(layers) - 2), nn.Linear(layers[-2], layers[-1]))\n",
    "        if len(layers) == 2:  # if only one layer, add batch normalization\n",
    "            self.mapping.add_module(\"bn_\" + str(len(layers) - 2), nn.BatchNorm1d(layers[-1]))\n",
    "\n",
    "        # Attention\n",
    "        att_dim, semi_att_dim = 128, 128\n",
    "        self.use_att = use_att\n",
    "        if self.use_att:\n",
    "            self.att_models = nn.ModuleList()\n",
    "            for _ in range(nclass):\n",
    "\n",
    "                att_model = nn.Sequential(\n",
    "                    nn.Linear(layers[-1], att_dim),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(att_dim, 1)\n",
    "                )\n",
    "                self.att_models.append(att_model)\n",
    "\n",
    "        self.use_ss = use_ss  # whether to use semi-supervised mode\n",
    "        self.sup_ratio = sup_ratio\n",
    "        if self.use_ss:\n",
    "            self.semi_att = nn.Sequential(\n",
    "                nn.Linear(layers[-1], semi_att_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(semi_att_dim, self.nclass)\n",
    "            )\n",
    "\n",
    "        # print(self.use_rp, self.rp_group, self.rp_dim)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x, labels, idx_train, idx_val, idx_test = input  # x is N * L, where L is the time-series feature dimension\n",
    "        ## define the labeled and unlabeled portions in dataset \n",
    "        n_sup = int(sup_ratio * self.n_train)\n",
    "\n",
    "        if not self.use_muse:\n",
    "            N = x.size(0)\n",
    "\n",
    "            # LSTM\n",
    "            if self.use_lstm:\n",
    "                x_lstm = self.lstm(x)[0]\n",
    "                x_lstm = x_lstm.mean(1)\n",
    "                x_lstm = x_lstm.view(N, -1)\n",
    "\n",
    "            if self.use_cnn:\n",
    "                # Covolutional Network\n",
    "                # input ts: # N * C * L\n",
    "                if self.use_rp:\n",
    "                    for i in range(len(self.conv_1_models)):\n",
    "                        #x_conv = x\n",
    "                        x_conv = self.conv_1_models[i](x[:, self.idx[i], :])\n",
    "                        x_conv = self.conv_bn_1(x_conv)\n",
    "                        x_conv = F.leaky_relu(x_conv)\n",
    "\n",
    "                        x_conv = self.conv_2(x_conv)\n",
    "                        x_conv = self.conv_bn_2(x_conv)\n",
    "                        x_conv = F.leaky_relu(x_conv)\n",
    "\n",
    "                        x_conv = self.conv_3(x_conv)\n",
    "                        x_conv = self.conv_bn_3(x_conv)\n",
    "                        x_conv = F.leaky_relu(x_conv)\n",
    "\n",
    "                        x_conv = torch.mean(x_conv, 2)\n",
    "\n",
    "                        if i == 0:\n",
    "                            x_conv_sum = x_conv\n",
    "                        else:\n",
    "                            x_conv_sum = torch.cat([x_conv_sum, x_conv], dim=1)\n",
    "\n",
    "                    x_conv = x_conv_sum\n",
    "                else:\n",
    "                    x_conv = x\n",
    "                    x_conv = self.conv_1(x_conv)  # N * C * L\n",
    "                    x_conv = self.conv_bn_1(x_conv)\n",
    "                    x_conv = F.leaky_relu(x_conv)\n",
    "\n",
    "                    x_conv = self.conv_2(x_conv)\n",
    "                    x_conv = self.conv_bn_2(x_conv)\n",
    "                    x_conv = F.leaky_relu(x_conv)\n",
    "\n",
    "                    x_conv = self.conv_3(x_conv)\n",
    "                    x_conv = self.conv_bn_3(x_conv)\n",
    "                    x_conv = F.leaky_relu(x_conv)\n",
    "\n",
    "                    x_conv = x_conv.view(N, -1)\n",
    "\n",
    "            if self.use_lstm and self.use_cnn:\n",
    "                x = torch.cat([x_conv, x_lstm], dim=1)\n",
    "            elif self.use_lstm:\n",
    "                x = x_lstm\n",
    "            elif self.use_cnn:\n",
    "                x = x_conv\n",
    "            #\n",
    "\n",
    "        # linear mapping to low-dimensional space\n",
    "        x = self.mapping(x)\n",
    "\n",
    "        # generate the class protocal with dimension C * D (nclass * dim)\n",
    "        proto_list = []\n",
    "        idx_unsup_list = []\n",
    "        for i in range(self.nclass):\n",
    "            idx = (labels[idx_train].squeeze() == i).nonzero().squeeze(1)\n",
    "            ## define the (un-)supervised portion in class i\n",
    "            n_sup_i = int(idx.shape[0] * sup_ratio)\n",
    "            idx_sup_i = idx[:n_sup_i]\n",
    "            idx_unsup_i = idx[n_sup_i:]\n",
    "            idx_unsup_list.append(idx_unsup_i)\n",
    "            \n",
    "            if self.use_att:\n",
    "                #A = self.attention(x[idx_train][idx])  # N_k * 1\n",
    "                A = self.att_models[i](x[idx_train][idx_sup_i])  # N_k * 1\n",
    "                A = torch.transpose(A, 1, 0)  # 1 * N_k\n",
    "                A = F.softmax(A, dim=1)  # softmax over N_k\n",
    "                #print(A)\n",
    "                class_repr = torch.mm(A, x[idx_train][idx_sup_i]) # 1 * L\n",
    "                class_repr = torch.transpose(class_repr, 1, 0)  # L * 1\n",
    "            else:  # if do not use attention, simply use the mean of training samples with the same labels.\n",
    "                class_repr = x[idx_train][idx_sup_i].mean(0)  # L * 1\n",
    "            proto_list.append(class_repr.view(1, -1))\n",
    "        x_proto = torch.cat(proto_list, dim=0)\n",
    "        #print(x_proto)\n",
    "        #dists = euclidean_dist(x, x_proto)\n",
    "        #log_dists = F.log_softmax(-dists * 1e7, dim=1)\n",
    "\n",
    "        # prototype distance\n",
    "        proto_dists = euclidean_dist(x_proto, x_proto)\n",
    "        num_proto_pairs = int(self.nclass * (self.nclass - 1) / 2)\n",
    "        proto_dist = torch.sum(proto_dists) / num_proto_pairs\n",
    "\n",
    "        ## apply the unsupervised portion to adjust the class prototype\n",
    "        idx_unsup = torch.cat(idx_unsup_list, dim=0)\n",
    "        if self.use_ss:\n",
    "            semi_A = self.semi_att(x[idx_train][idx_unsup])  # N_test * c\n",
    "            semi_A = torch.transpose(semi_A, 1, 0)  # c * N_test\n",
    "            semi_A = F.softmax(semi_A, dim=1)  # softmax over N_test\n",
    "            x_proto_test = torch.mm(semi_A, x[idx_train][idx_unsup])  # c * L\n",
    "            x_proto = (x_proto + x_proto_test) / 2\n",
    "\n",
    "            # solution 2\n",
    "            # row_sum = 1 / torch.sum(-dists[idx_test,], dim=1)\n",
    "            # row_sum = row_sum.unsqueeze(1).repeat(1, 2)\n",
    "            # prob = torch.mul(-dists[idx_test,], row_sum)\n",
    "            # x_proto_test = torch.transpose(torch.mm(torch.transpose(x[idx_test,], 0, 1), prob), 0, 1)\n",
    "\n",
    "        dists = euclidean_dist(x, x_proto)\n",
    "\n",
    "        #dump_embedding(x_proto, torch.from_numpy())\n",
    "        dump_embedding(x_proto, x, labels)\n",
    "        return -dists, proto_dist\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
