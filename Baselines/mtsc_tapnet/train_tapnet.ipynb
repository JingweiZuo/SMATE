{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "%run model_tapnet.ipynb\n",
    "%run UEA_utils.ipynb\n",
    "\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "datasets = [\"ArticularyWordRecognition\", \"AtrialFibrilation\", \"BasicMotions\", \"CharacterTrajectories\", \"Cricket\",\n",
    "            \"EigenWorms\", \"Epilepsy\", \"ERing\", \"EthanolConcentration\", \"FingerMovements\",\n",
    "             \"HandMovementDirection\", \"Handwriting\", \"Heartbeat\", \"JapaneseVowels\", \"Libras\",\n",
    "            \"LSST\", \"MotorImagery\", \"NATOPS\", \"PEMS-SF\", \"PenDigits\",\n",
    "            \"Phoneme\", \"RacketSports\", \"SelfRegulationSCP1\", \"SelfRegulationSCP2\", \"SpokenArabicDigits\",\n",
    "            \"StandWalkJump\", \"UWaveGestureLibrary\", \"\", \"\", \"\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# dataset settings\n",
    "parser.add_argument('--data_path', type=str, default=\"./dataset/\",\n",
    "                    help='the path of data.')\n",
    "parser.add_argument('--use_muse', action='store_true', default=True,\n",
    "                    help='whether to use the raw data. Default:False')\n",
    "parser.add_argument('--dataset', type=str, default=\"ArticularyWordRecognition\", #NATOPS\n",
    "                    help='time series dataset. Options: See the datasets list')\n",
    "\n",
    "# cuda settings\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "\n",
    "# Training parameter settings\n",
    "parser.add_argument('--epochs', type=int, default=1000,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=1e-6,\n",
    "                    help='Initial learning rate. default:[0.00001]')\n",
    "parser.add_argument('--wd', type=float, default=1e-3,\n",
    "                    help='Weight decay (L2 loss on parameters). default: 5e-3')\n",
    "parser.add_argument('--stop_thres', type=float, default=1e-9,\n",
    "                    help='The stop threshold for the training error. If the difference between training losses '\n",
    "                         'between epoches are less than the threshold, the training will be stopped. Default:1e-9')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--use_lstm', type=boolean_string, default=True,\n",
    "                    help='whether to use LSTM for feature extraction. Default:False')\n",
    "parser.add_argument('--use_cnn', type=boolean_string, default=True,\n",
    "                    help='whether to use CNN for feature extraction. Default:False')\n",
    "parser.add_argument('--use_rp', type=boolean_string, default=True,\n",
    "                    help='Whether to use random projection')\n",
    "parser.add_argument('--rp_params', type=str, default='-1,3',\n",
    "                    help='Parameters for random projection: number of random projection, '\n",
    "                         'sub-dimension for each random projection')\n",
    "parser.add_argument('--use_metric', action='store_true', default=False,\n",
    "                    help='whether to use the metric learning for class representation. Default:False')\n",
    "parser.add_argument('--metric_param', type=float, default=0.000001,\n",
    "                    help='Metric parameter for prototype distances between classes. Default:0.000001')\n",
    "parser.add_argument('--use_ss', action='store_true', default=True,\n",
    "                    help='Use semi-supervised learning.')\n",
    "parser.add_argument('--sup_ratio', type=float, default=0.5,\n",
    "                    help='Supervised ratio for labeled data in training set')\n",
    "parser.add_argument('--filters', type=str, default=\"256,256,128\",\n",
    "                    help='filters used for convolutional network. Default:256,256,128')\n",
    "parser.add_argument('--kernels', type=str, default=\"8,5,3\",\n",
    "                    help='kernels used for convolutional network. Default:8,5,3')\n",
    "parser.add_argument('--dilation', type=int, default=200,\n",
    "                    help='the dilation used for the first convolutional layer. '\n",
    "                         'If set to -1, use the automatic number. Default:-1')\n",
    "parser.add_argument('--layers', type=str, default=\"500,300\",\n",
    "                    help='layer settings of mapping function. [Default]: 500,300')\n",
    "parser.add_argument('--dropout', type=float, default=0,\n",
    "                    help='Dropout rate (1 - keep probability). Default:0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parser.parse_args()\n",
    "sup_ratio = 0.5\n",
    "\n",
    "dataset = 'Heartbeat'\n",
    "ds_path = '../../Datasets/MTS-UEA/'\n",
    "rep_ds_train = ds_path + dataset + \"/output_train/\"\n",
    "rep_ds_test = ds_path + dataset + \"/output_test/\"\n",
    "meta_csv = \"meta_data.csv\"  # the meta data of training/testing set\n",
    "data_collect = get_UEA_dataset(rep_ds_train, rep_ds_test, meta_csv, sup_ratio, mode='load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_collect['X_train'] # N * L * D\n",
    "y_train = data_collect['Y_train']\n",
    "x_test = data_collect['X_test']\n",
    "y_test = data_collect['Y_test']\n",
    "nclass = data_collect['n_classes']\n",
    "features = torch.Tensor(np.concatenate([x_train, x_test], axis=0)) # N * L * D\n",
    "features = features.permute(0, 2, 1)\n",
    "labels = torch.LongTensor(np.concatenate([y_train, y_test], axis=0))\n",
    "idx_train = torch.LongTensor(np.arange(x_train.shape[0]))\n",
    "idx_test = torch.LongTensor(idx_train + x_train.shape[0])\n",
    "idx_val = idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args = ['--data_path', ds_path,\n",
    "                                '--dataset', dataset])\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "args.sparse = True\n",
    "args.layers = [int(l) for l in args.layers.split(\",\")]\n",
    "args.kernels = [int(l) for l in args.kernels.split(\",\")]\n",
    "args.filters = [int(l) for l in args.filters.split(\",\")]\n",
    "args.rp_params = [int(l) for l in args.rp_params.split(\",\")]\n",
    "\n",
    "if not args.use_lstm and not args.use_cnn:\n",
    "    print(\"Must specify one encoding method: --use_lstm or --use_cnn\")\n",
    "    print(\"Program Exiting.\")\n",
    "    exit(-1)\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(args.__dict__.items()):\n",
    "    print(\"\\t{}={}\".format(attr.upper(), value))\n",
    "\n",
    "\n",
    "# Load data\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "print(\"Loading dataset\", args.dataset, \"...\")\n",
    "# Model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"TapNet\"  # Options: FGCN, ProtoGCN, BiGCN, MotifGCN, InterGCN, TPNet, TapNet\n",
    "if model_type == \"TapNet\":\n",
    "\n",
    "    # update random permutation parameter\n",
    "    if args.rp_params[0] < 0:\n",
    "        # dim = features.shape[1]\n",
    "        # if dim <= 6:\n",
    "        #     args.rp_params = [dim, math.ceil(dim / 2)]\n",
    "        # elif dim > 6 and dim <= 20:\n",
    "        #     args.rp_params = [10, 3]\n",
    "        # else:\n",
    "        #     args.rp_params = [int(dim / 2), 3]\n",
    "        dim = features.shape[1]\n",
    "        args.rp_params = [3, math.floor(dim * 2 / 3)]\n",
    "\n",
    "    print(\"rp_params:\", args.rp_params)\n",
    "\n",
    "    # update dilation parameter\n",
    "    if args.dilation == -1:\n",
    "        args.dilation = math.floor(features.shape[2] / 64)\n",
    "\n",
    "\n",
    "    model = TapNet(nfeat=features.shape[1],\n",
    "                   len_ts=features.shape[2],\n",
    "                   n_train = idx_train.shape[0],\n",
    "                   layers=args.layers,\n",
    "                   nclass=nclass,\n",
    "                   dropout=args.dropout,\n",
    "                   use_lstm=args.use_lstm,\n",
    "                   use_cnn=args.use_cnn,\n",
    "                   filters=args.filters,\n",
    "                   dilation=args.dilation,\n",
    "                   kernels=args.kernels,\n",
    "                   use_ss=args.use_ss,\n",
    "                   sup_ratio=args.sup_ratio, \n",
    "                   use_metric=args.use_metric,\n",
    "                   use_rp=args.use_rp,\n",
    "                   rp_params=args.rp_params\n",
    "                   )\n",
    "    print(\"number of training samples is:\", idx_train.shape[0])\n",
    "    # cuda\n",
    "    '''if args.cuda:\n",
    "        model.cuda()\n",
    "        features, labels, idx_train = features.cuda(), labels.cuda(), idx_train.cuda()\n",
    "    '''\n",
    "    input = (features, labels, idx_train, idx_val, idx_test)\n",
    "\n",
    "# init the optimizer\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.wd)\n",
    "\n",
    "\n",
    "idx_sup_list = []\n",
    "for i in range(nclass):\n",
    "    idx = (labels[idx_train].squeeze() == i).nonzero().squeeze(1)\n",
    "    ## define the (un-)supervised portion in class i\n",
    "    n_sup_i = int(idx.shape[0] * sup_ratio)\n",
    "    idx_sup_i = idx[:n_sup_i]\n",
    "    idx_unsup_i = idx[n_sup_i:]\n",
    "    idx_sup_list.append(idx_sup_i)\n",
    "\n",
    "idx_sup = torch.cat(idx_sup_list, dim=0)\n",
    "\n",
    "# training function\n",
    "def train():\n",
    "    loss_list = [sys.maxsize]\n",
    "    test_best_possible, best_so_far = 0.0, sys.maxsize\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #new_input = (features[idx_train, ], labels[idx_train], idx_train, idx_val, idx_test)\n",
    "        output, proto_dist = model(input)\n",
    "        # print(features[idx_train])\n",
    "        # print(output[idx_train])\n",
    "\n",
    "        loss_train = F.cross_entropy(output[idx_train][idx_sup], torch.squeeze(labels[idx_train][idx_sup]))\n",
    "        if args.use_metric:\n",
    "            loss_train = loss_train - args.metric_param * proto_dist\n",
    "\n",
    "        if abs(loss_train.item() - loss_list[-1]) < args.stop_thres \\\n",
    "                or loss_train.item() > loss_list[-1]:\n",
    "            break\n",
    "        else:\n",
    "            loss_list.append(loss_train.item())\n",
    "\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if not args.fastmode:\n",
    "        #     # Evaluate validation set performance separately,\n",
    "        #     # deactivates dropout during validation run.\n",
    "        #     model.eval()\n",
    "        #     output = model(features)\n",
    "\n",
    "        #print(output[idx_val])\n",
    "        loss_val = F.cross_entropy(output[idx_val], torch.squeeze(labels[idx_val]))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        # print(output[idx_val])\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "              'loss_train: {:.8f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "        if acc_val.item() > test_best_possible:\n",
    "            test_best_possible = acc_val.item()\n",
    "        if best_so_far > loss_train.item():\n",
    "            best_so_far = loss_train.item()\n",
    "            test_acc = acc_val.item()\n",
    "    print(\"test_acc: \" + str(test_acc))\n",
    "    print(\"best possible: \" + str(test_best_possible))\n",
    "\n",
    "    \n",
    "# test function\n",
    "def test():\n",
    "    output, proto_dist = model(input)\n",
    "    #print(output[idx_test])\n",
    "    loss_test = F.cross_entropy(output[idx_test], torch.squeeze(labels[idx_test]))\n",
    "    if args.use_metric:\n",
    "        loss_test = loss_test - args.metric_param * proto_dist\n",
    "\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(args.dataset, \"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
