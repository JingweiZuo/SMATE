{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic utils\n",
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Data Normalization for each dimension\n",
    "def z_normalization(mts):\n",
    "    M = len(mts[0, :])\n",
    "    for i in range(M):\n",
    "        mts_i = mts[:, i]\n",
    "        mean = np.mean(mts_i)\n",
    "        std = np.std(mts_i)\n",
    "        mts_i = (mts_i - mean) / (std + 10**(-8))\n",
    "        mts[:, i] = mts_i\n",
    "    return mts\n",
    "\n",
    "# %% Min Max Normalizer\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "def get_mapping_c_l(rep_train, meta_csv):\n",
    "    '''\n",
    "    Convert the classes in dataset into training labels in Keras\n",
    "\n",
    "    class_array: an array of classes for samples in dataset\n",
    "    '''\n",
    "\n",
    "    meta = np.genfromtxt(rep_train + meta_csv, delimiter=',', dtype=str, encoding=\"utf8\")\n",
    "    No = len(meta)\n",
    "    class_array = meta[:, 1]  # the 2nd column in meta_csv is an array of classes\n",
    "    classes, counts_cl = np.unique(class_array, return_counts=True)\n",
    "    print(\"class list is \" + str(classes))\n",
    "\n",
    "    mapping_c_l = {}  # a mappling between classes and labels\n",
    "    for idx, c in enumerate(list(classes)):\n",
    "        mapping_c_l.update({c: idx})\n",
    "    return mapping_c_l\n",
    "\n",
    "\n",
    "# split labeled and unlabeled dataset\n",
    "\n",
    "def split_dataset(X, X_s, masking, Y, sup_ratio, strategy='RandomSplit'):\n",
    "    '''\n",
    "    Objective: by supervised ratio, split the samples for supervised and unsupervised training\n",
    "\n",
    "    :param X: a 3-D array: Nbr_samples x L x D\n",
    "    :param X_s: a 5-D array: Nbr_samples x L x D x D x Chl\n",
    "    :param masking: a 3-D array: Nbr_samples x L x 1\n",
    "    :param Y: an 1-D array\n",
    "    :param sup_ratio: the ratio of supervised samples\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    X_list_label, X_s_list_label, masking_list_label, Y_list_label = list(), list(), list(), list()\n",
    "    X_list_unlabel, X_s_list_unlabel, masking_list_unlabel, Y_list_unlabel = list(), list(), list(), list()\n",
    "\n",
    "    # compute the number of samples with labels to take\n",
    "    n_samples = int(sup_ratio * len(Y))\n",
    "    classes, counts_cl = np.unique(Y, return_counts=True)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    if sup_ratio == 1:\n",
    "        return X, X, masking, Y, np.array([]), np.array([]), np.array([]), np.array([]), n_classes\n",
    "    \n",
    "    if strategy == 'RandomSplit':\n",
    "\n",
    "        ## not equal-split between classes\n",
    "        idx_label = rd.sample(range(0, X.shape[0]), n_samples)\n",
    "        X_sup = X[idx_label]\n",
    "        X_s_sup = X_s[idx_label]\n",
    "        masking_sup = masking[idx_label]\n",
    "        Y_sup = Y[idx_label]\n",
    "        # put the rest of the samples in the class in unlabeled samples\n",
    "        idx_unlabel = np.array(range(len(Y)))  # the total number of instance in the class\n",
    "        idx_unlabel = np.ma.array(idx_unlabel, mask=False)\n",
    "        for i in idx_label:\n",
    "            idx_unlabel.mask[i] = True\n",
    "        idx_unlabel = idx_unlabel.compressed()  # a list of index for unlabeled samples\n",
    "\n",
    "        X_unsup = X[idx_unlabel]\n",
    "        X_s_unsup = X_s[idx_unlabel]\n",
    "        masking_unsup = masking[idx_unlabel]\n",
    "        Y_unsup = Y[idx_unlabel]\n",
    "\n",
    "        return X_sup, X_s_sup, masking_sup, Y_sup, X_unsup, X_s_unsup, masking_unsup, Y_unsup, n_classes\n",
    "\n",
    "    else:\n",
    "        ## equal-split between classes\n",
    "\n",
    "        n_per_class = int(n_samples / n_classes)\n",
    "        for c in classes:\n",
    "            idx_c = np.where(Y == c)[0]  # take an array without 'dtype'\n",
    "            # get all samples for this class\n",
    "            X_c = [X[i] for i in idx_c]\n",
    "            X_s_c = [X_s[i] for i in idx_c]\n",
    "            masking_c = [masking[i] for i in idx_c]\n",
    "            # choose random instances\n",
    "            if (n_per_class > len(X_c)):\n",
    "                idx_label = range(0, len(X_c))\n",
    "            else:\n",
    "                idx_label = rd.sample(range(0, len(X_c)), n_per_class)\n",
    "            X_label = [X_c[i] for i in idx_label]\n",
    "            X_s_label = [X_s_c[i] for i in idx_label]\n",
    "            masking_label = [masking_c[i] for i in idx_label]\n",
    "            X_list_label.extend(X_label)\n",
    "            X_s_list_label.extend(X_s_label)\n",
    "            masking_list_label.extend(masking_label)\n",
    "            Y_list_label.extend([c] * len(idx_label))\n",
    "\n",
    "            # put the rest of the samples in the class in unlabeled samples\n",
    "            new_idx_c = np.array(range(len(idx_c)))  # the total number of instance in the class\n",
    "            new_idx_c = np.ma.array(new_idx_c, mask=False)\n",
    "            for i in idx_label:\n",
    "                new_idx_c.mask[i] = True\n",
    "            idx_unlabel = new_idx_c.compressed()  # a list of index for unlabeled samples\n",
    "\n",
    "            X_unlabel = [X_c[i] for i in idx_unlabel]\n",
    "            X_s_unlabel = [X_s_c[i] for i in idx_unlabel]\n",
    "            masking_unlabel = [masking_c[i] for i in idx_unlabel]\n",
    "\n",
    "            X_list_unlabel.extend(X_unlabel)\n",
    "            X_s_list_unlabel.extend(X_s_unlabel)\n",
    "            masking_list_unlabel.extend(masking_unlabel)\n",
    "            Y_list_unlabel.extend([c] * len(idx_unlabel))\n",
    "\n",
    "            X_sup = np.array(X_list_label)\n",
    "            X_s_sup = np.array(X_s_list_label)\n",
    "            masking_sup = np.array(masking_list_label)\n",
    "            Y_sup = np.asarray(Y_list_label).flatten()\n",
    "\n",
    "            X_unsup = np.array(X_list_unlabel)\n",
    "            X_s_unsup = np.array(X_s_list_unlabel)\n",
    "            masking_unsup = np.array(masking_list_unlabel)\n",
    "            Y_unsup = np.asarray(Y_list_unlabel).flatten()\n",
    "\n",
    "        return X_sup, X_s_sup, masking_sup, Y_sup, X_unsup, X_s_unsup, masking_unsup, Y_unsup, n_classes\n",
    "\n",
    "\n",
    "# select real samples from dataset\n",
    "def load_real_samples(X, Y, n_samples):\n",
    "    '''\n",
    "\n",
    "    :param X: a list of 2-D array for samples\n",
    "    :param Y: an 1-D array of class labels\n",
    "    :param n_samples:   the number of samples to be load\n",
    "    :return:\n",
    "    [X_samples, Y_samples]: the randomly selected samples and class labels\n",
    "    y_virtual: Fake / Real label\n",
    "\n",
    "    '''\n",
    "    # choose random instances\n",
    "    idx = rd.sample(range(0, len(X)), n_samples)\n",
    "    X_samples = [X[i] for i in idx]  # X is a list of array\n",
    "    Y_samples = Y[idx]  # Y should be an array\n",
    "\n",
    "    # generate real/fake class labels\n",
    "    y_virtual = np.ones((n_samples, 1))\n",
    "\n",
    "    return [X_samples, Y_samples], y_virtual\n",
    "\n",
    "\n",
    "# Compute the Correlation between dimensions\n",
    "def mtx_correlation(X, channels):\n",
    "    '''\n",
    "\n",
    "    :param X:   Input list of 2-D array, N x L\n",
    "    :param channels:    the channels at each time stamps\n",
    "    :return:    A list of 4-D array, L x N x N x nbr_chl\n",
    "    '''\n",
    "    import time\n",
    "    mtx_corr_list = list()\n",
    "    print(\"total number of samples is \" + str(len(X)))\n",
    "\n",
    "    start = time.time()\n",
    "    for idx, x in enumerate(X):\n",
    "        L = x.shape[0]  # the length of MTS\n",
    "        N = x.shape[1]  # the number of MTS dimension\n",
    "        mtx_corr = np.zeros((L, N, N, len(channels)))\n",
    "\n",
    "        for l in range(L):\n",
    "            # for each time stamp in MTS\n",
    "            mtx_corr_l = np.zeros((N, N, len(channels)))\n",
    "            for c_idx, c in enumerate(channels):\n",
    "                # c is the size of MTS segment\n",
    "                # each channel, output a\n",
    "                # keep the same length of each correlation sequence\n",
    "                if (l < c):  # for the first MTS segments\n",
    "                    mts_seg = x[:l, :]\n",
    "                else:\n",
    "                    mts_seg = x[l - c:l, :]  # split a MTS segment, mts_seg.shape = c x N\n",
    "                mts_ij = np.corrcoef(np.transpose(mts_seg[:, :]))  # input N x c matrice, output N x N matrice,\n",
    "                mts_ij = np.nan_to_num(mts_ij, nan=0)  # may have 'nan' value in the  matrice as the segment may be plat\n",
    "\n",
    "                mtx_corr_l[:, :, c_idx] = mts_ij\n",
    "            mtx_corr[l, :, :, :] = mtx_corr_l\n",
    "        mtx_corr_list.append(mtx_corr)\n",
    "        if idx % 100 == 0:\n",
    "            print('time cost until round ' + str(idx) + ' is ' + str(time.time() - start))\n",
    "    return mtx_corr_list\n",
    "\n",
    "\n",
    "# Get the max length of MTS samples\n",
    "def get_max_seq_len(X):\n",
    "    '''\n",
    "\n",
    "    :param X: A list of MTS samples (2-D array: length x dim)\n",
    "    :return: the max length of MTS samples\n",
    "    '''\n",
    "    No = len(X)\n",
    "    Max_Seq_Len = 0\n",
    "    for i in range(No):\n",
    "        Max_Seq_Len = max(Max_Seq_Len, X[i].shape[0])\n",
    "    return Max_Seq_Len\n",
    "\n",
    "\n",
    "# Padding the MTS batch into identical length (i.g., Max_Seq_Len)\n",
    "def padding_variable_length(X_samples, Max_Seq_Len):\n",
    "    '''\n",
    "\n",
    "    :param X_samples: a batch/list of samples 2-D array: length x dim\n",
    "    :return:\n",
    "        - Xpad: A 3-D array of No x length x dim\n",
    "        - L_samples: a list of length of initial samples\n",
    "\n",
    "    '''\n",
    "    print('total number of samples is ' + str(len(X_samples)))\n",
    "    No = len(X_samples)\n",
    "    dimension = len(X_samples[0][0, :])\n",
    "    L_samples = list()\n",
    "    for i in range(No):\n",
    "        L_samples.append(X_samples[i].shape[0])\n",
    "    # Padding and Masking\n",
    "    special_value = 0\n",
    "    Xpad = np.zeros((No, Max_Seq_Len, dimension))\n",
    "\n",
    "    for s, x in enumerate(X_samples):\n",
    "        seq_len = x.shape[0]\n",
    "        Xpad[s, 0:seq_len, :] = x\n",
    "\n",
    "    return Xpad, L_samples\n",
    "\n",
    "\n",
    "# Padding the Correlation matrix into identical length\n",
    "def padding_corr_matrix(mtx_corr_list, Max_Seq_Len):\n",
    "    '''\n",
    "\n",
    "    :param mtx_corr_list: List of 4-D array \"L  x N x N x Chl\"\n",
    "    :return:\n",
    "        - mtx_corr_pad: A 5-D array \"No x L x N x N x Chl  \"\n",
    "\n",
    "    '''\n",
    "    No = len(mtx_corr_list)\n",
    "    Chl = mtx_corr_list[0].shape[-1]\n",
    "    dimension = mtx_corr_list[0].shape[1]\n",
    "\n",
    "    # Padding and Masking\n",
    "    mtx_corr_pad = np.zeros((No, Max_Seq_Len, dimension, dimension, Chl))\n",
    "    for s, x in enumerate(mtx_corr_list):\n",
    "        seq_len = x.shape[0]\n",
    "        mtx_corr_pad[s, 0:seq_len, :, :, :] = x\n",
    "    return mtx_corr_pad\n",
    "\n",
    "\n",
    "# get the (padded) temporal and spatia samples, as well as masking array\n",
    "def generate_real_samples(X, X_s, masking, Y, n_samples):\n",
    "    # choose random instances\n",
    "    if (n_samples >= X.shape[0]):\n",
    "        n_samples = X.shape[0]\n",
    "    idx = rd.sample(range(0, len(X)), n_samples)\n",
    "    # X_samples = [X[i] for i in idx] #X is a list of array N_samples * 'L x D Chl'\n",
    "    # X_s_samples = [X_s[i] for i in idx] #X_s is a list of array N_samples * 'L x D x D x Chl'\n",
    "    X_samples = X[idx]  # X is an array of N_samples * 'L x D Chl'\n",
    "    X_s_samples = X_s[idx]  # X_s is an array of N_samples * 'L x D x D x Chl'\n",
    "\n",
    "    masking_samples = masking[idx]  # masking should be an array of 'N_samples x L x 1'\n",
    "    Y_samples = Y[idx]  # Y should be an array of 'N_samples'\n",
    "\n",
    "    # generate real/fake class labels\n",
    "    # real sample: -1\n",
    "    # fake sample: 1\n",
    "    y_virtual = np.ones((n_samples, 1))\n",
    "\n",
    "    return [X_samples, X_s_samples, masking_samples, Y_samples], y_virtual\n",
    "\n",
    "\n",
    "# generate random arrays with predefined data dimensions\n",
    "def random_generator(batch_size, data_dim, masking_seq, Max_Seq_Len):\n",
    "    '''\n",
    "        Create a 3-D array where each sample has different length, the extra parts are filled by 0\n",
    "\n",
    "    :param batch_size: number of noise samples\n",
    "    :param data_dim: the dimension number of each sample\n",
    "    :param masking_seq: the mask sequence to mark the length of the sequences\n",
    "    :param Max_Seq_Len: the max sequence length\n",
    "    :return:   A 3-D array\n",
    "    '''\n",
    "\n",
    "    Zs = np.random.uniform(0., 1, [batch_size, Max_Seq_Len, data_dim])\n",
    "    Zs = np.multiply(Zs, masking_seq)\n",
    "    '''\n",
    "    Zs = np.zeros([batch_size, Max_Seq_Len, z_dim])\n",
    "    for i in range(batch_size):\n",
    "        Z = np.random.uniform(0., 1, [Max_Seq_Len, data_dim])\n",
    "\n",
    "        Zs[i, :L[i], :] = Z'''\n",
    "\n",
    "    return Zs\n",
    "\n",
    "\n",
    "# generate fake representation from random noise\n",
    "def generate_fake_reprs(generator, batch_size, data_dim, L, masking_seq):\n",
    "    # generate MTS points in latent space\n",
    "    Z = random_generator(batch_size, data_dim, L)\n",
    "    # predict outputs\n",
    "    H_fake = generator.predict([Z, masking_seq])\n",
    "    # create class labels\n",
    "    Y_fake = np.zeros((batch_size, 1))\n",
    "    return H_fake, Y_fake\n",
    "\n",
    "# euclidean distance between two arrays\n",
    "def euclidean_dist(x, y):\n",
    "    # x: n * d\n",
    "    # y: m * d\n",
    "    n = x.shape[0]\n",
    "    d = x.shape[1]\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    assert d == y.shape[1]\n",
    "\n",
    "    x = K.repeat(x, m) # n * m * d\n",
    "    y = K.expand_dims(y, axis=0) # 1 * m * d\n",
    "    #y = Lambda(lambda t: K.gather(t, [0] * n))(y)\n",
    "    return K.sum(K.pow(x-y, 2), axis = 2) # n * m\n",
    "\n",
    "def euclidean_dist_mts(x, y):\n",
    "    # x: n * L * d\n",
    "    # y: m * L * d\n",
    "    n = x.shape[0]\n",
    "    l = x.shape[1]\n",
    "    d = x.shape[2]\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    assert d == y.shape[2]\n",
    "    \n",
    "    x = K.reshape(x, shape=(n, l*d))\n",
    "    y = K.reshape(y, shape=(m, l*d))\n",
    "\n",
    "    x = K.repeat(x, m) # n * m * d'\n",
    "    y = K.expand_dims(y, axis=0) # 1 * m * d'\n",
    "    #y = Lambda(lambda t: K.gather(t, [0] * n))(y)\n",
    "    return K.sum(K.pow(x-y, 2), axis = 2) # n * m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading MTS data of UEA\n",
    "\n",
    "import time, functools\n",
    "\n",
    "'''=================================================== Prepare data import (MTS-UEA) ========================================================'''\n",
    "\n",
    "\n",
    "channel_list = [10, 15, 20]\n",
    "channel_str = functools.reduce(lambda x,y:str(x)+'_' + str(y), channel_list) #'10_15_20'\n",
    "spatial_corr_file = 'spatial_corr_' + channel_str + '.npz'\n",
    "#spatial_corr_file = 'spatial_corr.npz' # test on existing files\n",
    "\n",
    "\n",
    "def import_data_UEA(rep, meta_csv, mapping_c_l):\n",
    "    '''\n",
    "\n",
    "    :param rep: the repository of dataset files\n",
    "    :param meta_csv: the meta-data of dataset\n",
    "    :return:\n",
    "        X_train: list of 2-D array, length * dimension\n",
    "        Labels: An 1-D array of labels\n",
    "    '''\n",
    "    meta = np.genfromtxt(rep + meta_csv, delimiter=',', dtype=str, encoding=\"utf8\")\n",
    "    No = len(meta)\n",
    "    Names = meta[:, 0]\n",
    "    class_array = meta[:, 1]\n",
    "    L = class_array.shape[0]\n",
    "    Labels = np.zeros(L)\n",
    "\n",
    "    i = 0\n",
    "    for c in class_array:  # get keras labels\n",
    "        Labels[i] = mapping_c_l[c]\n",
    "        i = i + 1\n",
    "\n",
    "    X_train = list()\n",
    "\n",
    "    for k in range(No):\n",
    "        dsName = Names[k]\n",
    "        raw_data = np.genfromtxt(rep + dsName + '.csv', delimiter=',', encoding=\"utf8\",\n",
    "                                 filling_values=0)  # No header in raw files\n",
    "        raw_data = raw_data.astype(np.float32)  # remove timestamp and the last row (label)\n",
    "        data = z_normalization(raw_data)\n",
    "        data = MinMaxScaler(data)\n",
    "\n",
    "        '''\n",
    "            Just for CharacterTrajectories, the end of MTS is all O\n",
    "        \n",
    "        L_seq = 0\n",
    "        for idx in range(raw_data.shape[0]):\n",
    "            if raw_data[idx].all() == 0:\n",
    "                L_seq = idx\n",
    "                break\n",
    "\n",
    "        data[L_seq:] = np.zeros_like(data[L_seq:])'''\n",
    "\n",
    "        # As the samples may not have the same length, then we put them into a list\n",
    "        X_train.append(data)\n",
    "    #print(Labels)\n",
    "    return X_train, Labels  # X_train is a list of 2-D array, length*dimension\n",
    "\n",
    "def prepare_data_UEA(rep, meta_csv, mapping_c_l, mode='load'):  # mode = 'load'/'save' samples' spatial correlation from/to disk\n",
    "    # Load meta-data about the dataset\n",
    "\n",
    "    X_list, Y = import_data_UEA(rep, meta_csv,\n",
    "                                mapping_c_l)  # a list of 2-D array, length*dimension; an 1-D array of labels\n",
    "\n",
    "    Max_Seq_Len = get_max_seq_len(X_list)\n",
    "    X, L = padding_variable_length(X_list, Max_Seq_Len)  # Padding the samples into an identical length\n",
    "\n",
    "    ds_size = X.shape[0]\n",
    "    maskings = np.zeros((ds_size, Max_Seq_Len, 1))  # padding the spatial correlation matrix\n",
    "    for idx in range(ds_size):\n",
    "        l_seq = L[idx]\n",
    "        maskings[idx, :l_seq, :] = np.ones((l_seq, 1))\n",
    "\n",
    "    start = time.time()  # counting the time of computing spatial correlation\n",
    "\n",
    "    '''if mode == 'save':\n",
    "        X_s_unpad = mtx_correlation(X_list, channel_list)  # calculate spatial input\n",
    "        X_s = padding_corr_matrix(X_s_unpad, Max_Seq_Len)\n",
    "        # persist the intermediate array into disque\n",
    "        np.savez_compressed(rep + spatial_corr_file, X_s)  # 45 kb VS 4 kb\n",
    "    else:\n",
    "        X_s = np.load(rep + spatial_corr_file)['arr_0']'''\n",
    "    X_s = X\n",
    "    # print(x_s[0, 20, :, :, 0]) # retrieve the 1st sample: mtx_corrs[0]\n",
    "\n",
    "    end_corr = time.time()\n",
    "    #print('time cost for computing spatial loading/correlation : ' + str(end_corr - start))\n",
    "    \n",
    "    return X, X_s, maskings, Y\n",
    "\n",
    "def get_UEA_dataset(rep_ds_train, rep_ds_test, meta_csv, sup_ratio, mode = 'load', split_strategy='EqualSplit'):\n",
    "    start = time.time()\n",
    "    mapping_c_l = get_mapping_c_l(rep_ds_train, meta_csv)\n",
    "    X_train, X_s_train, masking_train, Y_train = prepare_data_UEA(rep_ds_train, meta_csv, mapping_c_l, mode)\n",
    "    end_train = time.time()\n",
    "    #print('time cost for getting training data  : ' + str(end_train - start))\n",
    "    X_test, X_s_test, masking_test, Y_test = prepare_data_UEA(rep_ds_test, meta_csv, mapping_c_l, mode)\n",
    "    end_test = time.time()\n",
    "    #print('time cost for getting testing data  : ' + str(end_test - end_train))\n",
    "    X_sup, X_s_sup, masking_sup, Y_sup, X_unsup, X_s_unsup, masking_unsup, Y_unsup, n_classes = split_dataset(X_train,\n",
    "                                                                                                          X_s_train,\n",
    "                                                                                                    masking_train,\n",
    "                                                                                                          Y_train,\n",
    "                                                                                                          sup_ratio,\n",
    "                                                                                                            split_strategy)  # split the training set into labeled and unlabed samples\n",
    "    end_split = time.time()\n",
    "    #print('time cost for getting splitting data  : ' + str(end_split - end_test))\n",
    "    dataset = {}\n",
    "    dataset.update({'X_train': X_train})\n",
    "    dataset.update({'X_test': X_test})\n",
    "    dataset.update({'X_s_train': X_s_train})\n",
    "    dataset.update({'X_s_test': X_s_test})\n",
    "    dataset.update({'masking_train': masking_train})\n",
    "    dataset.update({'masking_test': masking_test})\n",
    "    dataset.update({'Y_train': Y_train})\n",
    "    dataset.update({'Y_test': Y_test})\n",
    "    dataset.update({'X_sup': X_sup})\n",
    "    dataset.update({'X_s_sup': X_s_sup})\n",
    "    dataset.update({'masking_sup': masking_sup})\n",
    "    dataset.update({'Y_sup': Y_sup})\n",
    "    dataset.update({'X_unsup': X_unsup})\n",
    "    dataset.update({'X_s_unsup': X_s_unsup})\n",
    "    dataset.update({'masking_unsup': masking_unsup})\n",
    "    dataset.update({'Y_unsup': Y_unsup})\n",
    "    dataset.update({'n_classes': n_classes})\n",
    "\n",
    "    return dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
