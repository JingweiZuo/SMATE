{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Unsupervised Scalable Representation Learning for MTS (USRL)](https://github.com/White-Link/UnsupervisedScalableRepresentationLearningTimeSeries)**\n",
    "===\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Requirements\n",
    "\n",
    "Experiments were done with the following package versions for Python 3.6:\n",
    "\n",
    "    Numpy (numpy) v1.15.2;\n",
    "    Matplotlib (matplotlib) v3.0.0;\n",
    "    Orange (Orange) v3.18.0;\n",
    "    Pandas (pandas) v0.23.4;\n",
    "    python-weka-wrapper3 v0.1.6 for multivariate time series (requires Oracle JDK 8 or OpenJDK 8);\n",
    "    PyTorch (torch) v0.4.1 with CUDA 9.0;\n",
    "    Scikit-learn (sklearn) v0.20.0;\n",
    "    Scipy (scipy) v1.1.0.\n",
    "    \n",
    "## Problems\n",
    "\n",
    "The pretrained models are missing. Then it's required to retrain the encoder models.\n",
    "In file \"scikit_wrappers.py\", the virtual class \"TimeSeriesEncoderClassifier\" and the sub-class \"CausalCNNEncoderClassifier\" should be considered. The exact encoder structure is defined in \"CausalCNNEncoderClassifier\", so the \"fit\" should be called to train the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format conversion for USRL \n",
    "===\n",
    "\n",
    "       \n",
    "\n",
    "---\n",
    "Input\n",
    "---\n",
    "\n",
    "A single file contains all samples and their labels: ***L * (3 + D)***\n",
    "\n",
    "\n",
    "\n",
    "- 1st col: sample_id\n",
    "- 2nd col: timestamps\n",
    "- 3rd col: label\n",
    "- after the 4th col: mts vector with D dimensions   \n",
    "\n",
    "---\n",
    "Output\n",
    "---\n",
    "\n",
    "Two array-like variables\n",
    "\n",
    "- X : array with shape (n_ts, d, sz)\n",
    "        Sequence data.\n",
    "- y : array with shape (n_ts, 1)\n",
    "        Target labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = \"./datasets/multivariate/\"\n",
    "ds = \"ECG\"\n",
    "ds_train = ds + '/' + ds + \"_TRAIN3\"\n",
    "ds_test = ds + '/' + ds + \"_TEST3\"\n",
    "\n",
    "NB_CLASS = 0\n",
    "MAX_TIMESTEPS = 0\n",
    "MAX_NB_VARIABLES = 0\n",
    "\n",
    "def z_normalization(mts):\n",
    "    M = len(mts[0, :])\n",
    "    for i in range(M):\n",
    "        mts_i = mts[:, i]\n",
    "        mean = np.mean(mts_i)\n",
    "        std = np.std(mts_i)\n",
    "        mts_i = (mts_i - mean) / std\n",
    "        mts[:, i] = mts_i\n",
    "    return mts\n",
    "\n",
    "def convert_mts(rep, dataset, z_normal = False):\n",
    "    global NB_CLASS, MAX_NB_VARIABLES\n",
    "    \n",
    "    seq = np.genfromtxt(rep + dataset, delimiter=' ', dtype=str, encoding=\"utf8\")\n",
    "    \n",
    "    ids, counts = np.unique(seq[:,0], return_counts=True)\n",
    "    No = ids.shape[0]\n",
    "    D = seq.shape[1] - 3\n",
    "    arr = np.asarray((ids, counts)).T\n",
    "    Max_Seq_Len = np.max(arr[:,1].astype(np.int))\n",
    "    out_X = np.zeros((No, D, Max_Seq_Len))\n",
    "    out_Y = np.zeros((No, ))\n",
    "\n",
    "    classes = np.unique(seq[:,2])\n",
    "    NB_CLASS = classes.shape[0]\n",
    "    MAX_NB_VARIABLES = D\n",
    "    \n",
    "    for idx, id in enumerate(ids):\n",
    "        seq_cpy = seq[seq[:,0] == id]\n",
    "        l_seq = seq_cpy.shape[0]\n",
    "        out_X[idx, :, :l_seq] = np.transpose(seq_cpy[:, 3:])\n",
    "        out_Y[idx] = seq_cpy[0, 2] \n",
    "        if z_normal: \n",
    "            out_X[idx, :, :l_seq] = np.transpose(z_normalization(np.transpose(out_X[idx, :, :l_seq])))\n",
    "        \n",
    "    return out_X, out_Y\n",
    "\n",
    "def load_datasets(rep, ds_train, ds_test, z_normal = False):\n",
    "    global MAX_TIMESTEPS\n",
    "    X_train, y_train = convert_mts(rep, ds_train, z_normal)\n",
    "    X_test, y_test = convert_mts(rep, ds_test, z_normal)\n",
    "    if X_train.shape[-1] != X_test.shape[-1]:\n",
    "        MAX_TIMESTEPS = min(X_train.shape[-1], X_test.shape[-1])\n",
    "        X_train = X_train[:,:,:MAX_TIMESTEPS]\n",
    "        X_test = X_test[:,:,:MAX_TIMESTEPS]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_datasets(rep, ds_train, ds_test, z_normal = True)\n",
    "\n",
    "# -> To input into USRL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USRL model building \n",
    "\n",
    "    USRL applies UEA archive datasets by default\n",
    "    \n",
    "    To modify the input data format for other datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import numpy\n",
    "import argparse\n",
    "import weka.core.jvm\n",
    "import weka.core.converters\n",
    "\n",
    "import scikit_wrappers\n",
    "\n",
    "\n",
    "def load_UEA_dataset(path, dataset):\n",
    "    \"\"\"\n",
    "    Loads the UEA dataset given in input in numpy arrays.\n",
    "\n",
    "    @param path Path where the UCR dataset is located.\n",
    "    @param dataset Name of the UCR dataset.\n",
    "\n",
    "    @return Quadruplet containing the training set, the corresponding training\n",
    "            labels, the testing set and the corresponding testing labels.\n",
    "    \"\"\"\n",
    "    # Initialization needed to load a file with Weka wrappers\n",
    "    weka.core.jvm.start()\n",
    "    loader = weka.core.converters.Loader(\n",
    "        classname=\"weka.core.converters.ArffLoader\"\n",
    "    )\n",
    "\n",
    "    train_file = os.path.join(path, dataset, dataset + \"_TRAIN.arff\")\n",
    "    test_file = os.path.join(path, dataset, dataset + \"_TEST.arff\")\n",
    "    train_weka = loader.load_file(train_file)\n",
    "    test_weka = loader.load_file(test_file)\n",
    "\n",
    "    train_size = train_weka.num_instances\n",
    "    test_size = test_weka.num_instances\n",
    "    nb_dims = train_weka.get_instance(0).get_relational_value(0).num_instances\n",
    "    length = train_weka.get_instance(0).get_relational_value(0).num_attributes\n",
    "\n",
    "    train = numpy.empty((train_size, nb_dims, length))\n",
    "    test = numpy.empty((test_size, nb_dims, length))\n",
    "    train_labels = numpy.empty(train_size, dtype=numpy.int)\n",
    "    test_labels = numpy.empty(test_size, dtype=numpy.int)\n",
    "\n",
    "    for i in range(train_size):\n",
    "        train_labels[i] = int(train_weka.get_instance(i).get_value(1))\n",
    "        time_series = train_weka.get_instance(i).get_relational_value(0)\n",
    "        for j in range(nb_dims):\n",
    "            train[i, j] = time_series.get_instance(j).values\n",
    "\n",
    "    for i in range(test_size):\n",
    "        test_labels[i] = int(test_weka.get_instance(i).get_value(1))\n",
    "        time_series = test_weka.get_instance(i).get_relational_value(0)\n",
    "        for j in range(nb_dims):\n",
    "            test[i, j] = time_series.get_instance(j).values\n",
    "\n",
    "    # Normalizing dimensions independently\n",
    "    for j in range(nb_dims):\n",
    "        mean = numpy.mean(numpy.concatenate([train[:, j], test[:, j]]))\n",
    "        var = numpy.var(numpy.concatenate([train[:, j], test[:, j]]))\n",
    "        train[:, j] = (train[:, j] - mean) / math.sqrt(var)\n",
    "        test[:, j] = (test[:, j] - mean) / math.sqrt(var)\n",
    "\n",
    "    # Move the labels to {0, ..., L-1}\n",
    "    labels = numpy.unique(train_labels)\n",
    "    transform = {}\n",
    "    for i, l in enumerate(labels):\n",
    "        transform[l] = i\n",
    "    train_labels = numpy.vectorize(transform.get)(train_labels)\n",
    "    test_labels = numpy.vectorize(transform.get)(test_labels)\n",
    "\n",
    "    weka.core.jvm.stop()\n",
    "    return train, train_labels, test, test_labels\n",
    "\n",
    "\n",
    "def fit_hyperparameters(file, train, train_labels, cuda, gpu,\n",
    "                        save_memory=False):\n",
    "    \"\"\"\n",
    "    Creates a classifier from the given set of hyperparameters in the input\n",
    "    file, fits it and return it.\n",
    "\n",
    "    @param file Path of a file containing a set of hyperparemeters.\n",
    "    @param train Training set.\n",
    "    @param train_labels Labels for the training set.\n",
    "    @param cuda If True, enables computations on the GPU.\n",
    "    @param gpu GPU to use if CUDA is enabled.\n",
    "    @param save_memory If True, save GPU memory by propagating gradients after\n",
    "           each loss term, instead of doing it after computing the whole loss.\n",
    "    \"\"\"\n",
    "    classifier = scikit_wrappers.CausalCNNEncoderClassifier()\n",
    "\n",
    "    # Loads a given set of hyperparameters and fits a model with those\n",
    "    hf = open(os.path.join(file), 'r')\n",
    "    params = json.load(hf)\n",
    "    hf.close()\n",
    "    # Check the number of input channels\n",
    "    params['in_channels'] = numpy.shape(train)[1]\n",
    "    params['cuda'] = cuda\n",
    "    params['gpu'] = gpu\n",
    "    classifier.set_params(**params)\n",
    "    return classifier.fit(\n",
    "        train, train_labels, save_memory=save_memory, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'BasicMotions'\n",
    "ds_path = '../../Datasets/MTS-UEA'\n",
    "save_path = './models'\n",
    "hyper_p = 'default_hyperparameters.json' # NEED to configure the hyperparameters\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Classification tests for UEA repository datasets'\n",
    "    )\n",
    "    parser.add_argument('--dataset', type=str, metavar='D', required=True,\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--path', type=str, metavar='PATH', required=True,\n",
    "                        help='path where the dataset is located')\n",
    "    parser.add_argument('--save_path', type=str, metavar='PATH', required=True,\n",
    "                        help='path where the estimator is/should be saved')\n",
    "    parser.add_argument('--cuda', action='store_true', default=True,\n",
    "                        help='activate to use CUDA')\n",
    "    parser.add_argument('--gpu', type=int, default=0, metavar='GPU',\n",
    "                        help='index of GPU used for computations (default: 0)')\n",
    "    parser.add_argument('--hyper', type=str, metavar='FILE', required=True,\n",
    "                        help='path of the file of hyperparameters to use ' +\n",
    "                             'for training; must be a JSON file')\n",
    "    parser.add_argument('--load', action='store_true', default=False,\n",
    "                        help='activate to load the estimator instead of ' +\n",
    "                             'training it')\n",
    "    parser.add_argument('--fit_classifier', action='store_true', default=False,\n",
    "                        help='if not supervised, activate to load the ' +\n",
    "                             'model and retrain the classifier')\n",
    "\n",
    "    return parser.parse_args(args = ['--dataset', dataset, '--path', ds_path,\n",
    "                            '--save_path', save_path, '--hyper', hyper_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:weka.core.jvm:Adding bundled jars\n",
      "DEBUG:weka.core.jvm:Classpath=['/opt/anaconda3/envs/python3_6/lib/python3.6/site-packages/javabridge/jars/rhino-1.7R4.jar', '/opt/anaconda3/envs/python3_6/lib/python3.6/site-packages/javabridge/jars/runnablequeue.jar', '/opt/anaconda3/envs/python3_6/lib/python3.6/site-packages/javabridge/jars/cpython.jar', '/opt/anaconda3/envs/python3_6/lib/python3.6/site-packages/weka/lib/python-weka-wrapper.jar', '/opt/anaconda3/envs/python3_6/lib/python3.6/site-packages/weka/lib/weka.jar']\n",
      "DEBUG:weka.core.jvm:MaxHeapSize=default\n",
      "DEBUG:weka.core.jvm:Package support disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_arguments()\n",
    "    if args.cuda and not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available, proceeding without it...\")\n",
    "        args.cuda = False\n",
    "\n",
    "    train, train_labels, test, test_labels = load_UEA_dataset(\n",
    "        args.path, args.dataset\n",
    "    )\n",
    "    if not args.load and not args.fit_classifier:\n",
    "        classifier = fit_hyperparameters(\n",
    "            args.hyper, train, train_labels, args.cuda, args.gpu,\n",
    "            save_memory=True\n",
    "        )\n",
    "    else:\n",
    "        classifier = scikit_wrappers.CausalCNNEncoderClassifier()\n",
    "        hf = open(\n",
    "            os.path.join(\n",
    "                args.save_path, args.dataset + '_hyperparameters.json'\n",
    "            ), 'r'\n",
    "        )\n",
    "        hp_dict = json.load(hf)\n",
    "        hf.close()\n",
    "        hp_dict['cuda'] = args.cuda\n",
    "        hp_dict['gpu'] = args.gpu\n",
    "        classifier.set_params(**hp_dict)\n",
    "        classifier.load(os.path.join(args.save_path, args.dataset))\n",
    "\n",
    "    if not args.load:\n",
    "        if args.fit_classifier:\n",
    "            classifier.fit_classifier(classifier.encode(train), train_labels)\n",
    "        classifier.save(\n",
    "            os.path.join(args.save_path, args.dataset)\n",
    "        )\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                args.save_path, args.dataset + '_hyperparameters.json'\n",
    "            ), 'w'\n",
    "        ) as fp:\n",
    "            json.dump(classifier.get_params(), fp)\n",
    "\n",
    "    print(\"Test accuracy: \" + str(classifier.score(test, test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = \"./datasets/multivariate/\"\n",
    "dataset = \"ECG\"\n",
    "model_path = \"./\"\n",
    "folders = \"models\"\n",
    "gpu = False\n",
    "#rep = \"./datasets/\"\n",
    "#ds = \"Cricket\"\n",
    "X_train, X_test, y_train, y_test = load_datasets(rep, ds_train, ds_test, z_normal = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python SMAT_GAN",
   "language": "python",
   "name": "python3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
