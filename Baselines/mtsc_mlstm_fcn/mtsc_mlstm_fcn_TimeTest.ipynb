{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format conversion for MLSTM-FCN\n",
    "===\n",
    "\n",
    "\n",
    "---\n",
    "Input\n",
    "---\n",
    "\n",
    "A single file contains all samples and their labels: ***L * (3 + D)***\n",
    "\n",
    "\n",
    "\n",
    "- 1st col: sample_id\n",
    "- 2nd col: timestamps\n",
    "- 3rd col: label\n",
    "- after the 4th col: mts vector with D dimensions   \n",
    "\n",
    "---\n",
    "Output\n",
    "---\n",
    "\n",
    "Two array-like variables\n",
    "\n",
    "- X : array with shape (n_ts, d, sz)\n",
    "        Sequence data.\n",
    "- y : array with shape (n_ts, 1)\n",
    "        Target labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%run ../../utils/PolluScope_utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess\n",
    "\n",
    "proc=subprocess.Popen('/bin/sh',stdout=subprocess.PIPE,stdin=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "pout=proc.stdout\n",
    "pin=proc.stdin\n",
    "\n",
    "def outLoop():\n",
    "    running=True\n",
    "    while(running):\n",
    "        line=pout.readline().decode(sys.stdout.encoding)\n",
    "        print(line,end='')\n",
    "        running='\\n' in line\n",
    "    print('Finished')\n",
    "\n",
    "threading.Thread(target=outLoop).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D 19:41:40.765 NotebookApp] Searching ['/gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/Baselines/mtsc_mlstm_fcn', '/gpfs7kw/linkhome/rech/genvsq01/ulz67kb/.jupyter', '/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files\n",
      "[D 19:41:40.765 NotebookApp] Looking for jupyter_config in /etc/jupyter\n",
      "[D 19:41:40.765 NotebookApp] Looking for jupyter_config in /usr/local/etc/jupyter\n",
      "[D 19:41:40.765 NotebookApp] Looking for jupyter_config in /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/etc/jupyter\n",
      "[D 19:41:40.766 NotebookApp] Looking for jupyter_config in /gpfs7kw/linkhome/rech/genvsq01/ulz67kb/.jupyter\n",
      "[D 19:41:40.766 NotebookApp] Looking for jupyter_config in /gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/Baselines/mtsc_mlstm_fcn\n",
      "[D 19:41:40.767 NotebookApp] Looking for jupyter_notebook_config in /etc/jupyter\n",
      "[D 19:41:40.767 NotebookApp] Looking for jupyter_notebook_config in /usr/local/etc/jupyter\n",
      "[D 19:41:40.767 NotebookApp] Looking for jupyter_notebook_config in /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/etc/jupyter\n",
      "[D 19:41:40.767 NotebookApp] Looking for jupyter_notebook_config in /gpfs7kw/linkhome/rech/genvsq01/ulz67kb/.jupyter\n",
      "[D 19:41:40.768 NotebookApp] Loaded config file: /gpfs7kw/linkhome/rech/genvsq01/ulz67kb/.jupyter/jupyter_notebook_config.py\n",
      "[D 19:41:40.768 NotebookApp] Loaded config file: /gpfs7kw/linkhome/rech/genvsq01/ulz67kb/.jupyter/jupyter_notebook_config.json\n",
      "[D 19:41:40.768 NotebookApp] Looking for jupyter_notebook_config in /gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/Baselines/mtsc_mlstm_fcn\n",
      "[D 19:41:40.773 NotebookApp] Paths used for configuration of jupyter_notebook_config: \n",
      "    \t/etc/jupyter/jupyter_notebook_config.json\n",
      "[D 19:41:40.774 NotebookApp] Paths used for configuration of jupyter_notebook_config: \n",
      "    \t/usr/local/etc/jupyter/jupyter_notebook_config.json\n",
      "[D 19:41:40.774 NotebookApp] Paths used for configuration of jupyter_notebook_config: \n",
      "    \t/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/etc/jupyter/jupyter_notebook_config.d/jupyterlab.json\n",
      "    \t/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/etc/jupyter/jupyter_notebook_config.json\n",
      "[D 19:41:40.775 NotebookApp] Paths used for configuration of jupyter_notebook_config: \n",
      "    \t/gpfs7kw/linkhome/rech/genvsq01/ulz67kb/.jupyter/jupyter_notebook_config.json\n",
      "[I 19:41:41.004 NotebookApp] The port 8005 is already in use, trying another port.\n",
      "[I 19:41:41.043 NotebookApp] JupyterLab extension loaded from /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/jupyterlab\n",
      "[I 19:41:41.043 NotebookApp] JupyterLab application directory is /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/share/jupyter/lab\n",
      "[D 19:41:41.044 NotebookApp] NodeJS was not found. Yarn user configuration is ignored.\n",
      "[I 19:41:41.046 NotebookApp] Serving notebooks from local directory: /linkhome/rech/genvsq01/ulz67kb\n",
      "[I 19:41:41.046 NotebookApp] The Jupyter Notebook is running at:\n",
      "[I 19:41:41.046 NotebookApp] https://r13i3n4:8006/r13i3n4_8005/\n",
      "[I 19:41:41.047 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n"
     ]
    }
   ],
   "source": [
    "pin.write(b' jupyter notebook --debug \\n')\n",
    "pin.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Build and train the Network Model\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from utils_mlstm.keras_utils import train_model, evaluate_model, set_trainable\n",
    "from utils_mlstm.layer_utils import AttentionLSTM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('../../../SMATE_MTS'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils.UEA_utils import *\n",
    "\n",
    "TRAINABLE = True\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_2(data_dim, L, n_classes):\n",
    "    ip = Input(shape=(data_dim, L))\n",
    "    # stride = 10\n",
    "\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n",
    "    #x = Permute((2, 1))(ip)\n",
    "    x = Masking()(ip)\n",
    "    x = AttentionLSTM(128)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    #model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se\n",
    "\n",
    "def running_time(dataset, sample_rate, train_rate, dimension_rate, rep, ds):\n",
    "    X_train = dataset['X_train'] # N * L * D\n",
    "    y_train = dataset['Y_train']\n",
    "    X_test = dataset['X_test'] # N * L * D\n",
    "    y_test = dataset['Y_test']\n",
    "    \n",
    "    nbr_sample = int(sample_rate * X_train.shape[1])\n",
    "    nbr_ts_instance = int(train_rate * X_train.shape[0])\n",
    "    nbr_dimension = int(dimension_rate * X_train.shape[2])\n",
    "    \n",
    "    print(\"X_train.shape is \", X_train.shape)\n",
    "    # vary dimension size\n",
    "    X_train = X_train[:, :, :nbr_dimension]\n",
    "    X_test = X_test[:, :, :nbr_dimension]\n",
    "    \n",
    "    '''\n",
    "    X_train = resample_dataset(X_train, nbr_sample)[: nbr_ts_instance][: nbr_dimension]\n",
    "    y_train = y_train[: nbr_ts_instance]\n",
    "    \n",
    "    \n",
    "    X_train = resample_dataset(X_train, nbr_sample)\n",
    "    \n",
    "    X_test = resample_dataset(X_test, nbr_sample)\n",
    "    print(\"Nbr_class in Train_set is %d, \\nNbr_class in Test_set is %d\" \n",
    "          %(len(np.unique(y_train)), len(np.unique(y_test))))\n",
    "    '''\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, _, y_train, _ = train_test_split(X_train, \n",
    "                                              y_train, \n",
    "                                              test_size=1-train_rate, \n",
    "                                              random_state=42)\n",
    "    \n",
    "    # Bacis Dataset Information and Model Configurations\n",
    "    train_size = X_train.shape[0] \n",
    "    L = X_train.shape[1]\n",
    "    data_dim = X_train.shape[2]\n",
    "    n_classes = dataset['n_classes']\n",
    "    \n",
    "    X_train = np.transpose(X_train, (0, 2, 1)) # N * D * L\n",
    "    X_test = np.transpose(X_test, (0, 2, 1)) # N * D * L\n",
    "    \n",
    "    np.save(rep + ds + '/X_train.npy', X_train)\n",
    "    np.save(rep + ds + '/y_train.npy', y_train)\n",
    "    np.save(rep + ds + '/X_test.npy', X_test)\n",
    "    np.save(rep + ds + '/y_test.npy', y_test)\n",
    "\n",
    "    # Build MLSTM-FCN model\n",
    "    DATASET_INDEX = rep + ds + '/'\n",
    "    model = generate_model_2(data_dim, L, n_classes)\n",
    "    \n",
    "    # Train SMATE model\n",
    "    start = time.time()\n",
    "    train_model(model, DATASET_INDEX, dataset_prefix=ds+'_', epochs=300, batch_size=128)\n",
    "    print(\"Training Time for sample_rate (%f2) train_rate (%f2) dimension_rate (%f2)  is %d\" \n",
    "          %(sample_rate, train_rate, dimension_rate, time.time() - start))\n",
    "    #K.clear_session()\n",
    "    #K.clear_session()\n",
    "    #tf.reset_default_graph()\n",
    "    \n",
    "    return time.time() - start\n",
    "    \n",
    "def resample_dataset(x, nbr_sample):\n",
    "    x_sampled = np.zeros(shape=(x.shape[0], nbr_sample, x.shape[2])) # N' * L * D \n",
    "    from scipy import signal\n",
    "    for i in range(x.shape[0]):\n",
    "        f = signal.resample(x[i], nbr_sample, axis = 0)\n",
    "        x_sampled[i] = f\n",
    "    return x_sampled\n",
    "\n",
    "def save_running_time(rep, ds_name, dataset, save_path, sample_rate, train_rate, dimension_rate):\n",
    "    df_time = pd.DataFrame(data = np.zeros((1, 5)), columns = ['Dataset', \"train_rate\", 'sample_rate', 'dimension_rate', 'run_time'])\n",
    "    run_time = running_time(dataset, sample_rate, train_rate, dimension_rate, rep, ds_name)\n",
    "    df_time['Dataset'] = ds_name\n",
    "    df_time['train_rate'] = train_rate\n",
    "    df_time['sample_rate'] = sample_rate\n",
    "    df_time['dimension_rate'] = dimension_rate\n",
    "    df_time['run_time'] = run_time\n",
    "    if not os.path.exists(save_path + \"MLSTM_running_time_full.csv\"):\n",
    "        df_time.to_csv(save_path + \"MLSTM_running_time_full.csv\", index=False)\n",
    "    else:\n",
    "        res = pd.read_csv(save_path + \"MLSTM_running_time_full.csv\")\n",
    "        res = pd.concat((res, df_time))\n",
    "        res.to_csv(save_path + \"MLSTM_running_time_full.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is ['15' '16' '42' '52' '53' '6' '62' '64' '65' '67' '88' '90' '92' '95']\n",
      "total number of samples is 2459\n",
      "total number of samples is 2466\n"
     ]
    }
   ],
   "source": [
    "'''=================================================== Prepare UEA data ========================================================'''\n",
    "\n",
    "rep = \"../../../Datasets/MTS-UEA/\"\n",
    "ds = \"LSST\"\n",
    "rep_ds_train = rep + ds + \"/output_train/\"\n",
    "rep_ds_test = rep + ds + \"/output_test/\"\n",
    "meta_csv = \"meta_data.csv\"  # the meta data of training/testing set\n",
    "rep_output = rep_ds_train + \"out_results/\"  # output results, e.g., training loss, models\n",
    "os.system(\"mkdir -p \" + rep_output)\n",
    "sup_ratio = 1\n",
    "\n",
    "# prepare UEA datasets form 'arff' files\n",
    "dataset = get_UEA_dataset(rep_ds_train, rep_ds_test, meta_csv, sup_ratio, mode = 'load', split_strategy='EqualSplit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-6aa3e6aa3a52>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6aa3e6aa3a52>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    rep, ds)\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# output training time for different sample_rate & train_rate & dimension_rate\n",
    "# A) vary sample_rate \n",
    "train_rate = 1\n",
    "dimension_rate = 1\n",
    "for sample_rate in np.linspace(0.1, 1, 10):\n",
    "    save_running_time(rep, ds, dataset, rep_output, sample_rate, train_rate, dimension_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0112 02:04:56.194144 23369307920192 deprecation_wrapper.py:119] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape is  (2459, 36, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 02:04:56.679972 23369307920192 deprecation.py:323] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0112 02:04:56.714828 23369307920192 deprecation.py:506] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0112 02:04:56.715455 23369307920192 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0112 02:04:57.103264 23369307920192 deprecation_wrapper.py:119] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  245 Number of test samples :  2466\n",
      "Number of classes :  14\n",
      "Sequence length :  36\n",
      "X_train.shape is  (245, 6, 36)\n",
      "X_test.shape is  (2466, 6, 36)\n",
      "Class weights :  [ 2.1875      0.72916667  0.5         3.5        17.5         8.75\n",
      "  1.34615385  8.75        0.60344828  3.5         0.92105263  0.20588235\n",
      "  1.16666667  8.75      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.1000002) dimension_rate (1.0000002)  is 26\n",
      "X_train.shape is  (2459, 36, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 02:05:23.874157 23369307920192 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  491 Number of test samples :  2466\n",
      "Number of classes :  14\n",
      "Sequence length :  36\n",
      "X_train.shape is  (491, 6, 36)\n",
      "X_test.shape is  (2466, 6, 36)\n",
      "Class weights :  [ 1.59415584  0.79707792  0.46146617  2.6978022  11.69047619  5.8452381\n",
      "  1.09598214  7.01428571  0.59443099  3.18831169  1.25255102  0.22197107\n",
      "  1.46130952  3.50714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-976c42b7d18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdimension_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msave_running_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1f9c6f2d662d>\u001b[0m in \u001b[0;36msave_running_time\u001b[0;34m(rep, ds_name, dataset, save_path, sample_rate, train_rate, dimension_rate)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_running_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mdf_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dimension_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mdf_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mdf_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_rate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1f9c6f2d662d>\u001b[0m in \u001b[0;36mrunning_time\u001b[0;34m(dataset, sample_rate, train_rate, dimension_rate, rep, ds)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Train SMATE model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     print(\"Training Time for sample_rate (%f2) train_rate (%f2) dimension_rate (%f2)  is %d\" \n\u001b[1;32m    117\u001b[0m           %(sample_rate, train_rate, dimension_rate, time.time() - start))\n",
      "\u001b[0;32m/gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/Baselines/mtsc_mlstm_fcn/utils_mlstm/keras_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset_id, dataset_prefix, dataset_fold_id, epochs, batch_size, val_subset, cutoff, normalize_timeseries, learning_rate, monitor, optimization_mode, compile_model)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, validation_data=(X_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;31m#model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callback_list, class_weight=class_weight, verbose=2, validation_split=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# B) vary train_rate\n",
    "sample_rate = 1\n",
    "dimension_rate = 1\n",
    "for train_rate in np.linspace(0.1, 1, 10):\n",
    "    save_running_time(rep, ds, dataset, rep_output, sample_rate, train_rate, dimension_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0112 01:51:50.859034 22978958038848 deprecation_wrapper.py:119] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0112 01:51:51.463855 22978958038848 deprecation.py:323] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0112 01:51:51.498858 22978958038848 deprecation.py:506] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0112 01:51:51.499492 22978958038848 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0112 01:51:51.979619 22978958038848 deprecation_wrapper.py:119] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  267 Number of test samples :  173\n",
      "Number of classes :  7\n",
      "Sequence length :  144\n",
      "X_train.shape is  (267, 48, 144)\n",
      "X_test.shape is  (173, 48, 144)\n",
      "Class weights :  [1.19196429 1.19196429 1.0037594  0.90816327 0.88704319 1.05952381\n",
      " 0.86688312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_rate (0.0500002)  is 23\n",
      "X_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 01:52:16.707644 22978958038848 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  267 Number of test samples :  173\n",
      "Number of classes :  7\n",
      "Sequence length :  144\n",
      "X_train.shape is  (267, 96, 144)\n",
      "X_test.shape is  (173, 96, 144)\n",
      "Class weights :  [1.19196429 1.19196429 1.0037594  0.90816327 0.88704319 1.05952381\n",
      " 0.86688312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_rate (0.1000002)  is 34\n",
      "X_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 01:52:52.788884 22978958038848 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  267 Number of test samples :  173\n",
      "Number of classes :  7\n",
      "Sequence length :  144\n",
      "X_train.shape is  (267, 192, 144)\n",
      "X_test.shape is  (173, 192, 144)\n",
      "Class weights :  [1.19196429 1.19196429 1.0037594  0.90816327 0.88704319 1.05952381\n",
      " 0.86688312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_rate (0.2000002)  is 63\n",
      "X_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0112 01:53:58.346570 22978958038848 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  267 Number of test samples :  173\n",
      "Number of classes :  7\n",
      "Sequence length :  144\n",
      "X_train.shape is  (267, 288, 144)\n",
      "X_test.shape is  (173, 288, 144)\n",
      "Class weights :  [1.19196429 1.19196429 1.0037594  0.90816327 0.88704319 1.05952381\n",
      " 0.86688312]\n"
     ]
    }
   ],
   "source": [
    "# C) vary dimension_rate\n",
    "sample_rate, train_rate = 1, 1\n",
    "dimension_rate = 0.05\n",
    "save_running_time(rep, ds, dataset, rep_output, sample_rate, train_rate, dimension_rate)\n",
    "for dimension_rate in np.linspace(0.1, 1, 10):\n",
    "    save_running_time(rep, ds, dataset, rep_output, sample_rate, train_rate, dimension_rate)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
