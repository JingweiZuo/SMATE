{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format conversion for MLSTM-FCN\n",
    "===\n",
    "\n",
    "\n",
    "---\n",
    "Input\n",
    "---\n",
    "\n",
    "A single file contains all samples and their labels: ***L * (3 + D)***\n",
    "\n",
    "\n",
    "\n",
    "- 1st col: sample_id\n",
    "- 2nd col: timestamps\n",
    "- 3rd col: label\n",
    "- after the 4th col: mts vector with D dimensions   \n",
    "\n",
    "---\n",
    "Output\n",
    "---\n",
    "\n",
    "Two array-like variables\n",
    "\n",
    "- X : array with shape (n_ts, d, sz)\n",
    "        Sequence data.\n",
    "- y : array with shape (n_ts, 1)\n",
    "        Target labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('../../../SMATE_MTS'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils.UEA_utils import *\n",
    "#%run ../../utils/PolluScope_utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess\n",
    "\n",
    "proc=subprocess.Popen('/bin/sh',stdout=subprocess.PIPE,stdin=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "pout=proc.stdout\n",
    "pin=proc.stdin\n",
    "\n",
    "def outLoop():\n",
    "    running=True\n",
    "    while(running):\n",
    "        line=pout.readline().decode(sys.stdout.encoding)\n",
    "        print(line,end='')\n",
    "        running='\\n' in line\n",
    "    print('Finished')\n",
    "\n",
    "threading.Thread(target=outLoop).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15_OverSample_X_test.npy\n",
      "15_OverSample_X_train.npy\n",
      "15_OverSample_y_test.npy\n",
      "15_OverSample_y_train.npy\n",
      "15_X_test.npy\n",
      "15_X_train.npy\n",
      "15_y_test.npy\n",
      "15_y_train.npy\n",
      "17_OverSample_X_test.npy\n",
      "17_OverSample_X_train.npy\n",
      "17_OverSample_y_test.npy\n",
      "17_OverSample_y_train.npy\n",
      "17_X_test.npy\n",
      "17_X_train.npy\n",
      "17_y_test.npy\n",
      "17_y_train.npy\n",
      "18_OverSample_X_test.npy\n",
      "18_OverSample_X_train.npy\n",
      "18_OverSample_y_test.npy\n",
      "18_OverSample_y_train.npy\n",
      "18_X_test.npy\n",
      "18_X_train.npy\n",
      "18_y_test.npy\n",
      "18_y_train.npy\n",
      "all_dimensions_Test.csv\n",
      "all_dimensions_Train.csv\n",
      "polluscop_speed.zip\n",
      "X_test.npy\n",
      "X_train.npy\n",
      "y_test.npy\n",
      "y_train.npy\n"
     ]
    }
   ],
   "source": [
    "pin.write(b' ls /ssd/jzuo/SMATE_MTS/Datasets/MTS-others/PolluScope_16_12_2020_speed_complet/all_dimensions \\n')\n",
    "pin.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "class list (norm) is [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n"
     ]
    }
   ],
   "source": [
    "'''=================================================== Prepare PolluScope data ========================================================'''\n",
    "\n",
    "'''\n",
    "rep = \"/ssd/jzuo/SMATE_MTS/Datasets/MTS-others/PolluScope_16_12_2020_speed_complet/\"\n",
    "ds = \"all_dimensions\"\n",
    "ds_train = ds + '/' + ds + \"_Train.csv\"\n",
    "ds_test = ds + '/' + ds + \"_Test.csv\"\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_datasets(rep, ds_train, ds_test, z_normal = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is ['1.0' '10.0' '11.0' '12.0' '2.0' '3.0' '4.0' '5.0' '6.0' '7.0' '8.0'\n",
      " '9.0']\n",
      "total number of samples is 108\n",
      "total number of samples is 72\n"
     ]
    }
   ],
   "source": [
    "'''=================================================== Prepare UEA data ========================================================'''\n",
    "\n",
    "rep = \"/ssd/jzuo/SMATE_MTS/Datasets/MTS-UEA/\"\n",
    "ds = \"Cricket\"\n",
    "rep_ds_train = rep + ds + \"/output_train/\"\n",
    "rep_ds_test = rep + ds + \"/output_test/\"\n",
    "meta_csv = \"meta_data.csv\"  # the meta data of training/testing set\n",
    "rep_output = rep_ds_train + \"out_results/\"  # output results, e.g., training loss, models\n",
    "os.system(\"mkdir -p \" + rep_output)\n",
    "sup_ratio = 1\n",
    "\n",
    "# prepare UEA datasets form 'arff' files\n",
    "dataset = get_UEA_dataset(rep_ds_train, rep_ds_test, meta_csv, sup_ratio, mode = 'load', split_strategy='EqualSplit')\n",
    "\n",
    "X_train = dataset['X_train']\n",
    "y_train = dataset['Y_train']\n",
    "X_test = dataset['X_test']\n",
    "y_test = dataset['Y_test']\n",
    "X_sup = dataset['X_sup']  # 3-D Array: N * L * D\n",
    "X_unsup = dataset['X_unsup']\n",
    "y_sup = dataset['Y_sup']  # 1-D Array\n",
    "y_unsup = dataset['Y_unsup']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset :  (1164, 8, 10) (1164,)\n",
      "Test dataset :  (508, 8, 10) (508,)\n",
      "Train dataset metrics :  0.4054528964000736 0.3742072611537749\n",
      "Test dataset :  0.4119964829388672 0.3803598759232224\n",
      "Nb classes train:  13\n",
      "Nb classes test:  13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "NB_CLASS = len(np.unique(y_train))\n",
    "MAX_TIMESTEPS =X_train.shape[-1]\n",
    "MAX_NB_VARIABLES = X_train.shape[-2]\n",
    "# Save the datasets\n",
    "print(\"Train dataset : \", X_train.shape, y_train.shape)\n",
    "print(\"Test dataset : \", X_test.shape, y_test.shape)\n",
    "print(\"Train dataset metrics : \", X_train.mean(), X_train.std())\n",
    "print(\"Test dataset : \", X_test.mean(), X_test.std())\n",
    "print(\"Nb classes train: \", len(np.unique(y_train)))\n",
    "print(\"Nb classes test: \", len(np.unique(y_test)))\n",
    "\n",
    "np.save(rep + ds + '/X_train.npy', X_train)\n",
    "np.save(rep + ds + '/y_train.npy', y_train)\n",
    "np.save(rep + ds + '/X_test.npy', X_test)\n",
    "np.save(rep + ds + '/y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_labels(y):\n",
    "    '''\n",
    "    Convert the classes in dataset into training labels in Keras [0, nbr_class-1]\n",
    "\n",
    "    class_array: an array of classes for samples in dataset\n",
    "    '''\n",
    "    classes, counts_cl = np.unique(y, return_counts=True)\n",
    "    print(\"class list is \" + str(classes))\n",
    "\n",
    "    mapping_c_l = {}  # a mappling between classes and labels\n",
    "    for idx, c in enumerate(list(classes)):\n",
    "        mapping_c_l.update({c: idx})\n",
    "    return mapping_c_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is [ 1.  2.  3.  4.  5.  7.  8. 10.]\n",
      "Train dataset :  (481, 8, 10) (481,)\n",
      "Test dataset :  (527, 8, 10) (527,)\n",
      "Train dataset metrics :  201.29303546119087 833.440601611844\n",
      "Test dataset :  201.43323874758403 1104.3744524147357\n",
      "Nb classes :  8\n",
      "Nb classes train:  8\n",
      "Nb classes test:  8\n"
     ]
    }
   ],
   "source": [
    "'''=============================Prepare PolluScope data From '.npy' file ==============================='''\n",
    "\n",
    "rep = \"/ssd/jzuo/SMATE_MTS/Datasets/MTS-others/PolluScope_16_12_2020_speed_complet/\"\n",
    "ds = \"all_dimensions\"\n",
    "threshold_taux = 0.017\n",
    "X_train = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'X_train.npy') #\"_OverSample_\"\n",
    "y_train = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'y_train.npy')\n",
    "X_test = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'X_test.npy')\n",
    "y_test = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'y_test.npy')\n",
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "'''********* Remove *speed* dimension *********'''\n",
    "#X_train = X_train[:, :-1, :]\n",
    "#X_test = X_test[:, :-1, :]\n",
    "\n",
    "NB_CLASS = len(np.unique(y_train))\n",
    "MAX_TIMESTEPS =X_train.shape[-1]\n",
    "MAX_NB_VARIABLES = X_train.shape[-2] \n",
    "\n",
    "if np.max(y_train) != len(y_train) - 1 :\n",
    "    mapping_c_l = get_mapping_labels(y_train)\n",
    "    y_train = np.array([mapping_c_l[i] for i in y_train])\n",
    "    y_test = np.array([mapping_c_l[i] for i in y_test])\n",
    "\n",
    "print(\"Train dataset : \", X_train.shape, y_train.shape)\n",
    "print(\"Test dataset : \", X_test.shape, y_test.shape)\n",
    "print(\"Train dataset metrics : \", X_train.mean(), X_train.std())\n",
    "print(\"Test dataset : \", X_test.mean(), X_test.std())\n",
    "print(\"Nb classes : \", len(np.unique(y_train)))\n",
    "print(\"Nb classes train: \", len(np.unique(y_train)))\n",
    "print(\"Nb classes test: \", len(np.unique(y_test)))\n",
    "\n",
    "np.save(rep + ds + '/X_train.npy', X_train)\n",
    "np.save(rep + ds + '/y_train.npy', y_train)\n",
    "np.save(rep + ds + '/X_test.npy', X_test)\n",
    "np.save(rep + ds + '/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2635. -2277. -2136. ... 29757. 30884. 46315.] \n",
      " [-5705. -2612. -2572. ... 29668. 42780. 55871.]\n"
     ]
    }
   ],
   "source": [
    "# the labels should range from 0 to nbr_class\n",
    "print(np.unique(y_train), '\\n', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Build and train the Network Model\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "from utils_mlstm.keras_utils import train_model, evaluate_model, set_trainable\n",
    "from utils_mlstm.layer_utils import AttentionLSTM\n",
    "\n",
    "TRAINABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(128)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_model_2():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "    # stride = 10\n",
    "\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n",
    "    #x = Permute((2, 1))(ip)\n",
    "    x = Masking()(ip)\n",
    "    x = AttentionLSTM(128)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_model_3():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    #y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    #y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_model_4():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "    # stride = 3\n",
    "    #\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = AttentionLSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    #y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    #y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 8, 10)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 10, 8)        0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 10, 128)      8320        permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 10, 128)      512         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 10, 128)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 128)       0           global_average_pooling1d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 8)         1024        reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1, 128)       1024        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 10, 128)      0           activation_10[0][0]              \n",
      "                                                                 dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 10, 256)      164096      multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 10, 256)      1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 10, 256)      0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 256)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1, 16)        4096        reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1, 256)       4096        dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 10, 256)      0           activation_11[0][0]              \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 10, 128)      98432       multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "masking_4 (Masking)             (None, 8, 10)        0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 10, 128)      512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_lstm_4 (AttentionLSTM (None, 128)          94208       masking_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 10, 128)      0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           attention_lstm_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           dropout_4[0][0]                  \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 8)            2056        concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 379,400\n",
      "Trainable params: 378,376\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = generate_model_2()\n",
    "\n",
    "DATASET_INDEX = rep + ds + '/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  481 Number of test samples :  527\n",
      "Number of classes :  8\n",
      "Sequence length :  10\n",
      "X_train.shape is  (481, 8, 10)\n",
      "X_test.shape is  (527, 8, 10)\n",
      "Class weights :  [1.00208333 1.00208333 1.00208333 1.00208333 1.00208333 0.98565574\n",
      " 1.00208333 1.00208333]\n",
      "Train on 481 samples, validate on 527 samples\n",
      "Epoch 1/300\n",
      " - 5s - loss: 2.2558 - acc: 0.2141 - val_loss: 1.6043 - val_acc: 0.3852\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.38520, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 2/300\n",
      " - 0s - loss: 1.9517 - acc: 0.3160 - val_loss: 1.4247 - val_acc: 0.5617\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.38520 to 0.56167, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 3/300\n",
      " - 0s - loss: 1.7938 - acc: 0.3659 - val_loss: 1.4280 - val_acc: 0.5275\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.56167\n",
      "Epoch 4/300\n",
      " - 0s - loss: 1.6775 - acc: 0.4304 - val_loss: 1.4765 - val_acc: 0.5028\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.56167\n",
      "Epoch 5/300\n",
      " - 0s - loss: 1.6386 - acc: 0.4075 - val_loss: 1.3975 - val_acc: 0.5199\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.56167\n",
      "Epoch 6/300\n",
      " - 0s - loss: 1.4664 - acc: 0.4865 - val_loss: 1.3360 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.56167 to 0.57495, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 7/300\n",
      " - 0s - loss: 1.4309 - acc: 0.4927 - val_loss: 1.3376 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.57495\n",
      "Epoch 8/300\n",
      " - 0s - loss: 1.3292 - acc: 0.5177 - val_loss: 1.4093 - val_acc: 0.4592\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.57495\n",
      "Epoch 9/300\n",
      " - 0s - loss: 1.2727 - acc: 0.5489 - val_loss: 1.4016 - val_acc: 0.4687\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.57495\n",
      "Epoch 10/300\n",
      " - 0s - loss: 1.2031 - acc: 0.5946 - val_loss: 1.3200 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.57495\n",
      "Epoch 11/300\n",
      " - 0s - loss: 1.1316 - acc: 0.6195 - val_loss: 1.2995 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.57495\n",
      "Epoch 12/300\n",
      " - 0s - loss: 1.0831 - acc: 0.6424 - val_loss: 1.3209 - val_acc: 0.5313\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.57495\n",
      "Epoch 13/300\n",
      " - 0s - loss: 1.0533 - acc: 0.6403 - val_loss: 1.2286 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.57495 to 0.57495, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.9618 - acc: 0.7110 - val_loss: 1.3033 - val_acc: 0.5579\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.57495\n",
      "Epoch 15/300\n",
      " - 0s - loss: 0.9190 - acc: 0.6861 - val_loss: 1.1613 - val_acc: 0.6034\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.57495 to 0.60342, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 16/300\n",
      " - 0s - loss: 0.8693 - acc: 0.7048 - val_loss: 1.1298 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.60342\n",
      "Epoch 17/300\n",
      " - 0s - loss: 0.7956 - acc: 0.7339 - val_loss: 1.1274 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.60342 to 0.61290, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 18/300\n",
      " - 0s - loss: 0.8057 - acc: 0.7277 - val_loss: 1.1749 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.61290\n",
      "Epoch 19/300\n",
      " - 0s - loss: 0.7308 - acc: 0.7360 - val_loss: 1.1013 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.61290\n",
      "Epoch 20/300\n",
      " - 0s - loss: 0.6774 - acc: 0.7838 - val_loss: 1.1528 - val_acc: 0.6205\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.61290 to 0.62049, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 21/300\n",
      " - 0s - loss: 0.6817 - acc: 0.7963 - val_loss: 1.0964 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.62049 to 0.62998, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 22/300\n",
      " - 0s - loss: 0.6332 - acc: 0.8191 - val_loss: 1.1016 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.62998 to 0.63567, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 23/300\n",
      " - 0s - loss: 0.6181 - acc: 0.8150 - val_loss: 1.1760 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.63567\n",
      "Epoch 24/300\n",
      " - 0s - loss: 0.5557 - acc: 0.8420 - val_loss: 1.0744 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.63567 to 0.64326, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 25/300\n",
      " - 0s - loss: 0.5710 - acc: 0.8233 - val_loss: 1.0376 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.64326 to 0.66034, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 26/300\n",
      " - 0s - loss: 0.5410 - acc: 0.8441 - val_loss: 1.0076 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.66034\n",
      "Epoch 27/300\n",
      " - 0s - loss: 0.5028 - acc: 0.8441 - val_loss: 1.0282 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.66034 to 0.66983, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 28/300\n",
      " - 0s - loss: 0.4866 - acc: 0.8399 - val_loss: 1.1198 - val_acc: 0.6243\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.66983\n",
      "Epoch 29/300\n",
      " - 0s - loss: 0.4598 - acc: 0.8669 - val_loss: 1.0952 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.66983\n",
      "Epoch 30/300\n",
      " - 0s - loss: 0.4573 - acc: 0.8607 - val_loss: 1.0438 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.66983\n",
      "Epoch 31/300\n",
      " - 0s - loss: 0.3872 - acc: 0.8940 - val_loss: 1.1377 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.66983\n",
      "Epoch 32/300\n",
      " - 0s - loss: 0.4476 - acc: 0.8669 - val_loss: 1.1558 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.66983\n",
      "Epoch 33/300\n",
      " - 0s - loss: 0.3787 - acc: 0.8940 - val_loss: 1.1789 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.66983\n",
      "Epoch 34/300\n",
      " - 0s - loss: 0.3898 - acc: 0.8753 - val_loss: 1.1735 - val_acc: 0.6224\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.66983\n",
      "Epoch 35/300\n",
      " - 0s - loss: 0.3578 - acc: 0.8960 - val_loss: 1.1148 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.66983\n",
      "Epoch 36/300\n",
      " - 0s - loss: 0.3167 - acc: 0.9106 - val_loss: 1.0947 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.66983\n",
      "Epoch 37/300\n",
      " - 0s - loss: 0.2987 - acc: 0.9210 - val_loss: 1.1910 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.66983\n",
      "Epoch 38/300\n",
      " - 0s - loss: 0.3212 - acc: 0.9023 - val_loss: 1.1997 - val_acc: 0.6262\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.66983\n",
      "Epoch 39/300\n",
      " - 0s - loss: 0.2547 - acc: 0.9314 - val_loss: 1.2254 - val_acc: 0.6148\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.66983\n",
      "Epoch 40/300\n",
      " - 0s - loss: 0.3173 - acc: 0.8960 - val_loss: 1.2609 - val_acc: 0.6034\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.66983\n",
      "Epoch 41/300\n",
      " - 0s - loss: 0.2548 - acc: 0.9293 - val_loss: 1.2804 - val_acc: 0.5977\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.66983\n",
      "Epoch 42/300\n",
      " - 0s - loss: 0.2582 - acc: 0.9272 - val_loss: 1.2257 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.66983\n",
      "Epoch 43/300\n",
      " - 0s - loss: 0.2483 - acc: 0.9356 - val_loss: 1.3555 - val_acc: 0.5863\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.66983\n",
      "Epoch 44/300\n",
      " - 0s - loss: 0.2462 - acc: 0.9376 - val_loss: 1.2745 - val_acc: 0.6224\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.66983\n",
      "Epoch 45/300\n",
      " - 0s - loss: 0.2816 - acc: 0.9023 - val_loss: 1.3868 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.66983\n",
      "Epoch 46/300\n",
      " - 0s - loss: 0.2451 - acc: 0.9397 - val_loss: 1.3574 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.66983\n",
      "Epoch 47/300\n",
      " - 0s - loss: 0.2219 - acc: 0.9439 - val_loss: 1.2948 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.66983\n",
      "Epoch 48/300\n",
      " - 0s - loss: 0.2368 - acc: 0.9314 - val_loss: 1.4125 - val_acc: 0.5996\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.66983\n",
      "Epoch 49/300\n",
      " - 0s - loss: 0.2113 - acc: 0.9459 - val_loss: 1.3377 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.66983\n",
      "Epoch 50/300\n",
      " - 0s - loss: 0.1876 - acc: 0.9501 - val_loss: 1.3471 - val_acc: 0.6186\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.66983\n",
      "Epoch 51/300\n",
      " - 0s - loss: 0.2131 - acc: 0.9314 - val_loss: 1.4552 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.66983\n",
      "Epoch 52/300\n",
      " - 0s - loss: 0.2094 - acc: 0.9459 - val_loss: 1.3652 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.66983\n",
      "Epoch 53/300\n",
      " - 0s - loss: 0.1777 - acc: 0.9522 - val_loss: 1.4055 - val_acc: 0.6015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_acc did not improve from 0.66983\n",
      "Epoch 54/300\n",
      " - 0s - loss: 0.1895 - acc: 0.9563 - val_loss: 1.3206 - val_acc: 0.6224\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.66983\n",
      "Epoch 55/300\n",
      " - 0s - loss: 0.1765 - acc: 0.9543 - val_loss: 1.4882 - val_acc: 0.5977\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.66983\n",
      "Epoch 56/300\n",
      " - 0s - loss: 0.2014 - acc: 0.9459 - val_loss: 1.3254 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.66983\n",
      "Epoch 57/300\n",
      " - 0s - loss: 0.1500 - acc: 0.9605 - val_loss: 1.2500 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.66983\n",
      "Epoch 58/300\n",
      " - 0s - loss: 0.2093 - acc: 0.9376 - val_loss: 1.2969 - val_acc: 0.6319\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.66983\n",
      "Epoch 59/300\n",
      " - 0s - loss: 0.1988 - acc: 0.9314 - val_loss: 1.2246 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.66983 to 0.67362, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 60/300\n",
      " - 0s - loss: 0.1931 - acc: 0.9397 - val_loss: 1.4862 - val_acc: 0.5731\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.67362\n",
      "Epoch 61/300\n",
      " - 0s - loss: 0.1590 - acc: 0.9522 - val_loss: 1.3839 - val_acc: 0.6186\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.67362\n",
      "Epoch 62/300\n",
      " - 0s - loss: 0.1464 - acc: 0.9584 - val_loss: 1.2457 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.67362\n",
      "Epoch 63/300\n",
      " - 0s - loss: 0.1647 - acc: 0.9459 - val_loss: 1.4023 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.67362\n",
      "Epoch 64/300\n",
      " - 0s - loss: 0.1416 - acc: 0.9522 - val_loss: 1.3224 - val_acc: 0.6243\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.67362\n",
      "Epoch 65/300\n",
      " - 0s - loss: 0.1378 - acc: 0.9626 - val_loss: 1.2470 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.67362\n",
      "Epoch 66/300\n",
      " - 0s - loss: 0.1258 - acc: 0.9584 - val_loss: 1.2497 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.67362\n",
      "Epoch 67/300\n",
      " - 0s - loss: 0.1445 - acc: 0.9626 - val_loss: 1.3462 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.67362\n",
      "Epoch 68/300\n",
      " - 0s - loss: 0.1383 - acc: 0.9667 - val_loss: 1.4157 - val_acc: 0.6262\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.67362\n",
      "Epoch 69/300\n",
      " - 0s - loss: 0.1333 - acc: 0.9626 - val_loss: 1.3458 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.67362\n",
      "Epoch 70/300\n",
      " - 0s - loss: 0.1248 - acc: 0.9647 - val_loss: 1.5867 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.67362\n",
      "Epoch 71/300\n",
      " - 0s - loss: 0.1203 - acc: 0.9647 - val_loss: 1.5145 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.67362\n",
      "Epoch 72/300\n",
      " - 0s - loss: 0.1327 - acc: 0.9688 - val_loss: 1.2824 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.67362\n",
      "Epoch 73/300\n",
      " - 0s - loss: 0.1002 - acc: 0.9792 - val_loss: 1.3060 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.67362\n",
      "Epoch 74/300\n",
      " - 0s - loss: 0.1161 - acc: 0.9709 - val_loss: 1.2324 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.67362\n",
      "Epoch 75/300\n",
      " - 0s - loss: 0.1200 - acc: 0.9751 - val_loss: 1.3984 - val_acc: 0.5977\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.67362\n",
      "Epoch 76/300\n",
      " - 0s - loss: 0.1007 - acc: 0.9792 - val_loss: 1.4920 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.67362\n",
      "Epoch 77/300\n",
      " - 0s - loss: 0.0939 - acc: 0.9813 - val_loss: 1.3349 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.67362\n",
      "Epoch 78/300\n",
      " - 0s - loss: 0.0973 - acc: 0.9667 - val_loss: 1.4268 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.67362\n",
      "Epoch 79/300\n",
      " - 0s - loss: 0.0931 - acc: 0.9771 - val_loss: 1.5199 - val_acc: 0.6224\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.67362\n",
      "Epoch 80/300\n",
      " - 0s - loss: 0.0939 - acc: 0.9751 - val_loss: 1.3060 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.67362\n",
      "Epoch 81/300\n",
      " - 0s - loss: 0.1078 - acc: 0.9563 - val_loss: 1.2816 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.67362\n",
      "Epoch 82/300\n",
      " - 0s - loss: 0.1020 - acc: 0.9709 - val_loss: 1.2583 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.67362\n",
      "Epoch 83/300\n",
      " - 0s - loss: 0.0813 - acc: 0.9751 - val_loss: 1.3566 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.67362\n",
      "Epoch 84/300\n",
      " - 0s - loss: 0.0707 - acc: 0.9875 - val_loss: 1.3042 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.67362\n",
      "Epoch 85/300\n",
      " - 0s - loss: 0.0662 - acc: 0.9875 - val_loss: 1.2227 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.67362 to 0.68501, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 86/300\n",
      " - 0s - loss: 0.0750 - acc: 0.9875 - val_loss: 1.3526 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.68501\n",
      "Epoch 87/300\n",
      " - 0s - loss: 0.0788 - acc: 0.9771 - val_loss: 1.3423 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.68501\n",
      "Epoch 88/300\n",
      " - 0s - loss: 0.0770 - acc: 0.9813 - val_loss: 1.2996 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.68501\n",
      "Epoch 89/300\n",
      " - 0s - loss: 0.0458 - acc: 0.9938 - val_loss: 1.3554 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.68501\n",
      "Epoch 90/300\n",
      " - 0s - loss: 0.0652 - acc: 0.9834 - val_loss: 1.3160 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.68501\n",
      "Epoch 91/300\n",
      " - 0s - loss: 0.0578 - acc: 0.9917 - val_loss: 1.3293 - val_acc: 0.6471\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.68501\n",
      "Epoch 92/300\n",
      " - 0s - loss: 0.0529 - acc: 0.9938 - val_loss: 1.3368 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.68501\n",
      "Epoch 93/300\n",
      " - 0s - loss: 0.0865 - acc: 0.9813 - val_loss: 1.2899 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.68501\n",
      "Epoch 94/300\n",
      " - 0s - loss: 0.0731 - acc: 0.9834 - val_loss: 1.4291 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.68501\n",
      "Epoch 95/300\n",
      " - 0s - loss: 0.0589 - acc: 0.9917 - val_loss: 1.4880 - val_acc: 0.6319\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.68501\n",
      "Epoch 96/300\n",
      " - 0s - loss: 0.0735 - acc: 0.9813 - val_loss: 1.4029 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.68501\n",
      "Epoch 97/300\n",
      " - 0s - loss: 0.0965 - acc: 0.9709 - val_loss: 1.3925 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.68501\n",
      "Epoch 98/300\n",
      " - 0s - loss: 0.1074 - acc: 0.9647 - val_loss: 1.3521 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.68501\n",
      "Epoch 99/300\n",
      " - 0s - loss: 0.1005 - acc: 0.9751 - val_loss: 1.7332 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.68501\n",
      "Epoch 100/300\n",
      " - 0s - loss: 0.0792 - acc: 0.9813 - val_loss: 1.5429 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.68501\n",
      "Epoch 101/300\n",
      " - 0s - loss: 0.0564 - acc: 0.9854 - val_loss: 1.4329 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.68501\n",
      "Epoch 102/300\n",
      " - 0s - loss: 0.0836 - acc: 0.9792 - val_loss: 1.6683 - val_acc: 0.6224\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.68501\n",
      "Epoch 103/300\n",
      " - 0s - loss: 0.0587 - acc: 0.9896 - val_loss: 1.6011 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.68501\n",
      "Epoch 104/300\n",
      " - 0s - loss: 0.0494 - acc: 0.9917 - val_loss: 1.3872 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.68501\n",
      "Epoch 105/300\n",
      " - 0s - loss: 0.0493 - acc: 0.9917 - val_loss: 1.3991 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.68501\n",
      "Epoch 106/300\n",
      " - 0s - loss: 0.0636 - acc: 0.9854 - val_loss: 1.4234 - val_acc: 0.6471\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.68501\n",
      "Epoch 107/300\n",
      " - 0s - loss: 0.0460 - acc: 0.9917 - val_loss: 1.4728 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.68501\n",
      "Epoch 108/300\n",
      " - 0s - loss: 0.0568 - acc: 0.9792 - val_loss: 1.3736 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.68501\n",
      "Epoch 109/300\n",
      " - 0s - loss: 0.0598 - acc: 0.9834 - val_loss: 1.3505 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.68501\n",
      "Epoch 110/300\n",
      " - 0s - loss: 0.0701 - acc: 0.9792 - val_loss: 1.3341 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.68501\n",
      "Epoch 111/300\n",
      " - 0s - loss: 0.0414 - acc: 0.9958 - val_loss: 1.4127 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.68501\n",
      "Epoch 112/300\n",
      " - 0s - loss: 0.0564 - acc: 0.9938 - val_loss: 1.3238 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00112: val_acc improved from 0.68501 to 0.68501, saving model to ./weights/all_dimensions__weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/300\n",
      " - 0s - loss: 0.0556 - acc: 0.9834 - val_loss: 1.5100 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.68501\n",
      "Epoch 114/300\n",
      " - 0s - loss: 0.0461 - acc: 0.9917 - val_loss: 1.4448 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.68501\n",
      "Epoch 115/300\n",
      " - 0s - loss: 0.0528 - acc: 0.9875 - val_loss: 1.5352 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.68501\n",
      "Epoch 116/300\n",
      " - 0s - loss: 0.0651 - acc: 0.9792 - val_loss: 1.4936 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.68501\n",
      "Epoch 117/300\n",
      " - 0s - loss: 0.0531 - acc: 0.9938 - val_loss: 1.3239 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00117: val_acc improved from 0.68501 to 0.68691, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 118/300\n",
      " - 0s - loss: 0.0541 - acc: 0.9896 - val_loss: 1.3072 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00118: val_acc improved from 0.68691 to 0.69260, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 119/300\n",
      " - 0s - loss: 0.0439 - acc: 0.9917 - val_loss: 1.4907 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.69260\n",
      "Epoch 120/300\n",
      " - 0s - loss: 0.0436 - acc: 0.9917 - val_loss: 1.4756 - val_acc: 0.6471\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.69260\n",
      "Epoch 121/300\n",
      " - 0s - loss: 0.0703 - acc: 0.9834 - val_loss: 1.6667 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.69260\n",
      "Epoch 122/300\n",
      " - 0s - loss: 0.0489 - acc: 0.9875 - val_loss: 1.3813 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.69260\n",
      "Epoch 123/300\n",
      " - 0s - loss: 0.0477 - acc: 0.9917 - val_loss: 1.3923 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.69260\n",
      "Epoch 124/300\n",
      " - 0s - loss: 0.0490 - acc: 0.9896 - val_loss: 1.4869 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.69260\n",
      "Epoch 125/300\n",
      " - 0s - loss: 0.0520 - acc: 0.9938 - val_loss: 1.4732 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.69260\n",
      "Epoch 126/300\n",
      " - 0s - loss: 0.1051 - acc: 0.9647 - val_loss: 1.5156 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.69260\n",
      "Epoch 127/300\n",
      " - 0s - loss: 0.1206 - acc: 0.9667 - val_loss: 1.5064 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.69260\n",
      "Epoch 128/300\n",
      " - 0s - loss: 0.0708 - acc: 0.9751 - val_loss: 1.6653 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.69260\n",
      "Epoch 129/300\n",
      " - 0s - loss: 0.0614 - acc: 0.9834 - val_loss: 1.7912 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.69260\n",
      "Epoch 130/300\n",
      " - 0s - loss: 0.0821 - acc: 0.9751 - val_loss: 1.5499 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.69260\n",
      "Epoch 131/300\n",
      " - 0s - loss: 0.1098 - acc: 0.9584 - val_loss: 1.6386 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.69260\n",
      "Epoch 132/300\n",
      " - 0s - loss: 0.0727 - acc: 0.9854 - val_loss: 1.5137 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.69260\n",
      "Epoch 133/300\n",
      " - 0s - loss: 0.0569 - acc: 0.9896 - val_loss: 1.4471 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.69260\n",
      "Epoch 134/300\n",
      " - 0s - loss: 0.0508 - acc: 0.9896 - val_loss: 1.5903 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.69260\n",
      "Epoch 135/300\n",
      " - 0s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.5483 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.69260\n",
      "Epoch 136/300\n",
      " - 0s - loss: 0.0341 - acc: 0.9979 - val_loss: 1.4260 - val_acc: 0.6831\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.69260\n",
      "Epoch 137/300\n",
      " - 0s - loss: 0.0249 - acc: 0.9979 - val_loss: 1.3447 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.69260\n",
      "Epoch 138/300\n",
      " - 0s - loss: 0.0357 - acc: 0.9938 - val_loss: 1.3878 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.69260\n",
      "Epoch 139/300\n",
      " - 0s - loss: 0.0295 - acc: 0.9958 - val_loss: 1.4324 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.69260\n",
      "Epoch 140/300\n",
      " - 0s - loss: 0.0392 - acc: 0.9917 - val_loss: 1.3767 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.69260\n",
      "Epoch 141/300\n",
      " - 0s - loss: 0.0259 - acc: 0.9979 - val_loss: 1.3878 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00141: val_acc improved from 0.69260 to 0.69829, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 142/300\n",
      " - 0s - loss: 0.0286 - acc: 0.9938 - val_loss: 1.3503 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.69829\n",
      "Epoch 143/300\n",
      " - 0s - loss: 0.0381 - acc: 0.9896 - val_loss: 1.3706 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.69829\n",
      "Epoch 144/300\n",
      " - 0s - loss: 0.0247 - acc: 0.9979 - val_loss: 1.4559 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.69829\n",
      "Epoch 145/300\n",
      " - 0s - loss: 0.0307 - acc: 0.9938 - val_loss: 1.5758 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.69829\n",
      "Epoch 146/300\n",
      " - 0s - loss: 0.0207 - acc: 0.9958 - val_loss: 1.4289 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.69829\n",
      "Epoch 147/300\n",
      " - 0s - loss: 0.0240 - acc: 0.9979 - val_loss: 1.4368 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.69829\n",
      "Epoch 148/300\n",
      " - 0s - loss: 0.0208 - acc: 0.9979 - val_loss: 1.4763 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.69829\n",
      "Epoch 149/300\n",
      " - 0s - loss: 0.0186 - acc: 1.0000 - val_loss: 1.4266 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.69829\n",
      "Epoch 150/300\n",
      " - 0s - loss: 0.0126 - acc: 1.0000 - val_loss: 1.3612 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.69829\n",
      "Epoch 151/300\n",
      " - 0s - loss: 0.0156 - acc: 0.9979 - val_loss: 1.4459 - val_acc: 0.6831\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.69829\n",
      "Epoch 152/300\n",
      " - 0s - loss: 0.0150 - acc: 1.0000 - val_loss: 1.4408 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.69829\n",
      "Epoch 153/300\n",
      " - 0s - loss: 0.0186 - acc: 0.9979 - val_loss: 1.4574 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.69829\n",
      "Epoch 154/300\n",
      " - 0s - loss: 0.0159 - acc: 0.9979 - val_loss: 1.5361 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.69829\n",
      "Epoch 155/300\n",
      " - 0s - loss: 0.0300 - acc: 0.9938 - val_loss: 1.4145 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.69829\n",
      "Epoch 156/300\n",
      " - 0s - loss: 0.0207 - acc: 0.9938 - val_loss: 1.4342 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.69829\n",
      "Epoch 157/300\n",
      " - 0s - loss: 0.0216 - acc: 0.9958 - val_loss: 1.3953 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.69829\n",
      "Epoch 158/300\n",
      " - 0s - loss: 0.0153 - acc: 1.0000 - val_loss: 1.4722 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.69829\n",
      "Epoch 159/300\n",
      " - 0s - loss: 0.0228 - acc: 0.9958 - val_loss: 1.5353 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.69829\n",
      "Epoch 160/300\n",
      " - 0s - loss: 0.0198 - acc: 1.0000 - val_loss: 1.4226 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.69829\n",
      "Epoch 161/300\n",
      " - 0s - loss: 0.0145 - acc: 0.9979 - val_loss: 1.4745 - val_acc: 0.6831\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.69829\n",
      "Epoch 162/300\n",
      " - 0s - loss: 0.0270 - acc: 0.9896 - val_loss: 1.3954 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.69829\n",
      "Epoch 163/300\n",
      " - 0s - loss: 0.0227 - acc: 0.9938 - val_loss: 1.4403 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.69829\n",
      "Epoch 164/300\n",
      " - 0s - loss: 0.0179 - acc: 0.9938 - val_loss: 1.5585 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.69829\n",
      "Epoch 165/300\n",
      " - 0s - loss: 0.0175 - acc: 0.9958 - val_loss: 1.4974 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.69829\n",
      "Epoch 166/300\n",
      " - 0s - loss: 0.0168 - acc: 0.9979 - val_loss: 1.5338 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.69829\n",
      "Epoch 167/300\n",
      " - 0s - loss: 0.0224 - acc: 0.9938 - val_loss: 1.4895 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.69829\n",
      "Epoch 168/300\n",
      " - 0s - loss: 0.0196 - acc: 0.9979 - val_loss: 1.5584 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.69829\n",
      "Epoch 169/300\n",
      " - 0s - loss: 0.0163 - acc: 0.9958 - val_loss: 1.4409 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.69829\n",
      "Epoch 170/300\n",
      " - 0s - loss: 0.0134 - acc: 1.0000 - val_loss: 1.4302 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.69829\n",
      "Epoch 171/300\n",
      " - 0s - loss: 0.0151 - acc: 0.9958 - val_loss: 1.5001 - val_acc: 0.6945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00171: val_acc did not improve from 0.69829\n",
      "Epoch 172/300\n",
      " - 0s - loss: 0.0091 - acc: 1.0000 - val_loss: 1.6716 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.69829\n",
      "Epoch 173/300\n",
      " - 0s - loss: 0.0139 - acc: 0.9979 - val_loss: 1.5708 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.69829\n",
      "Epoch 174/300\n",
      " - 0s - loss: 0.0186 - acc: 0.9958 - val_loss: 1.5010 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.69829\n",
      "Epoch 175/300\n",
      " - 0s - loss: 0.0184 - acc: 0.9958 - val_loss: 1.5710 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.69829\n",
      "Epoch 176/300\n",
      " - 0s - loss: 0.0091 - acc: 1.0000 - val_loss: 1.5725 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.69829\n",
      "Epoch 177/300\n",
      " - 0s - loss: 0.0205 - acc: 0.9958 - val_loss: 1.5611 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.69829\n",
      "Epoch 178/300\n",
      " - 0s - loss: 0.0187 - acc: 0.9958 - val_loss: 1.5887 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.69829\n",
      "Epoch 179/300\n",
      " - 0s - loss: 0.0101 - acc: 0.9979 - val_loss: 1.5754 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.69829\n",
      "Epoch 180/300\n",
      " - 0s - loss: 0.0236 - acc: 0.9896 - val_loss: 1.4547 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.69829\n",
      "Epoch 181/300\n",
      " - 0s - loss: 0.0339 - acc: 0.9938 - val_loss: 1.3793 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.69829\n",
      "Epoch 182/300\n",
      " - 0s - loss: 0.0231 - acc: 0.9958 - val_loss: 1.6588 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.69829\n",
      "Epoch 183/300\n",
      " - 0s - loss: 0.0479 - acc: 0.9854 - val_loss: 1.7327 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.69829\n",
      "Epoch 184/300\n",
      " - 0s - loss: 0.0363 - acc: 0.9875 - val_loss: 1.5867 - val_acc: 0.6471\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.69829\n",
      "Epoch 185/300\n",
      " - 0s - loss: 0.0541 - acc: 0.9813 - val_loss: 1.6353 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.69829\n",
      "Epoch 186/300\n",
      " - 0s - loss: 0.0518 - acc: 0.9813 - val_loss: 1.7897 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.69829\n",
      "Epoch 187/300\n",
      " - 0s - loss: 0.2035 - acc: 0.9501 - val_loss: 1.9270 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.69829\n",
      "Epoch 188/300\n",
      " - 0s - loss: 0.1177 - acc: 0.9709 - val_loss: 2.2991 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.69829\n",
      "Epoch 189/300\n",
      " - 0s - loss: 0.0965 - acc: 0.9688 - val_loss: 2.1379 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.69829\n",
      "Epoch 190/300\n",
      " - 0s - loss: 0.0768 - acc: 0.9792 - val_loss: 1.7130 - val_acc: 0.6338\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.69829\n",
      "Epoch 191/300\n",
      " - 0s - loss: 0.0954 - acc: 0.9751 - val_loss: 1.5385 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.69829\n",
      "Epoch 192/300\n",
      " - 0s - loss: 0.0815 - acc: 0.9730 - val_loss: 1.6377 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.69829\n",
      "Epoch 193/300\n",
      " - 0s - loss: 0.0776 - acc: 0.9792 - val_loss: 1.5193 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.69829\n",
      "Epoch 194/300\n",
      " - 0s - loss: 0.1044 - acc: 0.9647 - val_loss: 1.5888 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.69829\n",
      "Epoch 195/300\n",
      " - 0s - loss: 0.0551 - acc: 0.9896 - val_loss: 1.7533 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.69829\n",
      "Epoch 196/300\n",
      " - 0s - loss: 0.0488 - acc: 0.9896 - val_loss: 1.5238 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.69829\n",
      "Epoch 197/300\n",
      " - 0s - loss: 0.0337 - acc: 0.9979 - val_loss: 1.4747 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.69829\n",
      "Epoch 198/300\n",
      " - 0s - loss: 0.0224 - acc: 1.0000 - val_loss: 1.5418 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.69829\n",
      "Epoch 199/300\n",
      " - 0s - loss: 0.0293 - acc: 0.9958 - val_loss: 1.5167 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.69829\n",
      "Epoch 200/300\n",
      " - 0s - loss: 0.0191 - acc: 0.9979 - val_loss: 1.5340 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.69829\n",
      "Epoch 201/300\n",
      " - 0s - loss: 0.0221 - acc: 0.9938 - val_loss: 1.4732 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.69829\n",
      "Epoch 202/300\n",
      " - 0s - loss: 0.0189 - acc: 0.9979 - val_loss: 1.5835 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.69829\n",
      "Epoch 203/300\n",
      " - 0s - loss: 0.0296 - acc: 0.9896 - val_loss: 1.5456 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.69829\n",
      "Epoch 204/300\n",
      " - 0s - loss: 0.0226 - acc: 0.9979 - val_loss: 1.4623 - val_acc: 0.6964\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.69829\n",
      "Epoch 205/300\n",
      " - 0s - loss: 0.0233 - acc: 0.9938 - val_loss: 1.5196 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.69829\n",
      "Epoch 206/300\n",
      " - 0s - loss: 0.0317 - acc: 0.9854 - val_loss: 1.6211 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.69829\n",
      "Epoch 207/300\n",
      " - 0s - loss: 0.0265 - acc: 0.9938 - val_loss: 1.6002 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.69829\n",
      "Epoch 208/300\n",
      " - 0s - loss: 0.0571 - acc: 0.9792 - val_loss: 1.7687 - val_acc: 0.6376\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.69829\n",
      "Epoch 209/300\n",
      " - 0s - loss: 0.0303 - acc: 0.9938 - val_loss: 1.6558 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.69829\n",
      "Epoch 210/300\n",
      " - 0s - loss: 0.0334 - acc: 0.9896 - val_loss: 1.5721 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.69829\n",
      "Epoch 211/300\n",
      " - 0s - loss: 0.0192 - acc: 0.9958 - val_loss: 1.5737 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.69829\n",
      "Epoch 212/300\n",
      " - 0s - loss: 0.0224 - acc: 0.9958 - val_loss: 1.6229 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.69829\n",
      "Epoch 213/300\n",
      " - 0s - loss: 0.0159 - acc: 0.9979 - val_loss: 1.8029 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.69829\n",
      "Epoch 214/300\n",
      " - 0s - loss: 0.0176 - acc: 1.0000 - val_loss: 1.6022 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.69829\n",
      "Epoch 215/300\n",
      " - 0s - loss: 0.0179 - acc: 0.9958 - val_loss: 1.6274 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.69829\n",
      "Epoch 216/300\n",
      " - 0s - loss: 0.0145 - acc: 0.9979 - val_loss: 1.8337 - val_acc: 0.6148\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.69829\n",
      "Epoch 217/300\n",
      " - 0s - loss: 0.0303 - acc: 0.9938 - val_loss: 1.6822 - val_acc: 0.6433\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.69829\n",
      "Epoch 218/300\n",
      " - 0s - loss: 0.0124 - acc: 1.0000 - val_loss: 1.6026 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.69829\n",
      "Epoch 219/300\n",
      " - 0s - loss: 0.0225 - acc: 0.9958 - val_loss: 1.5653 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.69829\n",
      "Epoch 220/300\n",
      " - 0s - loss: 0.0112 - acc: 1.0000 - val_loss: 1.5044 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.69829\n",
      "Epoch 221/300\n",
      " - 0s - loss: 0.0213 - acc: 0.9958 - val_loss: 1.4944 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.69829\n",
      "Epoch 222/300\n",
      " - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 1.5351 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.69829\n",
      "Epoch 223/300\n",
      " - 0s - loss: 0.0105 - acc: 0.9979 - val_loss: 1.5264 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.69829\n",
      "Epoch 224/300\n",
      " - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 1.5012 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.69829\n",
      "Epoch 225/300\n",
      " - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 1.5654 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.69829\n",
      "Epoch 226/300\n",
      " - 0s - loss: 0.0089 - acc: 1.0000 - val_loss: 1.6476 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.69829\n",
      "Epoch 227/300\n",
      " - 0s - loss: 0.0079 - acc: 0.9979 - val_loss: 1.6750 - val_acc: 0.6528\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.69829\n",
      "Epoch 228/300\n",
      " - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 1.6497 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.69829\n",
      "Epoch 229/300\n",
      " - 0s - loss: 0.0191 - acc: 0.9938 - val_loss: 1.5699 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.69829\n",
      "Epoch 230/300\n",
      " - 0s - loss: 0.0159 - acc: 0.9958 - val_loss: 1.5716 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.69829\n",
      "Epoch 231/300\n",
      " - 0s - loss: 0.0221 - acc: 0.9896 - val_loss: 1.6904 - val_acc: 0.6471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00231: val_acc did not improve from 0.69829\n",
      "Epoch 232/300\n",
      " - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 1.7407 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.69829\n",
      "Epoch 233/300\n",
      " - 0s - loss: 0.0189 - acc: 0.9958 - val_loss: 1.7465 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.69829\n",
      "Epoch 234/300\n",
      " - 0s - loss: 0.0106 - acc: 1.0000 - val_loss: 1.6351 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.69829\n",
      "Epoch 235/300\n",
      " - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 1.5517 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.69829\n",
      "Epoch 236/300\n",
      " - 0s - loss: 0.0182 - acc: 0.9938 - val_loss: 1.5434 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00236: val_acc improved from 0.69829 to 0.69829, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 237/300\n",
      " - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 1.5918 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.69829\n",
      "Epoch 238/300\n",
      " - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 1.6248 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.69829\n",
      "Epoch 239/300\n",
      " - 0s - loss: 0.0053 - acc: 1.0000 - val_loss: 1.6302 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.69829\n",
      "Epoch 240/300\n",
      " - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 1.6138 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.69829\n",
      "Epoch 241/300\n",
      " - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 1.5861 - val_acc: 0.6831\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.69829\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.0007937005636828516.\n",
      "Epoch 242/300\n",
      " - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 1.5568 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.69829\n",
      "Epoch 243/300\n",
      " - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 1.5213 - val_acc: 0.7040\n",
      "\n",
      "Epoch 00243: val_acc improved from 0.69829 to 0.70398, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 244/300\n",
      " - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 1.5153 - val_acc: 0.7078\n",
      "\n",
      "Epoch 00244: val_acc improved from 0.70398 to 0.70778, saving model to ./weights/all_dimensions__weights.h5\n",
      "Epoch 245/300\n",
      " - 0s - loss: 0.0045 - acc: 1.0000 - val_loss: 1.5504 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.70778\n",
      "Epoch 246/300\n",
      " - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 1.5614 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.70778\n",
      "Epoch 247/300\n",
      " - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 1.5556 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.70778\n",
      "Epoch 248/300\n",
      " - 0s - loss: 0.0050 - acc: 1.0000 - val_loss: 1.5452 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.70778\n",
      "Epoch 249/300\n",
      " - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 1.5476 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.70778\n",
      "Epoch 250/300\n",
      " - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 1.5566 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.70778\n",
      "Epoch 251/300\n",
      " - 0s - loss: 0.0054 - acc: 1.0000 - val_loss: 1.5810 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.70778\n",
      "Epoch 252/300\n",
      " - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 1.6255 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.70778\n",
      "Epoch 253/300\n",
      " - 0s - loss: 0.0042 - acc: 1.0000 - val_loss: 1.6421 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.70778\n",
      "Epoch 254/300\n",
      " - 0s - loss: 0.0045 - acc: 1.0000 - val_loss: 1.6299 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.70778\n",
      "Epoch 255/300\n",
      " - 0s - loss: 0.0033 - acc: 1.0000 - val_loss: 1.6002 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.70778\n",
      "Epoch 256/300\n",
      " - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 1.5741 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.70778\n",
      "Epoch 257/300\n",
      " - 0s - loss: 0.0053 - acc: 1.0000 - val_loss: 1.5762 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.70778\n",
      "Epoch 258/300\n",
      " - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 1.6143 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.70778\n",
      "Epoch 259/300\n",
      " - 0s - loss: 0.0031 - acc: 1.0000 - val_loss: 1.6662 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.70778\n",
      "Epoch 260/300\n",
      " - 0s - loss: 0.0048 - acc: 1.0000 - val_loss: 1.6738 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.70778\n",
      "Epoch 261/300\n",
      " - 0s - loss: 0.0051 - acc: 0.9979 - val_loss: 1.6288 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.70778\n",
      "Epoch 262/300\n",
      " - 0s - loss: 0.0051 - acc: 1.0000 - val_loss: 1.5893 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.70778\n",
      "Epoch 263/300\n",
      " - 0s - loss: 0.0039 - acc: 1.0000 - val_loss: 1.5562 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.70778\n",
      "Epoch 264/300\n",
      " - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 1.5509 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.70778\n",
      "Epoch 265/300\n",
      " - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 1.5996 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.70778\n",
      "Epoch 266/300\n",
      " - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 1.6238 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.70778\n",
      "Epoch 267/300\n",
      " - 0s - loss: 0.0124 - acc: 0.9958 - val_loss: 1.6463 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.70778\n",
      "Epoch 268/300\n",
      " - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 1.7630 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.70778\n",
      "Epoch 269/300\n",
      " - 0s - loss: 0.0177 - acc: 0.9958 - val_loss: 1.6688 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.70778\n",
      "Epoch 270/300\n",
      " - 0s - loss: 0.0035 - acc: 1.0000 - val_loss: 1.7116 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.70778\n",
      "Epoch 271/300\n",
      " - 0s - loss: 0.0200 - acc: 0.9938 - val_loss: 1.6906 - val_acc: 0.6698\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.70778\n",
      "Epoch 272/300\n",
      " - 0s - loss: 0.0107 - acc: 0.9979 - val_loss: 1.7786 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.70778\n",
      "Epoch 273/300\n",
      " - 0s - loss: 0.0095 - acc: 0.9958 - val_loss: 1.7579 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.70778\n",
      "Epoch 274/300\n",
      " - 0s - loss: 0.0098 - acc: 0.9979 - val_loss: 1.6424 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.70778\n",
      "Epoch 275/300\n",
      " - 0s - loss: 0.0085 - acc: 0.9979 - val_loss: 1.6153 - val_acc: 0.6679\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.70778\n",
      "Epoch 276/300\n",
      " - 0s - loss: 0.0202 - acc: 0.9979 - val_loss: 1.7319 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.70778\n",
      "Epoch 277/300\n",
      " - 0s - loss: 0.0299 - acc: 0.9854 - val_loss: 1.5839 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.70778\n",
      "Epoch 278/300\n",
      " - 0s - loss: 0.0088 - acc: 0.9958 - val_loss: 1.6406 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.70778\n",
      "Epoch 279/300\n",
      " - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 1.7933 - val_acc: 0.6622\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.70778\n",
      "Epoch 280/300\n",
      " - 0s - loss: 0.0224 - acc: 0.9917 - val_loss: 1.7611 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.70778\n",
      "Epoch 281/300\n",
      " - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 1.7766 - val_acc: 0.6565\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.70778\n",
      "Epoch 282/300\n",
      " - 0s - loss: 0.0186 - acc: 0.9958 - val_loss: 1.9217 - val_acc: 0.6357\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.70778\n",
      "Epoch 283/300\n",
      " - 0s - loss: 0.0169 - acc: 0.9958 - val_loss: 1.9069 - val_acc: 0.6471\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.70778\n",
      "Epoch 284/300\n",
      " - 0s - loss: 0.0113 - acc: 1.0000 - val_loss: 1.7515 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.70778\n",
      "Epoch 285/300\n",
      " - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 1.7588 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.70778\n",
      "Epoch 286/300\n",
      " - 0s - loss: 0.0084 - acc: 0.9979 - val_loss: 1.8244 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.70778\n",
      "Epoch 287/300\n",
      " - 0s - loss: 0.0253 - acc: 0.9938 - val_loss: 1.7679 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.70778\n",
      "Epoch 288/300\n",
      " - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 1.7029 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.70778\n",
      "Epoch 289/300\n",
      " - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 1.7259 - val_acc: 0.6622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00289: val_acc did not improve from 0.70778\n",
      "Epoch 290/300\n",
      " - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 1.7169 - val_acc: 0.6641\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.70778\n",
      "Epoch 291/300\n",
      " - 0s - loss: 0.0064 - acc: 0.9979 - val_loss: 1.6412 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.70778\n",
      "Epoch 292/300\n",
      " - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 1.6362 - val_acc: 0.6869\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.70778\n",
      "Epoch 293/300\n",
      " - 0s - loss: 0.0040 - acc: 1.0000 - val_loss: 1.6585 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.70778\n",
      "Epoch 294/300\n",
      " - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 1.6872 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.70778\n",
      "Epoch 295/300\n",
      " - 0s - loss: 0.0042 - acc: 1.0000 - val_loss: 1.7137 - val_acc: 0.6736\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.70778\n",
      "Epoch 296/300\n",
      " - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 1.6253 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.70778\n",
      "Epoch 297/300\n",
      " - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 1.6417 - val_acc: 0.6888\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.70778\n",
      "Epoch 298/300\n",
      " - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 1.7188 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.70778\n",
      "Epoch 299/300\n",
      " - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 1.7244 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.70778\n",
      "Epoch 300/300\n",
      " - 0s - loss: 0.0035 - acc: 1.0000 - val_loss: 1.6537 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.70778\n"
     ]
    }
   ],
   "source": [
    "train_model(model, DATASET_INDEX, dataset_prefix=ds+'_', epochs=300, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLSTM-FCN] Results on dataset WITH Speed (8 classes): \n",
      " Class_sample_threshold=1.7%\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  481 Number of test samples :  527\n",
      "Number of classes :  8\n",
      "Sequence length :  10\n",
      "X_train.shape is  (481, 8, 10)\n",
      "X_test.shape is  (527, 8, 10)\n",
      "\n",
      "Evaluating : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.54      0.48        52\n",
      "           1       0.41      0.65      0.50        20\n",
      "           2       0.90      0.88      0.89       129\n",
      "           3       0.75      0.75      0.75         8\n",
      "           4       0.80      0.80      0.80       132\n",
      "           5       0.80      0.60      0.69       151\n",
      "           6       0.38      0.48      0.42        25\n",
      "           7       0.22      0.40      0.29        10\n",
      "\n",
      "    accuracy                           0.71       527\n",
      "   macro avg       0.59      0.64      0.60       527\n",
      "weighted avg       0.74      0.71      0.72       527\n",
      "\n",
      "Final Accuracy %.4d :  0.7077798869397202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7077798869397202, 1.5153405508008808)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[MLSTM-FCN] Results on dataset WITH Speed (8 classes): \\n Class_sample_threshold=1.7%\")\n",
    "evaluate_model(model, DATASET_INDEX, dataset_prefix=ds+'_', batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
