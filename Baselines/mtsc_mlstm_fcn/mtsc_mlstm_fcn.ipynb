{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data format conversion for MLSTM-FCN\n",
    "===\n",
    "\n",
    "\n",
    "---\n",
    "Input\n",
    "---\n",
    "\n",
    "A single file contains all samples and their labels: ***L * (3 + D)***\n",
    "\n",
    "\n",
    "\n",
    "- 1st col: sample_id\n",
    "- 2nd col: timestamps\n",
    "- 3rd col: label\n",
    "- after the 4th col: mts vector with D dimensions   \n",
    "\n",
    "---\n",
    "Output\n",
    "---\n",
    "\n",
    "Two array-like variables\n",
    "\n",
    "- X : array with shape (n_ts, d, sz)\n",
    "        Sequence data.\n",
    "- y : array with shape (n_ts, 1)\n",
    "        Target labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('../../../SMATE_MTS'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils.UEA_utils import *\n",
    "#%run ../../utils/PolluScope_utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess\n",
    "\n",
    "proc=subprocess.Popen('/bin/sh',stdout=subprocess.PIPE,stdin=subprocess.PIPE,stderr=subprocess.STDOUT)\n",
    "pout=proc.stdout\n",
    "pin=proc.stdin\n",
    "\n",
    "def outLoop():\n",
    "    running=True\n",
    "    while(running):\n",
    "        line=pout.readline().decode(sys.stdout.encoding)\n",
    "        print(line,end='')\n",
    "        running='\\n' in line\n",
    "    print('Finished')\n",
    "\n",
    "threading.Thread(target=outLoop).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15_OverSample_X_test.npy\n",
      "15_OverSample_X_train.npy\n",
      "15_OverSample_y_test.npy\n",
      "15_OverSample_y_train.npy\n",
      "15_X_test.npy\n",
      "15_X_train.npy\n",
      "15_y_test.npy\n",
      "15_y_train.npy\n",
      "17_OverSample_X_test.npy\n",
      "17_OverSample_X_train.npy\n",
      "17_OverSample_y_test.npy\n",
      "17_OverSample_y_train.npy\n",
      "17_X_test.npy\n",
      "17_X_train.npy\n",
      "17_y_test.npy\n",
      "17_y_train.npy\n",
      "18_OverSample_X_test.npy\n",
      "18_OverSample_X_train.npy\n",
      "18_OverSample_y_test.npy\n",
      "18_OverSample_y_train.npy\n",
      "18_X_test.npy\n",
      "18_X_train.npy\n",
      "18_y_test.npy\n",
      "18_y_train.npy\n",
      "all_dimensions_Test.csv\n",
      "all_dimensions_Train.csv\n",
      "polluscop_speed.zip\n",
      "X_test.npy\n",
      "X_train.npy\n",
      "y_test.npy\n",
      "y_train.npy\n"
     ]
    }
   ],
   "source": [
    "pin.write(b' ls /ssd/jzuo/SMATE_MTS/Datasets/MTS-others/PolluScope_16_12_2020_speed_complet/all_dimensions \\n')\n",
    "pin.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      "class list (norm) is [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n"
     ]
    }
   ],
   "source": [
    "'''=================================================== Prepare PolluScope data ========================================================'''\n",
    "\n",
    "'''\n",
    "rep = \"/ssd/jzuo/SMATE_MTS/Datasets/MTS-others/PolluScope_16_12_2020_speed_complet/\"\n",
    "ds = \"all_dimensions\"\n",
    "ds_train = ds + '/' + ds + \"_Train.csv\"\n",
    "ds_test = ds + '/' + ds + \"_Test.csv\"\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_datasets(rep, ds_train, ds_test, z_normal = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is ['E35' 'E38' 'E40' 'E45']\n",
      "total number of samples is 261\n",
      "total number of samples is 263\n"
     ]
    }
   ],
   "source": [
    "'''=================================================== Prepare UEA data ========================================================'''\n",
    "\n",
    "rep = \"../../../Datasets/MTS-UEA/\"\n",
    "ds = \"EthanolConcentration\"\n",
    "rep_ds_train = rep + ds + \"/output_train/\"\n",
    "rep_ds_test = rep + ds + \"/output_test/\"\n",
    "meta_csv = \"meta_data.csv\"  # the meta data of training/testing set\n",
    "rep_output = rep_ds_train + \"out_results/\"  # output results, e.g., training loss, models\n",
    "os.system(\"mkdir -p \" + rep_output)\n",
    "sup_ratio = 1\n",
    "\n",
    "# prepare UEA datasets form 'arff' files\n",
    "dataset = get_UEA_dataset(rep_ds_train, rep_ds_test, meta_csv, sup_ratio, mode = 'load', split_strategy='EqualSplit')\n",
    "\n",
    "X_train = dataset['X_train']\n",
    "y_train = dataset['Y_train']\n",
    "X_test = dataset['X_test']\n",
    "y_test = dataset['Y_test']\n",
    "X_sup = dataset['X_sup']  # 3-D Array: N * L * D\n",
    "X_unsup = dataset['X_unsup']\n",
    "y_sup = dataset['Y_sup']  # 1-D Array\n",
    "y_unsup = dataset['Y_unsup']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset :  (261, 3, 1751) (261,)\n",
      "Test dataset :  (263, 3, 1751) (263,)\n",
      "Train dataset metrics :  0.33776522185413554 0.3550588786133562\n",
      "Test dataset :  0.3382842365567585 0.35526999693239597\n",
      "Nb classes train:  4\n",
      "Nb classes test:  4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "NB_CLASS = len(np.unique(y_train))\n",
    "MAX_TIMESTEPS =X_train.shape[-1]\n",
    "MAX_NB_VARIABLES = X_train.shape[-2]\n",
    "# Save the datasets\n",
    "print(\"Train dataset : \", X_train.shape, y_train.shape)\n",
    "print(\"Test dataset : \", X_test.shape, y_test.shape)\n",
    "print(\"Train dataset metrics : \", X_train.mean(), X_train.std())\n",
    "print(\"Test dataset : \", X_test.mean(), X_test.std())\n",
    "print(\"Nb classes train: \", len(np.unique(y_train)))\n",
    "print(\"Nb classes test: \", len(np.unique(y_test)))\n",
    "\n",
    "np.save(rep + ds + '/X_train.npy', X_train)\n",
    "np.save(rep + ds + '/y_train.npy', y_train)\n",
    "np.save(rep + ds + '/X_test.npy', X_test)\n",
    "np.save(rep + ds + '/y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_labels(y):\n",
    "    '''\n",
    "    Convert the classes in dataset into training labels in Keras [0, nbr_class-1]\n",
    "\n",
    "    class_array: an array of classes for samples in dataset\n",
    "    '''\n",
    "    classes, counts_cl = np.unique(y, return_counts=True)\n",
    "    print(\"class list is \" + str(classes))\n",
    "\n",
    "    mapping_c_l = {}  # a mappling between classes and labels\n",
    "    for idx, c in enumerate(list(classes)):\n",
    "        mapping_c_l.update({c: idx})\n",
    "    return mapping_c_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is [ 1.  2.  3.  4.  5.  7.  8. 10.]\n",
      "Train dataset :  (481, 8, 10) (481,)\n",
      "Test dataset :  (527, 8, 10) (527,)\n",
      "Train dataset metrics :  201.29303546119087 833.440601611844\n",
      "Test dataset :  201.43323874758403 1104.3744524147357\n",
      "Nb classes :  8\n",
      "Nb classes train:  8\n",
      "Nb classes test:  8\n"
     ]
    }
   ],
   "source": [
    "'''=============================Prepare PolluScope data From '.npy' file ==============================='''\n",
    "\n",
    "rep = \"/ssd/jzuo/SMATE_MTS/Datasets/MTS-others/PolluScope_16_12_2020_speed_complet/\"\n",
    "ds = \"all_dimensions\"\n",
    "threshold_taux = 0.017\n",
    "X_train = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'X_train.npy') #\"_OverSample_\"\n",
    "y_train = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'y_train.npy')\n",
    "X_test = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'X_test.npy')\n",
    "y_test = np.load(rep + ds + '/' + str(int(threshold_taux*1000)) + '_OverSample_'+ 'y_test.npy')\n",
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "'''********* Remove *speed* dimension *********'''\n",
    "#X_train = X_train[:, :-1, :]\n",
    "#X_test = X_test[:, :-1, :]\n",
    "\n",
    "NB_CLASS = len(np.unique(y_train))\n",
    "MAX_TIMESTEPS =X_train.shape[-1]\n",
    "MAX_NB_VARIABLES = X_train.shape[-2] \n",
    "\n",
    "if np.max(y_train) != len(y_train) - 1 :\n",
    "    mapping_c_l = get_mapping_labels(y_train)\n",
    "    y_train = np.array([mapping_c_l[i] for i in y_train])\n",
    "    y_test = np.array([mapping_c_l[i] for i in y_test])\n",
    "\n",
    "print(\"Train dataset : \", X_train.shape, y_train.shape)\n",
    "print(\"Test dataset : \", X_test.shape, y_test.shape)\n",
    "print(\"Train dataset metrics : \", X_train.mean(), X_train.std())\n",
    "print(\"Test dataset : \", X_test.mean(), X_test.std())\n",
    "print(\"Nb classes : \", len(np.unique(y_train)))\n",
    "print(\"Nb classes train: \", len(np.unique(y_train)))\n",
    "print(\"Nb classes test: \", len(np.unique(y_test)))\n",
    "\n",
    "np.save(rep + ds + '/X_train.npy', X_train)\n",
    "np.save(rep + ds + '/y_train.npy', y_train)\n",
    "np.save(rep + ds + '/X_test.npy', X_test)\n",
    "np.save(rep + ds + '/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3.] \n",
      " [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# the labels should range from 0 to nbr_class\n",
    "print(np.unique(y_train), '\\n', np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Build and train the Network Model\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "from utils_mlstm.keras_utils import train_model, evaluate_model, set_trainable\n",
    "from utils_mlstm.layer_utils import AttentionLSTM\n",
    "\n",
    "TRAINABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(128)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_model_2():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "    # stride = 10\n",
    "\n",
    "    # x = Permute((2, 1))(ip)\n",
    "    # x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "    #            kernel_initializer='he_uniform')(x)  # (None, variables / stride, timesteps)\n",
    "    # x = Permute((2, 1))(x)\n",
    "\n",
    "    #ip1 = K.reshape(ip,shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n",
    "    #x = Permute((2, 1))(ip)\n",
    "    x = Masking()(ip)\n",
    "    x = AttentionLSTM(128)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input._keras_shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 02:57:36.877216 23335415461696 deprecation_wrapper.py:119] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0106 02:57:37.818396 23335415461696 deprecation.py:323] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0106 02:57:37.853159 23335415461696 deprecation.py:506] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0106 02:57:37.853790 23335415461696 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 1751)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 1751, 3)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1751, 128)    3200        permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1751, 128)    512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1751, 128)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 128)       0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 8)         1024        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 128)       1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1751, 128)    0           activation_1[0][0]               \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1751, 256)    164096      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1751, 256)    1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1751, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 16)        4096        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 256)       4096        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 1751, 256)    0           activation_2[0][0]               \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1751, 128)    98432       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 3, 1751)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1751, 128)    512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_lstm_1 (AttentionLSTM (None, 128)          2099840     masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1751, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           attention_lstm_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           dropout_1[0][0]                  \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            1028        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,378,884\n",
      "Trainable params: 2,377,860\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = generate_model_2()\n",
    "\n",
    "DATASET_INDEX = rep + ds + '/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  261 Number of test samples :  263\n",
      "Number of classes :  4\n",
      "Sequence length :  1751\n",
      "X_train.shape is  (261, 3, 1751)\n",
      "X_test.shape is  (263, 3, 1751)\n",
      "Class weights :  [1.00384615 1.00384615 0.98863636 1.00384615]\n",
      "Train on 261 samples, validate on 263 samples\n",
      "Epoch 1/300\n",
      " - 4s - loss: 1.0462 - acc: 0.5517 - val_loss: 2.2334 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.25475, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 2/300\n",
      " - 0s - loss: 1.0089 - acc: 0.5747 - val_loss: 1.9680 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.25475\n",
      "Epoch 3/300\n",
      " - 0s - loss: 1.0295 - acc: 0.5556 - val_loss: 1.7834 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.25475 to 0.26236, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 4/300\n",
      " - 0s - loss: 1.0666 - acc: 0.5287 - val_loss: 1.6657 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.26236 to 0.26996, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 5/300\n",
      " - 0s - loss: 1.0432 - acc: 0.5517 - val_loss: 1.7102 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.26996\n",
      "Epoch 6/300\n",
      " - 0s - loss: 1.0429 - acc: 0.5479 - val_loss: 1.6781 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.26996\n",
      "Epoch 7/300\n",
      " - 0s - loss: 1.0671 - acc: 0.5326 - val_loss: 1.7158 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.26996\n",
      "Epoch 8/300\n",
      " - 0s - loss: 1.0243 - acc: 0.5709 - val_loss: 1.7764 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.26996\n",
      "Epoch 9/300\n",
      " - 0s - loss: 1.0355 - acc: 0.5594 - val_loss: 1.7636 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.26996\n",
      "Epoch 10/300\n",
      " - 0s - loss: 1.0333 - acc: 0.5747 - val_loss: 1.6994 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.26996 to 0.28517, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 11/300\n",
      " - 0s - loss: 1.0563 - acc: 0.5441 - val_loss: 1.6573 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.28517 to 0.30418, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 12/300\n",
      " - 0s - loss: 1.0638 - acc: 0.5441 - val_loss: 1.6356 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.30418\n",
      "Epoch 13/300\n",
      " - 0s - loss: 1.0354 - acc: 0.5670 - val_loss: 1.6476 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.30418\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.9848 - acc: 0.5709 - val_loss: 1.6374 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.30418\n",
      "Epoch 15/300\n",
      " - 0s - loss: 1.0159 - acc: 0.5747 - val_loss: 1.7370 - val_acc: 0.2205\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.30418\n",
      "Epoch 16/300\n",
      " - 0s - loss: 1.0325 - acc: 0.5517 - val_loss: 1.9184 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.30418\n",
      "Epoch 17/300\n",
      " - 0s - loss: 1.0329 - acc: 0.5479 - val_loss: 2.0139 - val_acc: 0.2205\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.30418\n",
      "Epoch 18/300\n",
      " - 0s - loss: 1.0254 - acc: 0.5747 - val_loss: 2.0085 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.30418\n",
      "Epoch 19/300\n",
      " - 0s - loss: 1.0203 - acc: 0.5479 - val_loss: 2.0341 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.30418\n",
      "Epoch 20/300\n",
      " - 0s - loss: 1.0297 - acc: 0.5632 - val_loss: 2.4114 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.30418\n",
      "Epoch 21/300\n",
      " - 0s - loss: 1.0073 - acc: 0.5824 - val_loss: 2.8424 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.30418\n",
      "Epoch 22/300\n",
      " - 0s - loss: 1.0278 - acc: 0.5594 - val_loss: 3.0143 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.30418\n",
      "Epoch 23/300\n",
      " - 0s - loss: 1.0247 - acc: 0.5441 - val_loss: 2.9467 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.30418\n",
      "Epoch 24/300\n",
      " - 0s - loss: 1.0193 - acc: 0.5556 - val_loss: 2.6922 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.30418\n",
      "Epoch 25/300\n",
      " - 0s - loss: 1.0198 - acc: 0.5479 - val_loss: 2.2871 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.30418\n",
      "Epoch 26/300\n",
      " - 0s - loss: 0.9780 - acc: 0.5517 - val_loss: 1.8732 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.30418\n",
      "Epoch 27/300\n",
      " - 0s - loss: 0.9963 - acc: 0.5709 - val_loss: 1.9501 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.30418\n",
      "Epoch 28/300\n",
      " - 0s - loss: 1.0187 - acc: 0.5556 - val_loss: 2.6164 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.30418\n",
      "Epoch 29/300\n",
      " - 0s - loss: 1.0101 - acc: 0.5402 - val_loss: 3.1423 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.30418\n",
      "Epoch 30/300\n",
      " - 0s - loss: 1.0284 - acc: 0.5326 - val_loss: 3.0619 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.30418\n",
      "Epoch 31/300\n",
      " - 0s - loss: 1.0161 - acc: 0.5326 - val_loss: 2.7850 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.30418\n",
      "Epoch 32/300\n",
      " - 0s - loss: 0.9962 - acc: 0.5824 - val_loss: 2.6344 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.30418\n",
      "Epoch 33/300\n",
      " - 0s - loss: 1.0037 - acc: 0.5632 - val_loss: 2.6202 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.30418\n",
      "Epoch 34/300\n",
      " - 0s - loss: 1.0450 - acc: 0.5211 - val_loss: 2.4740 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.30418\n",
      "Epoch 35/300\n",
      " - 0s - loss: 1.0518 - acc: 0.5211 - val_loss: 2.2100 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.30418\n",
      "Epoch 36/300\n",
      " - 0s - loss: 1.0239 - acc: 0.5441 - val_loss: 1.9162 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.30418\n",
      "Epoch 37/300\n",
      " - 0s - loss: 1.0173 - acc: 0.5670 - val_loss: 1.8818 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.30418\n",
      "Epoch 38/300\n",
      " - 0s - loss: 0.9955 - acc: 0.5632 - val_loss: 2.0581 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.30418 to 0.30798, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 39/300\n",
      " - 0s - loss: 1.0079 - acc: 0.5517 - val_loss: 2.2313 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.30798\n",
      "Epoch 40/300\n",
      " - 0s - loss: 1.0134 - acc: 0.5594 - val_loss: 2.2221 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.30798\n",
      "Epoch 41/300\n",
      " - 0s - loss: 1.0078 - acc: 0.5402 - val_loss: 1.9580 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.30798\n",
      "Epoch 42/300\n",
      " - 0s - loss: 0.9843 - acc: 0.5517 - val_loss: 1.7970 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.30798\n",
      "Epoch 43/300\n",
      " - 0s - loss: 1.0279 - acc: 0.5172 - val_loss: 1.8360 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.30798\n",
      "Epoch 44/300\n",
      " - 0s - loss: 1.0193 - acc: 0.5287 - val_loss: 1.9799 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.30798\n",
      "Epoch 45/300\n",
      " - 0s - loss: 1.0229 - acc: 0.5249 - val_loss: 2.6894 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.30798\n",
      "Epoch 46/300\n",
      " - 0s - loss: 1.0320 - acc: 0.5326 - val_loss: 2.8440 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.30798\n",
      "Epoch 47/300\n",
      " - 0s - loss: 1.0329 - acc: 0.5364 - val_loss: 2.5546 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.30798\n",
      "Epoch 48/300\n",
      " - 0s - loss: 1.0266 - acc: 0.5441 - val_loss: 2.1393 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.30798\n",
      "Epoch 49/300\n",
      " - 0s - loss: 1.0198 - acc: 0.5670 - val_loss: 1.8841 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.30798\n",
      "Epoch 50/300\n",
      " - 0s - loss: 1.0128 - acc: 0.5517 - val_loss: 1.8077 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.30798\n",
      "Epoch 51/300\n",
      " - 0s - loss: 1.0045 - acc: 0.5632 - val_loss: 1.8654 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.30798\n",
      "Epoch 52/300\n",
      " - 0s - loss: 1.0103 - acc: 0.5594 - val_loss: 1.9643 - val_acc: 0.2167\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.30798\n",
      "Epoch 53/300\n",
      " - 0s - loss: 1.0095 - acc: 0.5556 - val_loss: 2.0094 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.30798\n",
      "Epoch 54/300\n",
      " - 0s - loss: 1.0213 - acc: 0.5326 - val_loss: 2.0691 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.30798\n",
      "Epoch 55/300\n",
      " - 0s - loss: 1.0257 - acc: 0.5402 - val_loss: 2.0221 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.30798\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 1.0442 - acc: 0.5364 - val_loss: 1.9663 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.30798\n",
      "Epoch 57/300\n",
      " - 0s - loss: 1.0392 - acc: 0.5057 - val_loss: 2.0061 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.30798\n",
      "Epoch 58/300\n",
      " - 0s - loss: 1.0461 - acc: 0.5326 - val_loss: 2.1176 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.30798\n",
      "Epoch 59/300\n",
      " - 0s - loss: 1.0268 - acc: 0.5517 - val_loss: 2.5595 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.30798\n",
      "Epoch 60/300\n",
      " - 0s - loss: 1.0128 - acc: 0.5977 - val_loss: 2.7086 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.30798\n",
      "Epoch 61/300\n",
      " - 0s - loss: 1.0606 - acc: 0.5211 - val_loss: 2.5420 - val_acc: 0.2243\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.30798\n",
      "Epoch 62/300\n",
      " - 0s - loss: 1.0848 - acc: 0.5019 - val_loss: 2.7342 - val_acc: 0.2205\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.30798\n",
      "Epoch 63/300\n",
      " - 0s - loss: 1.0479 - acc: 0.5211 - val_loss: 3.5803 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.30798\n",
      "Epoch 64/300\n",
      " - 0s - loss: 1.0036 - acc: 0.5824 - val_loss: 3.8580 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.30798\n",
      "Epoch 65/300\n",
      " - 0s - loss: 1.0177 - acc: 0.5287 - val_loss: 3.7848 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.30798\n",
      "Epoch 66/300\n",
      " - 0s - loss: 1.0002 - acc: 0.5594 - val_loss: 3.2872 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.30798\n",
      "Epoch 67/300\n",
      " - 0s - loss: 0.9950 - acc: 0.5556 - val_loss: 2.8243 - val_acc: 0.2205\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.30798\n",
      "Epoch 68/300\n",
      " - 0s - loss: 1.0186 - acc: 0.5364 - val_loss: 2.7644 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.30798\n",
      "Epoch 69/300\n",
      " - 0s - loss: 1.0006 - acc: 0.5517 - val_loss: 2.6587 - val_acc: 0.2243\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.30798\n",
      "Epoch 70/300\n",
      " - 0s - loss: 0.9949 - acc: 0.5632 - val_loss: 2.4794 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.30798\n",
      "Epoch 71/300\n",
      " - 0s - loss: 0.9770 - acc: 0.5824 - val_loss: 2.3981 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.30798\n",
      "Epoch 72/300\n",
      " - 0s - loss: 0.9988 - acc: 0.5594 - val_loss: 2.3542 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.30798\n",
      "Epoch 73/300\n",
      " - 0s - loss: 1.0236 - acc: 0.5441 - val_loss: 2.3128 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.30798\n",
      "Epoch 74/300\n",
      " - 0s - loss: 1.0391 - acc: 0.5249 - val_loss: 2.2682 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.30798\n",
      "Epoch 75/300\n",
      " - 0s - loss: 1.0095 - acc: 0.5517 - val_loss: 2.3216 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.30798\n",
      "Epoch 76/300\n",
      " - 0s - loss: 0.9813 - acc: 0.5862 - val_loss: 2.2537 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.30798\n",
      "Epoch 77/300\n",
      " - 0s - loss: 0.9831 - acc: 0.5670 - val_loss: 2.1919 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.30798\n",
      "Epoch 78/300\n",
      " - 0s - loss: 0.9805 - acc: 0.5632 - val_loss: 2.2342 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.30798\n",
      "Epoch 79/300\n",
      " - 0s - loss: 0.9874 - acc: 0.5670 - val_loss: 2.2584 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.30798\n",
      "Epoch 80/300\n",
      " - 0s - loss: 1.0206 - acc: 0.5402 - val_loss: 2.2792 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.30798\n",
      "Epoch 81/300\n",
      " - 0s - loss: 1.0080 - acc: 0.5326 - val_loss: 2.3571 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.30798\n",
      "Epoch 82/300\n",
      " - 0s - loss: 0.9825 - acc: 0.5785 - val_loss: 2.4200 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.30798\n",
      "Epoch 83/300\n",
      " - 0s - loss: 0.9820 - acc: 0.5939 - val_loss: 2.5885 - val_acc: 0.2243\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.30798\n",
      "Epoch 84/300\n",
      " - 0s - loss: 1.0060 - acc: 0.5594 - val_loss: 2.6105 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.30798\n",
      "Epoch 85/300\n",
      " - 0s - loss: 1.0370 - acc: 0.5249 - val_loss: 2.4063 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.30798\n",
      "Epoch 86/300\n",
      " - 0s - loss: 0.9895 - acc: 0.5709 - val_loss: 2.0828 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.30798\n",
      "Epoch 87/300\n",
      " - 0s - loss: 0.9899 - acc: 0.5556 - val_loss: 2.0131 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.30798\n",
      "Epoch 88/300\n",
      " - 0s - loss: 0.9910 - acc: 0.5670 - val_loss: 2.1773 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.30798\n",
      "Epoch 89/300\n",
      " - 0s - loss: 0.9845 - acc: 0.5709 - val_loss: 2.7345 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.30798\n",
      "Epoch 90/300\n",
      " - 0s - loss: 0.9975 - acc: 0.5402 - val_loss: 3.1764 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.30798\n",
      "Epoch 91/300\n",
      " - 0s - loss: 1.0114 - acc: 0.5172 - val_loss: 3.1898 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.30798\n",
      "Epoch 92/300\n",
      " - 0s - loss: 1.0032 - acc: 0.5747 - val_loss: 2.9934 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.30798\n",
      "Epoch 93/300\n",
      " - 0s - loss: 1.0096 - acc: 0.5747 - val_loss: 2.6533 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.30798\n",
      "Epoch 94/300\n",
      " - 0s - loss: 0.9803 - acc: 0.5939 - val_loss: 2.3142 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.30798\n",
      "Epoch 95/300\n",
      " - 0s - loss: 0.9509 - acc: 0.5900 - val_loss: 1.9613 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.30798\n",
      "Epoch 96/300\n",
      " - 0s - loss: 0.9556 - acc: 0.5900 - val_loss: 1.8158 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.30798\n",
      "Epoch 97/300\n",
      " - 0s - loss: 0.9760 - acc: 0.6015 - val_loss: 2.0441 - val_acc: 0.3080\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.30798\n",
      "Epoch 98/300\n",
      " - 0s - loss: 0.9780 - acc: 0.5977 - val_loss: 2.2660 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.30798\n",
      "Epoch 99/300\n",
      " - 0s - loss: 0.9701 - acc: 0.5632 - val_loss: 2.3418 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.30798\n",
      "Epoch 100/300\n",
      " - 0s - loss: 0.9811 - acc: 0.5556 - val_loss: 2.3659 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.30798\n",
      "Epoch 101/300\n",
      " - 0s - loss: 0.9925 - acc: 0.5517 - val_loss: 2.3703 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.30798\n",
      "Epoch 102/300\n",
      " - 0s - loss: 1.0175 - acc: 0.5517 - val_loss: 2.4392 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.30798\n",
      "Epoch 103/300\n",
      " - 0s - loss: 1.0011 - acc: 0.5632 - val_loss: 2.4711 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.30798\n",
      "Epoch 104/300\n",
      " - 0s - loss: 0.9886 - acc: 0.5632 - val_loss: 2.4629 - val_acc: 0.2966\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.30798\n",
      "Epoch 105/300\n",
      " - 0s - loss: 0.9610 - acc: 0.5900 - val_loss: 2.1832 - val_acc: 0.2966\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.30798\n",
      "Epoch 106/300\n",
      " - 0s - loss: 1.0054 - acc: 0.5862 - val_loss: 1.9003 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.30798\n",
      "Epoch 107/300\n",
      " - 0s - loss: 1.0040 - acc: 0.5785 - val_loss: 1.8778 - val_acc: 0.2966\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.30798\n",
      "Epoch 108/300\n",
      " - 0s - loss: 0.9928 - acc: 0.5785 - val_loss: 2.0940 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.30798\n",
      "Epoch 109/300\n",
      " - 0s - loss: 0.9915 - acc: 0.5900 - val_loss: 2.3020 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.30798\n",
      "Epoch 110/300\n",
      " - 0s - loss: 0.9800 - acc: 0.6015 - val_loss: 2.2923 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.30798\n",
      "Epoch 111/300\n",
      " - 0s - loss: 0.9675 - acc: 0.6092 - val_loss: 2.2717 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.30798\n",
      "Epoch 112/300\n",
      " - 0s - loss: 0.9726 - acc: 0.5785 - val_loss: 2.2138 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.30798\n",
      "Epoch 113/300\n",
      " - 0s - loss: 0.9770 - acc: 0.5824 - val_loss: 2.1184 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.30798\n",
      "Epoch 114/300\n",
      " - 0s - loss: 0.9595 - acc: 0.6015 - val_loss: 1.9814 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.30798\n",
      "Epoch 115/300\n",
      " - 0s - loss: 0.9631 - acc: 0.6130 - val_loss: 1.8903 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.30798\n",
      "Epoch 116/300\n",
      " - 0s - loss: 1.0085 - acc: 0.5824 - val_loss: 1.9351 - val_acc: 0.2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00116: val_acc did not improve from 0.30798\n",
      "Epoch 117/300\n",
      " - 0s - loss: 0.9914 - acc: 0.5632 - val_loss: 1.9819 - val_acc: 0.2091\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.30798\n",
      "Epoch 118/300\n",
      " - 0s - loss: 1.0084 - acc: 0.5709 - val_loss: 2.2021 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.30798\n",
      "Epoch 119/300\n",
      " - 0s - loss: 1.0089 - acc: 0.5594 - val_loss: 2.5424 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.30798\n",
      "Epoch 120/300\n",
      " - 0s - loss: 0.9849 - acc: 0.5670 - val_loss: 2.9924 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.30798\n",
      "Epoch 121/300\n",
      " - 0s - loss: 0.9830 - acc: 0.5670 - val_loss: 3.2662 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.30798\n",
      "Epoch 122/300\n",
      " - 0s - loss: 0.9783 - acc: 0.6015 - val_loss: 2.9589 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.30798\n",
      "Epoch 123/300\n",
      " - 0s - loss: 1.0205 - acc: 0.5632 - val_loss: 2.8703 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.30798\n",
      "Epoch 124/300\n",
      " - 0s - loss: 1.0290 - acc: 0.5632 - val_loss: 2.9890 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.30798\n",
      "Epoch 125/300\n",
      " - 0s - loss: 1.0930 - acc: 0.5211 - val_loss: 3.1229 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.30798\n",
      "Epoch 126/300\n",
      " - 0s - loss: 1.0615 - acc: 0.5364 - val_loss: 3.5265 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.30798\n",
      "Epoch 127/300\n",
      " - 0s - loss: 1.0885 - acc: 0.4981 - val_loss: 3.6593 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.30798\n",
      "Epoch 128/300\n",
      " - 0s - loss: 1.0872 - acc: 0.5134 - val_loss: 3.8281 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.30798\n",
      "Epoch 129/300\n",
      " - 0s - loss: 1.0507 - acc: 0.5211 - val_loss: 4.0939 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.30798\n",
      "Epoch 130/300\n",
      " - 0s - loss: 1.0329 - acc: 0.5479 - val_loss: 4.1288 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.30798\n",
      "Epoch 131/300\n",
      " - 0s - loss: 1.0856 - acc: 0.5019 - val_loss: 4.1559 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.30798\n",
      "Epoch 132/300\n",
      " - 0s - loss: 1.0739 - acc: 0.4751 - val_loss: 4.2143 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.30798\n",
      "Epoch 133/300\n",
      " - 0s - loss: 1.0614 - acc: 0.5019 - val_loss: 4.2399 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.30798\n",
      "Epoch 134/300\n",
      " - 0s - loss: 1.0536 - acc: 0.5057 - val_loss: 4.5037 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.30798\n",
      "Epoch 135/300\n",
      " - 0s - loss: 1.0577 - acc: 0.5172 - val_loss: 4.7463 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.30798\n",
      "Epoch 136/300\n",
      " - 0s - loss: 1.0615 - acc: 0.5287 - val_loss: 4.5907 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.30798\n",
      "Epoch 137/300\n",
      " - 0s - loss: 1.0792 - acc: 0.5211 - val_loss: 4.3643 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.30798\n",
      "Epoch 138/300\n",
      " - 0s - loss: 1.0687 - acc: 0.5287 - val_loss: 3.9060 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.30798\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 0.0007937005636828516.\n",
      "Epoch 139/300\n",
      " - 0s - loss: 1.0282 - acc: 0.5670 - val_loss: 3.2265 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.30798\n",
      "Epoch 140/300\n",
      " - 0s - loss: 0.9947 - acc: 0.5594 - val_loss: 2.4714 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.30798\n",
      "Epoch 141/300\n",
      " - 0s - loss: 0.9924 - acc: 0.5632 - val_loss: 2.0808 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.30798\n",
      "Epoch 142/300\n",
      " - 0s - loss: 0.9906 - acc: 0.5977 - val_loss: 1.9433 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.30798\n",
      "Epoch 143/300\n",
      " - 0s - loss: 0.9788 - acc: 0.5939 - val_loss: 1.8625 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.30798\n",
      "Epoch 144/300\n",
      " - 0s - loss: 0.9980 - acc: 0.5824 - val_loss: 1.8744 - val_acc: 0.3270\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.30798 to 0.32700, saving model to ./weights/EthanolConcentration__weights.h5\n",
      "Epoch 145/300\n",
      " - 0s - loss: 0.9970 - acc: 0.5747 - val_loss: 2.0705 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.32700\n",
      "Epoch 146/300\n",
      " - 0s - loss: 0.9955 - acc: 0.6169 - val_loss: 2.3106 - val_acc: 0.3194\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.32700\n",
      "Epoch 147/300\n",
      " - 0s - loss: 0.9762 - acc: 0.6360 - val_loss: 2.3255 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.32700\n",
      "Epoch 148/300\n",
      " - 0s - loss: 0.9782 - acc: 0.5709 - val_loss: 2.1225 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.32700\n",
      "Epoch 149/300\n",
      " - 0s - loss: 0.9797 - acc: 0.5785 - val_loss: 1.9267 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.32700\n",
      "Epoch 150/300\n",
      " - 0s - loss: 0.9713 - acc: 0.5862 - val_loss: 1.7992 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.32700\n",
      "Epoch 151/300\n",
      " - 0s - loss: 0.9678 - acc: 0.5785 - val_loss: 1.8314 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.32700\n",
      "Epoch 152/300\n",
      " - 0s - loss: 0.9612 - acc: 0.5977 - val_loss: 1.8917 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.32700\n",
      "Epoch 153/300\n",
      " - 0s - loss: 0.9884 - acc: 0.5709 - val_loss: 1.9582 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.32700\n",
      "Epoch 154/300\n",
      " - 0s - loss: 1.0235 - acc: 0.5441 - val_loss: 1.9765 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.32700\n",
      "Epoch 155/300\n",
      " - 0s - loss: 1.0173 - acc: 0.5287 - val_loss: 1.9275 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.32700\n",
      "Epoch 156/300\n",
      " - 0s - loss: 0.9721 - acc: 0.5824 - val_loss: 1.8901 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.32700\n",
      "Epoch 157/300\n",
      " - 0s - loss: 0.9366 - acc: 0.6207 - val_loss: 1.9831 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.32700\n",
      "Epoch 158/300\n",
      " - 0s - loss: 0.9734 - acc: 0.6015 - val_loss: 2.1202 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.32700\n",
      "Epoch 159/300\n",
      " - 0s - loss: 0.9670 - acc: 0.5824 - val_loss: 2.2123 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.32700\n",
      "Epoch 160/300\n",
      " - 0s - loss: 0.9843 - acc: 0.5939 - val_loss: 2.3598 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.32700\n",
      "Epoch 161/300\n",
      " - 0s - loss: 1.0010 - acc: 0.5632 - val_loss: 2.5169 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.32700\n",
      "Epoch 162/300\n",
      " - 0s - loss: 1.0074 - acc: 0.5670 - val_loss: 2.3304 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.32700\n",
      "Epoch 163/300\n",
      " - 0s - loss: 0.9864 - acc: 0.5862 - val_loss: 2.0966 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.32700\n",
      "Epoch 164/300\n",
      " - 0s - loss: 0.9886 - acc: 0.5862 - val_loss: 1.9873 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.32700\n",
      "Epoch 165/300\n",
      " - 0s - loss: 0.9635 - acc: 0.5900 - val_loss: 1.9920 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.32700\n",
      "Epoch 166/300\n",
      " - 0s - loss: 0.9613 - acc: 0.5785 - val_loss: 1.9884 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.32700\n",
      "Epoch 167/300\n",
      " - 0s - loss: 0.9543 - acc: 0.5977 - val_loss: 2.0266 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.32700\n",
      "Epoch 168/300\n",
      " - 0s - loss: 0.9597 - acc: 0.5670 - val_loss: 2.0811 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.32700\n",
      "Epoch 169/300\n",
      " - 0s - loss: 0.9729 - acc: 0.5862 - val_loss: 2.1320 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.32700\n",
      "Epoch 170/300\n",
      " - 0s - loss: 0.9671 - acc: 0.5824 - val_loss: 2.1668 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.32700\n",
      "Epoch 171/300\n",
      " - 0s - loss: 0.9534 - acc: 0.5785 - val_loss: 2.2256 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.32700\n",
      "Epoch 172/300\n",
      " - 0s - loss: 0.9715 - acc: 0.5479 - val_loss: 2.0015 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.32700\n",
      "Epoch 173/300\n",
      " - 0s - loss: 0.9419 - acc: 0.5900 - val_loss: 2.0746 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.32700\n",
      "Epoch 174/300\n",
      " - 0s - loss: 0.9774 - acc: 0.5747 - val_loss: 2.7627 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.32700\n",
      "Epoch 175/300\n",
      " - 0s - loss: 0.9841 - acc: 0.5709 - val_loss: 2.8431 - val_acc: 0.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00175: val_acc did not improve from 0.32700\n",
      "Epoch 176/300\n",
      " - 0s - loss: 0.9740 - acc: 0.5977 - val_loss: 2.6183 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.32700\n",
      "Epoch 177/300\n",
      " - 0s - loss: 0.9680 - acc: 0.5747 - val_loss: 3.0973 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.32700\n",
      "Epoch 178/300\n",
      " - 0s - loss: 0.9852 - acc: 0.5632 - val_loss: 3.1154 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.32700\n",
      "Epoch 179/300\n",
      " - 0s - loss: 1.0222 - acc: 0.5939 - val_loss: 2.9420 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.32700\n",
      "Epoch 180/300\n",
      " - 0s - loss: 1.0031 - acc: 0.5670 - val_loss: 2.7535 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.32700\n",
      "Epoch 181/300\n",
      " - 0s - loss: 0.9943 - acc: 0.5364 - val_loss: 2.6207 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.32700\n",
      "Epoch 182/300\n",
      " - 0s - loss: 0.9739 - acc: 0.5747 - val_loss: 2.4370 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.32700\n",
      "Epoch 183/300\n",
      " - 0s - loss: 0.9356 - acc: 0.5900 - val_loss: 2.2086 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.32700\n",
      "Epoch 184/300\n",
      " - 0s - loss: 0.9349 - acc: 0.5900 - val_loss: 2.0937 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.32700\n",
      "Epoch 185/300\n",
      " - 0s - loss: 0.9356 - acc: 0.6360 - val_loss: 2.0266 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.32700\n",
      "Epoch 186/300\n",
      " - 0s - loss: 0.9411 - acc: 0.5900 - val_loss: 1.9397 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.32700\n",
      "Epoch 187/300\n",
      " - 0s - loss: 0.9630 - acc: 0.5479 - val_loss: 1.8994 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.32700\n",
      "Epoch 188/300\n",
      " - 0s - loss: 0.9612 - acc: 0.5517 - val_loss: 2.1131 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.32700\n",
      "Epoch 189/300\n",
      " - 0s - loss: 0.9826 - acc: 0.5632 - val_loss: 2.2204 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.32700\n",
      "Epoch 190/300\n",
      " - 0s - loss: 0.9804 - acc: 0.5594 - val_loss: 2.1298 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.32700\n",
      "Epoch 191/300\n",
      " - 0s - loss: 0.9714 - acc: 0.5900 - val_loss: 1.9776 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.32700\n",
      "Epoch 192/300\n",
      " - 0s - loss: 0.9613 - acc: 0.5939 - val_loss: 1.7805 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.32700\n",
      "Epoch 193/300\n",
      " - 0s - loss: 0.9468 - acc: 0.6015 - val_loss: 1.7338 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.32700\n",
      "Epoch 194/300\n",
      " - 0s - loss: 0.9584 - acc: 0.6169 - val_loss: 1.8046 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.32700\n",
      "Epoch 195/300\n",
      " - 0s - loss: 0.9670 - acc: 0.5939 - val_loss: 1.7567 - val_acc: 0.2814\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.32700\n",
      "Epoch 196/300\n",
      " - 0s - loss: 0.9658 - acc: 0.6130 - val_loss: 1.7443 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.32700\n",
      "Epoch 197/300\n",
      " - 0s - loss: 0.9568 - acc: 0.5977 - val_loss: 1.6353 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.32700\n",
      "Epoch 198/300\n",
      " - 0s - loss: 0.9257 - acc: 0.5939 - val_loss: 2.5089 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.32700\n",
      "Epoch 199/300\n",
      " - 0s - loss: 0.9142 - acc: 0.6207 - val_loss: 3.8763 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.32700\n",
      "Epoch 200/300\n",
      " - 0s - loss: 0.9517 - acc: 0.6130 - val_loss: 4.6085 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.32700\n",
      "Epoch 201/300\n",
      " - 0s - loss: 0.9799 - acc: 0.5709 - val_loss: 4.7590 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.32700\n",
      "Epoch 202/300\n",
      " - 0s - loss: 0.9828 - acc: 0.5326 - val_loss: 4.3616 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.32700\n",
      "Epoch 203/300\n",
      " - 0s - loss: 0.9854 - acc: 0.5479 - val_loss: 3.5645 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.32700\n",
      "Epoch 204/300\n",
      " - 0s - loss: 0.9628 - acc: 0.5670 - val_loss: 2.7695 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.32700\n",
      "Epoch 205/300\n",
      " - 0s - loss: 0.9453 - acc: 0.6169 - val_loss: 2.2312 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.32700\n",
      "Epoch 206/300\n",
      " - 0s - loss: 0.9405 - acc: 0.6207 - val_loss: 2.1741 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.32700\n",
      "Epoch 207/300\n",
      " - 0s - loss: 0.9413 - acc: 0.6207 - val_loss: 2.3336 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.32700\n",
      "Epoch 208/300\n",
      " - 0s - loss: 0.9239 - acc: 0.5824 - val_loss: 2.3716 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.32700\n",
      "Epoch 209/300\n",
      " - 0s - loss: 0.9059 - acc: 0.6054 - val_loss: 2.3648 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.32700\n",
      "Epoch 210/300\n",
      " - 0s - loss: 0.9157 - acc: 0.6207 - val_loss: 2.4292 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.32700\n",
      "Epoch 211/300\n",
      " - 0s - loss: 0.9299 - acc: 0.6092 - val_loss: 2.4518 - val_acc: 0.2129\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.32700\n",
      "Epoch 212/300\n",
      " - 0s - loss: 0.9315 - acc: 0.5862 - val_loss: 2.4387 - val_acc: 0.2243\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.32700\n",
      "Epoch 213/300\n",
      " - 0s - loss: 0.9228 - acc: 0.6245 - val_loss: 2.3422 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.32700\n",
      "Epoch 214/300\n",
      " - 0s - loss: 0.9105 - acc: 0.6437 - val_loss: 2.2943 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.32700\n",
      "Epoch 215/300\n",
      " - 0s - loss: 0.8990 - acc: 0.6437 - val_loss: 2.3623 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.32700\n",
      "Epoch 216/300\n",
      " - 0s - loss: 0.9060 - acc: 0.6398 - val_loss: 2.5670 - val_acc: 0.2281\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.32700\n",
      "Epoch 217/300\n",
      " - 0s - loss: 0.9092 - acc: 0.6207 - val_loss: 2.9206 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.32700\n",
      "Epoch 218/300\n",
      " - 0s - loss: 0.9033 - acc: 0.6284 - val_loss: 3.2201 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.32700\n",
      "Epoch 219/300\n",
      " - 0s - loss: 0.9103 - acc: 0.6207 - val_loss: 3.2027 - val_acc: 0.2471\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.32700\n",
      "Epoch 220/300\n",
      " - 0s - loss: 0.8957 - acc: 0.6054 - val_loss: 2.7474 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.32700\n",
      "Epoch 221/300\n",
      " - 0s - loss: 0.9069 - acc: 0.6322 - val_loss: 2.2020 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.32700\n",
      "Epoch 222/300\n",
      " - 0s - loss: 0.9193 - acc: 0.5939 - val_loss: 1.8828 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.32700\n",
      "Epoch 223/300\n",
      " - 0s - loss: 0.9210 - acc: 0.5900 - val_loss: 1.8186 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.32700\n",
      "Epoch 224/300\n",
      " - 0s - loss: 0.9528 - acc: 0.5939 - val_loss: 1.8013 - val_acc: 0.3004\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.32700\n",
      "Epoch 225/300\n",
      " - 0s - loss: 0.9371 - acc: 0.6015 - val_loss: 1.9799 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.32700\n",
      "Epoch 226/300\n",
      " - 0s - loss: 0.9295 - acc: 0.6015 - val_loss: 2.1982 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.32700\n",
      "Epoch 227/300\n",
      " - 0s - loss: 0.8981 - acc: 0.6245 - val_loss: 2.2468 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.32700\n",
      "Epoch 228/300\n",
      " - 0s - loss: 0.9062 - acc: 0.6245 - val_loss: 2.2037 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.32700\n",
      "Epoch 229/300\n",
      " - 0s - loss: 0.9016 - acc: 0.5977 - val_loss: 2.1994 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.32700\n",
      "Epoch 230/300\n",
      " - 0s - loss: 0.9022 - acc: 0.6092 - val_loss: 2.2046 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.32700\n",
      "Epoch 231/300\n",
      " - 0s - loss: 0.9156 - acc: 0.6054 - val_loss: 2.0548 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.32700\n",
      "Epoch 232/300\n",
      " - 0s - loss: 0.9030 - acc: 0.6322 - val_loss: 1.8391 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.32700\n",
      "Epoch 233/300\n",
      " - 0s - loss: 0.8918 - acc: 0.6552 - val_loss: 1.7266 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.32700\n",
      "Epoch 234/300\n",
      " - 0s - loss: 0.9077 - acc: 0.6284 - val_loss: 1.7575 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.32700\n",
      "Epoch 235/300\n",
      " - 0s - loss: 0.9317 - acc: 0.6092 - val_loss: 1.8159 - val_acc: 0.2814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00235: val_acc did not improve from 0.32700\n",
      "Epoch 236/300\n",
      " - 0s - loss: 0.9210 - acc: 0.6207 - val_loss: 1.8697 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.32700\n",
      "Epoch 237/300\n",
      " - 0s - loss: 0.9100 - acc: 0.6322 - val_loss: 1.8701 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.32700\n",
      "Epoch 238/300\n",
      " - 0s - loss: 0.8979 - acc: 0.6552 - val_loss: 1.9987 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.32700\n",
      "Epoch 239/300\n",
      " - 0s - loss: 0.9109 - acc: 0.6284 - val_loss: 2.2041 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.32700\n",
      "Epoch 240/300\n",
      " - 0s - loss: 0.9351 - acc: 0.5862 - val_loss: 2.4800 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.32700\n",
      "Epoch 241/300\n",
      " - 0s - loss: 0.9560 - acc: 0.5939 - val_loss: 2.3193 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.32700\n",
      "Epoch 242/300\n",
      " - 0s - loss: 0.9697 - acc: 0.5670 - val_loss: 2.0476 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.32700\n",
      "Epoch 243/300\n",
      " - 0s - loss: 0.9654 - acc: 0.5709 - val_loss: 1.9984 - val_acc: 0.2890\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.32700\n",
      "Epoch 244/300\n",
      " - 0s - loss: 0.9444 - acc: 0.5900 - val_loss: 2.0568 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.32700\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 0.0006299605615522398.\n",
      "Epoch 245/300\n",
      " - 0s - loss: 0.9745 - acc: 0.5862 - val_loss: 2.0997 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.32700\n",
      "Epoch 246/300\n",
      " - 0s - loss: 0.9568 - acc: 0.6015 - val_loss: 2.2005 - val_acc: 0.2662\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.32700\n",
      "Epoch 247/300\n",
      " - 0s - loss: 0.9192 - acc: 0.6245 - val_loss: 2.2548 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.32700\n",
      "Epoch 248/300\n",
      " - 0s - loss: 0.9000 - acc: 0.6207 - val_loss: 2.2866 - val_acc: 0.2586\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.32700\n",
      "Epoch 249/300\n",
      " - 0s - loss: 0.8881 - acc: 0.6092 - val_loss: 2.1933 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.32700\n",
      "Epoch 250/300\n",
      " - 0s - loss: 0.8774 - acc: 0.6360 - val_loss: 2.1004 - val_acc: 0.2510\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.32700\n",
      "Epoch 251/300\n",
      " - 0s - loss: 0.8907 - acc: 0.6130 - val_loss: 2.0115 - val_acc: 0.2243\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.32700\n",
      "Epoch 252/300\n",
      " - 0s - loss: 0.8997 - acc: 0.5939 - val_loss: 1.9160 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.32700\n",
      "Epoch 253/300\n",
      " - 0s - loss: 0.8819 - acc: 0.6513 - val_loss: 1.8593 - val_acc: 0.2319\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.32700\n",
      "Epoch 254/300\n",
      " - 0s - loss: 0.8554 - acc: 0.6590 - val_loss: 1.8598 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.32700\n",
      "Epoch 255/300\n",
      " - 0s - loss: 0.8630 - acc: 0.6284 - val_loss: 1.8273 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.32700\n",
      "Epoch 256/300\n",
      " - 0s - loss: 0.8507 - acc: 0.6590 - val_loss: 1.7953 - val_acc: 0.2548\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.32700\n",
      "Epoch 257/300\n",
      " - 0s - loss: 0.8833 - acc: 0.6437 - val_loss: 1.8456 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.32700\n",
      "Epoch 258/300\n",
      " - 0s - loss: 0.8648 - acc: 0.6513 - val_loss: 1.9011 - val_acc: 0.3042\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.32700\n",
      "Epoch 259/300\n",
      " - 0s - loss: 0.8885 - acc: 0.6322 - val_loss: 1.9712 - val_acc: 0.2700\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.32700\n",
      "Epoch 260/300\n",
      " - 0s - loss: 0.9213 - acc: 0.6360 - val_loss: 1.9793 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.32700\n",
      "Epoch 261/300\n",
      " - 0s - loss: 0.8687 - acc: 0.6513 - val_loss: 1.9289 - val_acc: 0.2624\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.32700\n",
      "Epoch 262/300\n",
      " - 0s - loss: 0.8653 - acc: 0.6667 - val_loss: 1.9578 - val_acc: 0.2776\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.32700\n",
      "Epoch 263/300\n",
      " - 0s - loss: 0.8489 - acc: 0.6513 - val_loss: 2.0276 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.32700\n",
      "Epoch 264/300\n",
      " - 0s - loss: 0.8682 - acc: 0.6513 - val_loss: 2.1245 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.32700\n",
      "Epoch 265/300\n",
      " - 0s - loss: 0.8890 - acc: 0.6322 - val_loss: 2.2574 - val_acc: 0.2395\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.32700\n",
      "Epoch 266/300\n",
      " - 0s - loss: 0.8895 - acc: 0.6322 - val_loss: 2.2690 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.32700\n",
      "Epoch 267/300\n",
      " - 0s - loss: 0.8821 - acc: 0.6705 - val_loss: 2.2747 - val_acc: 0.2357\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.32700\n",
      "Epoch 268/300\n",
      " - 0s - loss: 0.8753 - acc: 0.6475 - val_loss: 2.4070 - val_acc: 0.2205\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.32700\n",
      "Epoch 269/300\n",
      " - 0s - loss: 0.8858 - acc: 0.6437 - val_loss: 2.6537 - val_acc: 0.2433\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.32700\n",
      "Epoch 270/300\n",
      " - 0s - loss: 0.9055 - acc: 0.6054 - val_loss: 2.6562 - val_acc: 0.2738\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.32700\n",
      "Epoch 271/300\n",
      " - 0s - loss: 0.9053 - acc: 0.6322 - val_loss: 2.4030 - val_acc: 0.2852\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_model(model, DATASET_INDEX, dataset_prefix=ds+'_', epochs=300, batch_size=128)\n",
    "print(\"Training time of %d epochs is %d \" %(300, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLSTM-FCN] Results on dataset WITH Speed (8 classes): \n",
      " Class_sample_threshold=1.7%\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  481 Number of test samples :  527\n",
      "Number of classes :  8\n",
      "Sequence length :  10\n",
      "X_train.shape is  (481, 8, 10)\n",
      "X_test.shape is  (527, 8, 10)\n",
      "\n",
      "Evaluating : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.54      0.48        52\n",
      "           1       0.41      0.65      0.50        20\n",
      "           2       0.90      0.88      0.89       129\n",
      "           3       0.75      0.75      0.75         8\n",
      "           4       0.80      0.80      0.80       132\n",
      "           5       0.80      0.60      0.69       151\n",
      "           6       0.38      0.48      0.42        25\n",
      "           7       0.22      0.40      0.29        10\n",
      "\n",
      "    accuracy                           0.71       527\n",
      "   macro avg       0.59      0.64      0.60       527\n",
      "weighted avg       0.74      0.71      0.72       527\n",
      "\n",
      "Final Accuracy %.4d :  0.7077798869397202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7077798869397202, 1.5153405508008808)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[MLSTM-FCN] Results on dataset WITH Speed (8 classes): \\n Class_sample_threshold=1.7%\")\n",
    "evaluate_model(model, DATASET_INDEX, dataset_prefix=ds+'_', batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
