{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "os.system(\"du -h ./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UEA-MTS archive datasets:\n",
    "\"ArticularyWordRecognition\",\n",
    "\"AtrialFibrillation\",\n",
    "\"BasicMotions\",\n",
    "\"CharacterTrajectories\",\n",
    "\"Cricket\",\n",
    "\"DuckDuckGeese\",#5\n",
    "\"EigenWorms\",\n",
    "\"Epilepsy\",\n",
    "\"ERing\",\n",
    "\"EthanolConcentration\",\n",
    "\"FaceDetection\",#10\n",
    "\"FingerMovements\",\n",
    "\"HandMovementDirection\",\n",
    "\"Handwriting\",\n",
    "\"Heartbeat\",\n",
    "\"InsectWingbeat\",#15\n",
    "\"JapaneseVowels\",\n",
    "\"Libras\",\n",
    "\"LSST\",\n",
    "\"MotorImagery\",\n",
    "\"NATOPS\",#20\n",
    "\"PEMS-SF\",\n",
    "\"PenDigits\",\n",
    "\"PhonemeSpectra\",\n",
    "\"RacketSports\",\n",
    "\"SelfRegulationSCP1\",#25\n",
    "\"SelfRegulationSCP2\",\n",
    "\"SpokenArabicDigits\",\n",
    "\"StandWalkJump\",        \n",
    "\"UWaveGestureLibrary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = './'\n",
    "UEA_MTS_List = [\n",
    "        \"PEMS-SF\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert UEA '.arff' format to SMATE format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert UEA '.arff' format to meta file and data file containing one single instance\n",
    "def conv_UEA_SMATE(dataset_names):\n",
    "    for name in dataset_names:\n",
    "        dict = rep + name + '/'\n",
    "        file_train = dict + name + '_TRAIN.arff'\n",
    "        file_test = dict + name + '_TEST.arff'\n",
    "        dict_train_out = dict + 'output_train/'\n",
    "        dict_test_out = dict + 'output_test/'\n",
    "        os.system(\"mkdir \" + dict_train_out)\n",
    "        os.system(\"mkdir \" + dict_test_out)\n",
    "        convert_arff_samples(file_train, dict_train_out)\n",
    "        convert_arff_samples(file_test, dict_test_out)\n",
    "\n",
    "def convert_arff_samples(file, dict_out):\n",
    "    data = arff.loadarff(file) #load 'arff' files\n",
    "    df = pd.DataFrame(data[0])\n",
    "    #df['input'] = df.iloc[:,0] \n",
    "    df.insert(loc=0, column='d_input', value=df.iloc[:,0]) #standardize the input attribute name\n",
    "    df = df.drop(df.columns[1], axis=1)\n",
    "    df['d_class'] = df.iloc[:,-1] #standardize the class attribute name\n",
    "    \n",
    "    df = df.drop(df.columns[-2], axis=1)\n",
    "    df['sample_id'] = df.index # add index as sample_id \n",
    "    df['d_class'] = df['d_class'].apply(lambda x: x.decode(\"utf-8\") ) # convert Byte to String for class column\n",
    "    df_meta = df[['sample_id', 'd_class']]\n",
    "    df_meta.to_csv(dict_out + 'meta_data.csv', index=False, header=None) #save meta_data.csv\n",
    "    \n",
    "    for name, sample in df.groupby(['sample_id']):\n",
    "        df_sample = pd.DataFrame(sample['d_input'].values.tolist()).add_prefix('dimension_')\n",
    "        df_sample_conv = pd.DataFrame()\n",
    "        for c in df_sample.columns:\n",
    "            df_sample_conv[[c]] = pd.DataFrame(df_sample[c].values[0].tolist())\n",
    "        df_sample_conv.to_csv(dict_out + str(name) + '.csv', index=False, header=None) #save samples into individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_UEA_SMATE(UEA_MTS_List) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert UEA '.arff' format to NMSU'IJCAI'20 format (i.e., CA-SFCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert UEA '.arff' format to NMSU_IJCAI'20 input format \n",
    "'''\n",
    "output: N * (D * L)\n",
    "\n",
    "'''\n",
    "\n",
    "def conv_UEA_NMSU(dataset_names):\n",
    "    for name in dataset_names:\n",
    "        dict =  rep + name + '/'\n",
    "        file_train = dict + name + '_TRAIN.arff'\n",
    "        file_test = dict + name + '_TEST.arff'\n",
    "        dict_out = 'uea_nmsu' + '/' + dict \n",
    "        os.makedirs(dict_out, exist_ok=True)\n",
    "        X_train, y_train, map_c_l = convert_arff_nmsu(file_train, dict_out, 'train' )\n",
    "        X_test, y_test = convert_arff_nmsu(file_test, dict_out, 'test',  map_c_l)\n",
    "        \n",
    "        x_row, attr_num, attr_len = X_train.shape\n",
    "        X_train = X_train.reshape(x_row, (attr_num*attr_len))\n",
    "        x_row, attr_num, attr_len = X_test.shape\n",
    "        X_test = X_test.reshape(x_row, (attr_num*attr_len))\n",
    "        \n",
    "        file_writingxy(X_train, y_train, dict_out + \"train.txt\", attr_num)\n",
    "        file_writingxy(X_test, y_test, dict_out + \"test.txt\", attr_num)\n",
    "        #return X_train\n",
    "        \n",
    "\n",
    "def convert_arff_nmsu(file, dict_out, out_name, map_c_l = {}):\n",
    "    data = arff.loadarff(file) #load 'arff' files\n",
    "    df = pd.DataFrame(data[0])\n",
    "    df.insert(loc=0, column='d_input', value=df.iloc[:,0]) #standardize the input attribute name\n",
    "    df = df.drop(df.columns[1], axis=1)\n",
    "    df['d_class'] = df.iloc[:,-1] #standardize the class attribute name\n",
    "    df = df.drop(df.columns[-2], axis=1)\n",
    "    df['d_class'] = df['d_class'].apply(lambda x: x.decode(\"utf-8\") ) # convert Byte to String for class column\n",
    "    df = df[['d_class', 'd_input']]\n",
    "    \n",
    "    def convert_D_L(x):\n",
    "        x_transpose = [[row[i] for row in x] for i in range(len(x[0]))] \n",
    "        x_arr = np.transpose(np.asarray(x_transpose))\n",
    "        return x_arr\n",
    "    df['d_input'] = df['d_input'].map(convert_D_L)\n",
    "    x = df['d_input']\n",
    "    print(x.shape)\n",
    "    x_matrix = np.zeros([len(x), x[0].shape[0], x[0].shape[1]])\n",
    "    print(x_matrix.shape)\n",
    "    for i in range(len(x)):\n",
    "        x_matrix[i,:,:] = x[i]\n",
    "    \n",
    "    y_vector = np.array(df['d_class'])\n",
    "    if out_name == 'train':\n",
    "        map_c_l = get_label_map(y_vector)\n",
    "        #print(\"map_c_l is \", map_c_l)\n",
    "        y_num = np.zeros(y_vector.shape[0])\n",
    "        for idx, y in enumerate(y_vector):\n",
    "            #print(\"y is \", y)\n",
    "            y_num[idx] = map_c_l[y]\n",
    "        y_num = y_num.reshape(-1, 1)\n",
    "        return np.nan_to_num(x_matrix), np.nan_to_num(y_num), map_c_l\n",
    "    else: \n",
    "        y_num = np.zeros(y_vector.shape[0])\n",
    "        for idx, y in enumerate(y_vector):\n",
    "            y_num[idx] = map_c_l[y]\n",
    "        y_num = y_num.reshape(-1, 1)\n",
    "        return np.nan_to_num(x_matrix), np.nan_to_num(y_num)\n",
    "\n",
    "\n",
    "def file_writingxy(data_x_matrix, data_y_vector, file_name, attr_num=-1, delimiter=' '):\n",
    "    data_row, data_col = data_x_matrix.shape\n",
    "    with open(file_name, 'w') as f:\n",
    "        if attr_num > 0:\n",
    "            f.write(str(int(attr_num)) + '\\n')\n",
    "        for row in range(0, data_row):\n",
    "            row_vector = data_x_matrix[row, :]\n",
    "            row_label = str(int(float(data_y_vector[row])))\n",
    "            row_str = row_label\n",
    "            for index in range(0, data_col):\n",
    "                row_str = row_str + delimiter + str(row_vector[index])\n",
    "            f.write(row_str + '\\n')\n",
    "\n",
    "def get_label_map(y_train):\n",
    "    '''\n",
    "    Input:\n",
    "    - y_train: a vector (n, )\n",
    "    \n",
    "    Output:\n",
    "    - mapping_c_l: dict {label: number}, number is in [0, n_class - 1]\n",
    "    \n",
    "    '''\n",
    "    No = len(y_train)\n",
    "    classes, counts_cl = np.unique(y_train, return_counts=True)\n",
    "    print(\"class list is \" + str(classes))\n",
    "    mapping_c_l = {}  # a mappling between classes and labels\n",
    "    for idx, c in enumerate(list(classes)):\n",
    "        mapping_c_l.update({c: idx})\n",
    "    return mapping_c_l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_UEA_NMSU(UEA_MTS_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, scipy\n",
    "print(sys.version)\n",
    "print (scipy.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python SMAT_GAN",
   "language": "python",
   "name": "python3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
