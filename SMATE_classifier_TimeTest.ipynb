{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.generic_utils import *\n",
    "from utils.basic_modules import *\n",
    "from utils.UEA_utils import *\n",
    "from utils.data_plot import *\n",
    "from SMATE_model import *\n",
    "\n",
    "import time, os, math\n",
    "import pydot, pydotplus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as ll\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from decimal import Decimal\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Conv2D, Conv3D, TimeDistributed\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, add, GaussianNoise, Lambda\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical, plot_model, multi_gpu_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm, neighbors\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "\n",
    "K.clear_session()\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "config.graph_options.rewrite_options.arithmetic_optimization = off\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_ratio = 1 # the supervised ratio in training set, 1 by defaut (fully supervised)\n",
    "n_epochs = 300\n",
    "\n",
    "UEA_MTS_List = [\n",
    "        \"ArticularyWordRecognition\",\n",
    "        \"AtrialFibrillation\",\n",
    "        \"BasicMotions\",\n",
    "        \"CharacterTrajectories\",\n",
    "        \"Cricket\", #5\n",
    "        #\"DuckDuckGeese\",\n",
    "        #\"EigenWorms\",\n",
    "        \"Epilepsy\",\n",
    "        \"EthanolConcentration\",\n",
    "        \"ERing\",#10\n",
    "        \"FaceDetection\",\n",
    "        \"FingerMovements\",\n",
    "        \"HandMovementDirection\",\n",
    "        #\"Handwriting\",\n",
    "        \"Heartbeat\",#15\n",
    "        #\"InsectWingbeat\",\n",
    "        #\"JapaneseVowels\",\n",
    "        #\"Libras\",\n",
    "        \"LSST\",\n",
    "        \"MotorImagery\",#20\n",
    "        \"NATOPS\",\n",
    "        \"PenDigits\",\n",
    "        \"PEMS-SF\",\n",
    "        \"PhonemeSpectra\",\n",
    "        #\"RacketSports\",#25\n",
    "        \"SelfRegulationSCP1\",\n",
    "        \"SelfRegulationSCP2\",\n",
    "        \"SpokenArabicDigits\",\n",
    "        \"StandWalkJump\",        \n",
    "        #\"UWaveGestureLibrary\"\n",
    "]\n",
    "\n",
    "hyper_paras_map = {\n",
    "        \"ArticularyWordRecognition\" : [10, 4], \n",
    "        \"AtrialFibrillation\" : [64, 2],\n",
    "        \"BasicMotions\" : [100, 2],\n",
    "        \"CharacterTrajectories\" : [30, 2],\n",
    "        \"Cricket\" : [100, 4], #5\n",
    "        #\"DuckDuckGeese\",\n",
    "        #\"EigenWorms\",\n",
    "        \"Epilepsy\" : [30, 2],\n",
    "        \"EthanolConcentration\" : [50, 2],  \n",
    "        \"ERing\": [5, 4],#10\n",
    "        \"FaceDetection\" : [8, 16],  \n",
    "        \"FingerMovements\" : [5, 8],  \n",
    "        \"HandMovementDirection\" : [40, 4],  \n",
    "        #\"Handwriting\",\n",
    "        \"Heartbeat\" : [5, 16],  #15\n",
    "        #\"InsectWingbeat\",\n",
    "        #\"JapaneseVowels\",\n",
    "        #\"Libras\",\n",
    "        \"LSST\" : [4, 4],  \n",
    "        \"MotorImagery\" : [100, 8],  #20\n",
    "        \"NATOPS\" : [3, 6],\n",
    "        \"PenDigits\" : [2, 2],\n",
    "        \"PEMS-SF\" : [10, 64],  \n",
    "        \"PhonemeSpectra\" : [30, 4],  \n",
    "        #\"RacketSports\",#25\n",
    "        \"SelfRegulationSCP1\" : [100, 6],  \n",
    "        \"SelfRegulationSCP2\" : [100, 6],  \n",
    "        \"SpokenArabicDigits\" : [10, 4],  \n",
    "        \"StandWalkJump\" : [100, 4],        \n",
    "        #\"UWaveGestureLibrary\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "Input Data format (MTS-others)\n",
    "---\n",
    "\n",
    "A single file contains all samples and their labels: ***L * (3 + D)***\n",
    "\n",
    "\n",
    "\n",
    "- 1st col: sample_id\n",
    "- 2nd col: timestamps\n",
    "- 3rd col: label\n",
    "- after the 4th col: mts vector with D dimensions   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class list is ['1.0' '2.0' '3.0' '4.0' '5.0' '6.0' '7.0']\n",
      "total number of samples is 267\n",
      "total number of samples is 173\n"
     ]
    }
   ],
   "source": [
    " '''=================================================== Prepare data ========================================================'''\n",
    "## Prepare UEA data\n",
    "rep_main = \"../Datasets/MTS-UEA/\"\n",
    "ds_name = \"PEMS-SF\"\n",
    "rep_ds_train = rep_main + ds_name + \"/output_train/\"\n",
    "rep_ds_test = rep_main + ds_name + \"/output_test/\"\n",
    "meta_csv = \"meta_data.csv\"  # the meta data of training/testing set\n",
    "rep_output = rep_ds_train + \"out_results/\"  # output results, e.g., training loss, models\n",
    "#os.system(\"rm \" + rep_output + \"running_time.csv\")\n",
    "\n",
    "dataset = get_UEA_dataset(rep_ds_train, rep_ds_test, meta_csv, sup_ratio, mode='load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267, 144, 963)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_time(dataset, sample_rate, train_rate, dimension_rate):\n",
    "    x_train = dataset['X_train']\n",
    "    x_sup = dataset['X_sup']  # 3-D Array: N * L * D\n",
    "    x_unsup = dataset['X_unsup']\n",
    "    y_train = dataset['Y_train']\n",
    "    y_sup = dataset['Y_sup']\n",
    "    \n",
    "    nbr_sample = int(sample_rate * x_train.shape[1])\n",
    "    nbr_ts_instance = int(train_rate * x_train.shape[0])\n",
    "    nbr_dimension = int(dimension_rate * x_train.shape[2])\n",
    "    # vary sample rate\n",
    "    #x_train = resample_dataset(x_train, nbr_sample)\n",
    "    #x_sup, x_unsup = x_train, x_train\n",
    "    #x_test = resample_dataset(x_test, nbr_sample)\n",
    "    #y_train = y_train[: nbr_ts_instance]\n",
    "\n",
    "    print(\"x_train.shape is \", x_train.shape)\n",
    "    # vary dimension size\n",
    "    x_train = x_train[:, :, :nbr_dimension]\n",
    "    x_sup, x_unsup = x_train, x_train\n",
    "    #x_test = x_test[:, :, :nbr_dimension]\n",
    "    \n",
    "    # Bacis Dataset Information and Model Configurations\n",
    "    train_size = x_train.shape[0] \n",
    "    L = x_train.shape[1]\n",
    "    data_dim = x_train.shape[2]\n",
    "    n_classes = dataset['n_classes']\n",
    "\n",
    "    label_size = x_sup.shape[0]\n",
    "    unlabel_size = x_unsup.shape[0]\n",
    "\n",
    "    # Two hyper-parameters vary with datasets\n",
    "    pool_step = hyper_paras_map[ds_name][0]\n",
    "    d_prime = hyper_paras_map[ds_name][1]\n",
    "    \n",
    "    # Build SMATE model\n",
    "    #K.set_session(sess)\n",
    "    smate = SMATE(L, data_dim, n_classes, label_size, unlabel_size, \n",
    "                  y_sup, sup_ratio, pool_step, d_prime)\n",
    "\n",
    "    smate.build_model()\n",
    "\n",
    "    # Train SMATE model\n",
    "    start = time.time()\n",
    "    smate.fit(n_epochs, x_train, x_sup, x_unsup)\n",
    "    print(\"Training Time for sample_rate (%f2) train_rate (%f2) dimension_size (%f2) is %d\" \n",
    "          %(sample_rate, train_rate, data_dim, time.time() - start))\n",
    "    #K.clear_session()\n",
    "    return time.time() - start\n",
    "    \n",
    "def resample_dataset(x, nbr_sample):\n",
    "    x_sampled = np.zeros(shape=(x.shape[0], nbr_sample, x.shape[2])) # N' * L * D \n",
    "    from scipy import signal\n",
    "    for i in range(x.shape[0]):\n",
    "        f = signal.resample(x[i], nbr_sample, axis = 0)\n",
    "        x_sampled[i] = f\n",
    "    return x_sampled\n",
    "\n",
    "def save_running_time(ds_name, dataset, save_path, sample_rate, train_rate, dimension_rate):\n",
    "    df_time = pd.DataFrame(data = np.zeros((1, 5)), columns = ['Dataset', \"train_rate\", 'sample_rate', 'dimension_rate', 'run_time'])\n",
    "    run_time = running_time(dataset, sample_rate, train_rate, dimension_rate)\n",
    "    df_time['Dataset'] = ds_name\n",
    "    df_time['train_rate'] = train_rate\n",
    "    df_time['sample_rate'] = sample_rate\n",
    "    df_time['dimension_rate'] = dimension_rate\n",
    "    df_time['run_time'] = run_time\n",
    "    if not os.path.exists(save_path + \"SMATE_running_time_full.csv\"):\n",
    "        df_time.to_csv(save_path + \"SMATE_running_time_full.csv\", index=False)\n",
    "    else:\n",
    "        res = pd.read_csv(save_path + \"SMATE_running_time_full.csv\")\n",
    "        res = pd.concat((res, df_time))\n",
    "        res.to_csv(save_path + \"SMATE_running_time_full.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 20:01:35.308179 23389298612032 deprecation_wrapper.py:119] From /gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/utils/basic_modules.py:60: The name tf.keras.layers.CuDNNGRU is deprecated. Please use tf.compat.v1.keras.layers.CuDNNGRU instead.\n",
      "\n",
      "W0106 20:01:35.312204 23389298612032 deprecation.py:506] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train is 3 ; L = 175 ; D = 3 ; n_class = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 20:01:42.028955 23389298612032 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n",
      "W0106 20:01:42.063597 23389298612032 deprecation.py:323] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=261, n_sup=261, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (0.1000002) train_rate (1.0000002)  is 25\n",
      "n_train is 3 ; L = 350 ; D = 3 ; n_class = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 20:02:25.882560 23389298612032 training_utils.py:1101] Output dense_17 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_17.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=261, n_sup=261, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (0.2000002) train_rate (1.0000002)  is 4\n",
      "n_train is 3 ; L = 525 ; D = 3 ; n_class = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 20:03:03.899891 23389298612032 training_utils.py:1101] Output dense_26 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_26.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=261, n_sup=261, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (0.3000002) train_rate (1.0000002)  is 5\n",
      "n_train is 3 ; L = 700 ; D = 3 ; n_class = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 20:03:58.046490 23389298612032 training_utils.py:1101] Output dense_35 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_35.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=261, n_sup=261, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (0.4000002) train_rate (1.0000002)  is 6\n",
      "n_train is 3 ; L = 875 ; D = 3 ; n_class = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2f2eae9c2f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdimension_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msave_running_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-72f95fd3ac1a>\u001b[0m in \u001b[0;36msave_running_time\u001b[0;34m(ds_name, dataset, save_path, sample_rate, train_rate, dimension_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_running_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdf_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mdf_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdf_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_rate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-72f95fd3ac1a>\u001b[0m in \u001b[0;36mrunning_time\u001b[0;34m(dataset, sample_rate, train_rate, dimension_rate)\u001b[0m\n\u001b[1;32m     34\u001b[0m                   y_sup, sup_ratio, pool_step, d_prime)\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msmate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Train SMATE model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/SMATE_model.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mclass_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1 * L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mproto_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_repr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n_classes * L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mh_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n_classes * L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Adjust central points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m       \u001b[0;31m# framework.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_keras_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_keras_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;31m# Handle Keras mask propagation from previous layer to current layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mcreate_keras_history\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mkeras_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mTensors\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcame\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreated_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_keras_history_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcreated_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m_create_keras_history_helper\u001b[0;34m(tensors, processed_ops, created_layers)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mconstants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       processed_ops, created_layers = _create_keras_history_helper(\n\u001b[0;32m--> 246\u001b[0;31m           layer_inputs, processed_ops, created_layers)\n\u001b[0m\u001b[1;32m    247\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       \u001b[0mnode_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m_create_keras_history_helper\u001b[0;34m(tensors, processed_ops, created_layers)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mconstants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       processed_ops, created_layers = _create_keras_history_helper(\n\u001b[0;32m--> 246\u001b[0;31m           layer_inputs, processed_ops, created_layers)\n\u001b[0m\u001b[1;32m    247\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       \u001b[0mnode_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m_create_keras_history_helper\u001b[0;34m(tensors, processed_ops, created_layers)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mconstants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mconstants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m       processed_ops, created_layers = _create_keras_history_helper(\n\u001b[1;32m    246\u001b[0m           layer_inputs, processed_ops, created_layers)\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3287\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m         session != self._session):\n\u001b[0;32m-> 3289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   3220\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m     \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m     \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m     \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m     \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1486\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \"\"\"\n\u001b[0;32m-> 1488\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1489\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# output training time for different sample_rate & train_rate & dimension_rate\n",
    "# A) vary sample_rate \n",
    "train_rate = 1\n",
    "dimension_rate = 1\n",
    "for sample_rate in np.linspace(0.1, 1, 10):\n",
    "    save_running_time(ds_name, dataset, rep_output, sample_rate, train_rate, dimension_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0105 22:36:37.122359 22666830645056 deprecation_wrapper.py:119] From /gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/utils/basic_modules.py:60: The name tf.keras.layers.CuDNNGRU is deprecated. Please use tf.compat.v1.keras.layers.CuDNNGRU instead.\n",
      "\n",
      "W0105 22:36:37.126225 22666830645056 deprecation.py:506] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train is 245 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:36:47.621098 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n",
      "W0105 22:36:47.656050 22666830645056 deprecation.py:323] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.1000002)  is 6\n",
      "n_train is 491 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:37:04.084472 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.2000002)  is 73\n",
      "n_train is 737 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:38:27.480664 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.3000002)  is 121\n",
      "n_train is 983 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:40:39.490383 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.4000002)  is 129\n",
      "n_train is 1229 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:42:59.175713 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.5000002)  is 161\n",
      "n_train is 1475 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:45:51.243655 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.6000002)  is 170\n",
      "n_train is 1721 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:48:52.176080 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.7000002)  is 193\n",
      "n_train is 1967 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:52:15.856755 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.8000002)  is 209\n",
      "n_train is 2213 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:55:55.288800 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (0.9000002)  is 221\n",
      "n_train is 2459 ; L = 36 ; D = 6 ; n_class = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 22:59:47.853975 22666830645056 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=3000, batch_size=2459, n_sup=2459, n_sup=0, steps=3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002)  is 236\n"
     ]
    }
   ],
   "source": [
    "# B) vary train_rate\n",
    "sample_rate = 1\n",
    "dimension_rate = 1\n",
    "for train_rate in np.linspace(0.1, 1, 10):\n",
    "    save_running_time(ds_name, dataset, rep_output, sample_rate, train_rate, dimension_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0111 18:02:44.278718 23347293087552 deprecation_wrapper.py:119] From /gpfsdswork/projects/rech/pch/ulz67kb/SMATE_MTS/utils/basic_modules.py:60: The name tf.keras.layers.CuDNNGRU is deprecated. Please use tf.compat.v1.keras.layers.CuDNNGRU instead.\n",
      "\n",
      "W0111 18:02:44.281931 23347293087552 deprecation.py:506] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:02:50.455989 23347293087552 training_utils.py:1101] Output dense_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_8.\n",
      "W0111 18:02:50.488826 23347293087552 deprecation.py:323] From /linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (48.0000002) is 24\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:03:35.548574 23347293087552 training_utils.py:1101] Output dense_17 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_17.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (96.0000002) is 26\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:04:38.569157 23347293087552 training_utils.py:1101] Output dense_26 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_26.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (192.0000002) is 32\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:06:09.875384 23347293087552 training_utils.py:1101] Output dense_35 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_35.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (288.0000002) is 41\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:08:11.164425 23347293087552 training_utils.py:1101] Output dense_44 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_44.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (385.0000002) is 48\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:10:41.436360 23347293087552 training_utils.py:1101] Output dense_53 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_53.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (481.0000002) is 56\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:13:39.242166 23347293087552 training_utils.py:1101] Output dense_62 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (577.0000002) is 62\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:17:04.144081 23347293087552 training_utils.py:1101] Output dense_71 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_71.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (674.0000002) is 69\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:20:59.809541 23347293087552 training_utils.py:1101] Output dense_80 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_80.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (770.0000002) is 75\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:25:23.801935 23347293087552 training_utils.py:1101] Output dense_89 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_89.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (866.0000002) is 82\n",
      "x_train.shape is  (267, 144, 963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0111 18:30:19.099166 23347293087552 training_utils.py:1101] Output dense_98 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_98.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs=300, batch_size=534, n_sup=267, n_sup=267, steps=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genvsq01/ulz67kb/.conda/envs/SMAT_ADE/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time for sample_rate (1.0000002) train_rate (1.0000002) dimension_size (963.0000002) is 88\n"
     ]
    }
   ],
   "source": [
    "# C) vary dimension_rate\n",
    "sample_rate, train_rate = 1, 1\n",
    "dimension_rate = 0.05\n",
    "save_running_time(ds_name, dataset, rep_output, sample_rate, train_rate, dimension_rate)\n",
    "for dimension_rate in np.linspace(0.1, 1, 10):\n",
    "    save_running_time(ds_name, dataset, rep_output, sample_rate, train_rate, dimension_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2132a220a582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
